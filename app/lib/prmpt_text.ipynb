{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デバッグの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/paper_translator/lib',\n",
      " '/home/paper_translator',\n",
      " '/usr/lib/python311.zip',\n",
      " '/usr/lib/python3.11',\n",
      " '/usr/lib/python3.11/lib-dynload',\n",
      " '',\n",
      " '/home/paper_translator/.venv/lib/python3.11/site-packages',\n",
      " '/home/paper_translator/',\n",
      " '/home/paper_translator/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pprint import pprint\n",
    "sys.path.append(\"/home/paper_translator/\")\n",
    "pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmama_debug_handler = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llmama_debug_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Context の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. ストレージコンテクストの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage Context の作成\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore = SimpleDocumentStore(),\n",
    "    vector_store = SimpleVectorStore(),\n",
    "    index_store = SimpleIndexStore()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. データベースコンテクストの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "#from llama_index.llms import LlamaCPP\n",
    "from src.translator.llama_cpp import create_llama_cpp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_llama_cpp_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/paper_translator/lib/prmpt_text.ipynb セル 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prmpt_text.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prmpt_text.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prmpt_text.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mn_gpu_layers = 32\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prmpt_text.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mn_batch = 512\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prmpt_text.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prmpt_text.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prmpt_text.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m llm \u001b[39m=\u001b[39m create_llama_cpp_model(package_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_index\u001b[39m\u001b[39m\"\u001b[39m, model_path\u001b[39m=\u001b[39mmodel_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_llama_cpp_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf\"\n",
    "\"\"\"\n",
    "n_gpu_layers = 32\n",
    "n_batch = 512\n",
    "n_ctx = 4096\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    #model_url = model_url,\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=3900,\n",
    "    model_kwargs={\"n_gpu_layers=\": n_gpu_layers, \"n_batch=\": n_batch, \"n_ctx=\": n_ctx},\n",
    "    verbose=True\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "llm = create_llama_cpp_model(package_name=\"llama_index\", model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdd021d25e14c588d05109c4b81eace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121a81215dbf4890b33932202bf8b095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b902c056ef4509bb4be14c9197c91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646ac797046b44068bae7b263d9f83a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0758d858e3c4e128586b31290ba727d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1827763c6f8c46af888f9cc72be83092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286dbd6c996a4cccaca54cb00a537363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6de60b7b44547acb36aad179b9df8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f01d0b325749daaad3b3f14143997b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a640079eb12a46fda5df5fc72c785720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f678e92bd1854b6193522ddb94b42877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d23f28cd65f45aa8b421527e796e510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568858536af24c6398a268a504997219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37c00762a7445118e016aede658a29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "embed_model = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    callback_manager=callback_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ドキュメントの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index import SimpleDirectoryReader\n",
    "from src.XMLUtils import DocumentReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/paper_translator/data\"\n",
    "document_name = (\n",
    "    \"Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    ")\n",
    "document_path = f\"{base_path}/documents/{document_name}\"\n",
    "xml_path = f\"{document_path}/{document_name}.tei.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paper_translator/.venv/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#required_exts = [\".pdf\"]\n",
    "#reader = SimpleDirectoryReader(input_dir=document_path, required_exts=required_exts, recursive=True)\n",
    "reader = DocumentReader()\n",
    "docs = reader.load_data(xml_path=xml_path)\n",
    "#print(f\"documents: {docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Index の構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.NODE_PARSING ->  0.200998 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000644 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000511 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001072 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000898 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.002145 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000169 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000128 seconds\n",
      "Trace: index_construction\n",
      "    |_CBEventType.NODE_PARSING ->  0.200998 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000644 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000511 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001072 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000898 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.002145 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000169 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000128 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000583 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000288 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000145 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000785 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000998 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001095 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000468 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000235 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000364 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00019 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000537 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000544 seconds\n",
      "      |_CBEventType.CHUNKING ->  5.7e-05 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000255 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000204 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000146 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000319 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000263 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000262 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000251 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000283 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000275 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00025 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000251 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000248 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000253 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000264 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000258 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000431 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000249 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000257 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000263 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000224 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000224 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000219 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000236 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000204 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000204 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00029 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000211 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000207 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000204 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000203 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000212 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000232 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000199 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0002 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0002 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000198 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000198 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000198 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000206 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000205 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000206 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000205 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000203 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00028 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000248 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000231 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000214 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000211 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000209 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000263 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000208 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00021 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000204 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000215 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000294 seconds\n",
      "    |_CBEventType.EMBEDDING ->  69.114788 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.072013 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.045648 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.049649 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.048979 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.048199 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.045913 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.026022 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 構築したIndexの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存用のディレクトリを作成\n",
    "import os\n",
    "vector_index_dir = f\"{base_path}/vector_store\"\n",
    "os.makedirs(vector_index_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_context を保存\n",
    "storage_context = vector_index.storage_context\n",
    "storage_context.persist(vector_index_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 保存したIndexの読み出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore = SimpleDocumentStore.from_persist_dir(persist_dir=vector_index_dir),\n",
    "    vector_store = SimpleVectorStore.from_persist_dir(persist_dir=vector_index_dir),\n",
    "    index_store = SimpleIndexStore.from_persist_dir(persist_dir=vector_index_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "vector_store_index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_store_index.as_query_engine(service_context=service_context)\n",
    "response = query_engine.query(\"各セクションの翻訳文を作成してください。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "近年、自然言語処理の研究は深層学習の手法を用いて、文章の意味や関係性などより高次な情報を抽出する手法の開発に注目が集まっています。また、画像とテキストの両方の情報から一つの結果を出力するバイタルの研究も盛んです。\n",
      "\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: 自然言語処理の最近の動向について\n",
      "Answer: 近年、自然言語処理の研究は深層学習の手法を用いて、文章の意味や関係性などより高次な情報を抽出する手法の開発に注目が集まっています。また、画像とテキストの両方の情報から一つの結果を出力するバイタルの研究も盛んです。\n",
      "\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: 自然言語処理の最近の動向について\n",
      "Answer: 近年、自然言語処理の研究は深層学習の手法を用いて、文章の意味や関係性などより高次な情報を抽出する手法の開発に注目が集ま\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. プロンプトテンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = \"prompt.txt\"\n",
    "promot_dir_path = f\"{base_path}/prompt_temp\"\n",
    "f_path = f\"{promot_dir_path}/{f_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f_path, 'r', encoding='utf-8') as file:\n",
    "    text_qa_template_str = file.read()\n",
    "\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "query_engine = vector_store_index.as_query_engine(\n",
    "    #response_mode=\"refine\",\n",
    "    response_mode=\"compact\",\n",
    "    #response_mode=\"tree_summarize\", \n",
    "    text_qa_template=text_qa_template, \n",
    "    service_context=service_context, \n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  204.172424 seconds\n",
      "      |_CBEventType.RETRIEVE ->  0.073542 seconds\n",
      "        |_CBEventType.EMBEDDING ->  0.067013 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  204.094899 seconds\n",
      "        |_CBEventType.TEMPLATING ->  4.4e-05 seconds\n",
      "        |_CBEventType.LLM ->  203.644593 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 249080.89 ms\n",
      "llama_print_timings:      sample time =   166.36 ms /   221 runs   (    0.75 ms per token,  1328.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45636.30 ms /  2412 tokens (   18.92 ms per token,    52.85 tokens per second)\n",
      "llama_print_timings:        eval time = 155638.97 ms /   220 runs   (  707.45 ms per token,     1.41 tokens per second)\n",
      "llama_print_timings:       total time = 203544.82 ms\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"自然言語処理の最近の動向について\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然言語処理の近年の動きは、深層学習による進化が著しいです。\n",
      "- 文章を理解するためには、単語や句から構成要素とその関係性を学ぶ必要があります。\n",
      "- この構成要素とその関係性は、文書の構造と表現に反映されています。\n",
      "- 従って、文書の構造と表現を学習することで、文章を理解する能力を身につけることが可能です。\n",
      "- 深層学習の手法は、このような課題を解決するために適しているため、近年急速に普及してきました。\n",
      "- 特に、自然言語処理では、大量の文書からデータを集めることができるため、大規模なモデルを訓練し、性能を向上させることが可能です。\n",
      "- また、構成要素とその関係性を表す情報が、単なる文字列ではなく、数値的な表現で表されることも深層学習の手法を自然言語処理に適している理由の一つです。\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. プロンプトの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.XMLUtils import get_sections, make_xml_file\n",
    "from src.OpenAIUtils import get_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/home/paper_translator/data/documents/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    "pdf_name = (\n",
    "    \"Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training methods which learn directly from raw text have revolutionized NLP over the last few years (Dai & Le, 2015;Peters et al., 2018;Howard & Ruder, 2018;Radford et al., 2018;Devlin et al., 2018;Raffel et al., 2019).Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities. The development of \"text-to-text\" as a standardized input-output interface (McCann et al., 2018;Radford et al., 2019;Raffel et al., 2019) has enabled taskagnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization. Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data.These results suggest that the aggregate supervision accessible to modern pre-training methods within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets. However, in other fields such as computer vision it is still standard practice to pre-train models on crowd-labeled datasets such as ImageNet (Deng et al., 2009). Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision? Prior work is encouraging.Over 20 years ago Mori et al. (1999) explored improving content based image retrieval by training a model to predict the nouns and adjectives in text documents paired with images. Quattoni et al. (2007) demonstrated it was possible to learn more data efficient image representations via manifold learning in the weight space of classifiers trained to predict words in captions associated with images. Srivastava & Salakhutdinov (2012) explored deep representation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features. Joulin et al. (2016) modernized this line of work and demonstrated that CNNs trained to predict words in image captions learn useful image representations. They converted the title, description, and hashtag metadata of images in the YFCC100M dataset (Thomee et al., 2016) into a bag-ofwords multi-label classification task and showed that pretraining AlexNet (Krizhevsky et al., 2012) to predict these labels learned representations which preformed similarly to ImageNet-based pre-training on transfer tasks. Li et al. (2017) then extended this approach to predicting phrase ngrams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image classification datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score. Adopting more recent architectures and pre-training approaches, VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-VIRT (Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text.While exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on common benchmarks is much lower than alternative approaches. For example, Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accuracy of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012). Instead, more narrowly scoped but well-targeted uses of weak supervision have improved performance. Mahajan et al. (2018) showed that predicting ImageNet-related hashtags on Instagram images is an effective pre-training task. When fine-tuned to ImageNet these pre-trained models increased accuracy by over 5% and improved the overall state of the art at the time. Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset.This line of work represents the current pragmatic middle ground between learning from a limited amount of supervised \"gold-labels\" and learning from practically unlimited amounts of raw text. However, it is not without compro-mises. Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their \"zero-shot\" capabilities.A crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale. While Mahajan et al. (2018) and Kolesnikov et al. (2019) trained their models for accelerator years on millions to billions of images, VirTex, ICMLM, and ConVIRT trained for accelerator days on one to two hundred thousand images. In this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision. We study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute and observe that transfer performance is a smoothly predictable function of compute (Hestness et al., 2017;Kaplan et al., 2020). We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find  . CLIP is much more efficient at zero-shot transfer than our image caption baseline. Although highly expressive, we found that transformer-based language models are relatively weak at zero-shot ImageNet classification. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words (BoW) encoding of the text (Joulin et al., 2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x.it can be competitive with prior task-specific supervised models. We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model's capability. These results have significant policy and ethical implications, which we consider in Section 7.\n"
     ]
    }
   ],
   "source": [
    "root = make_xml_file(dir_path=dir_path, pdf_name=pdf_name, is_debug=True)\n",
    "section = get_sections(root=root)[0]\n",
    "print(section.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"\n",
    "### 指示 ###\n",
    "文章の内容の中で、重要なポイントを3つ箇条書きしてください。\n",
    "箇条書きは、以下の制約に従ってください。\n",
    "\n",
    "### 箇条書きの制約 ###\n",
    "- 箇条書きの数は3個\n",
    "- 箇条書きは、文章の内容を簡潔にまとめたものである必要があります。\n",
    "- 箇条書き1個を50文字以内\n",
    "\n",
    "### 対象とする論文の内容 ###\n",
    "{text}\n",
    "\n",
    "### 出力形式 ###\n",
    "- 箇条書き1\n",
    "- 箇条書き2\n",
    "- 箇条書き3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pre-training methods have revolutionized NLP by learning directly from raw text, scaling across compute, model capacity, and data, allowing for task-agnostic architectures to zero-shot transfer to downstream datasets.\n",
      "- Prior work suggests that using natural language supervision for image representation learning is promising, although demonstrated performance on common benchmarks is much lower than alternative approaches.\n",
      "- CLIP, a simplified version of ConVIRT trained from scratch for Contrastive Language-Image Pre-training, is an efficient method for learning from natural language supervision. CLIP learns to perform a wide set of tasks during pre-training and is competitive with prior task-specific supervised models.\n"
     ]
    }
   ],
   "source": [
    "prompt_text = get_message(text=section.body, system=SYSTEM)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = query_engine.query(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLIP (Contrastive Language-Image Pre-training) is a method that enables efficient learning from natural language supervision and competitive performance with task-specific supervised models in various computer vision tasks. It uses a contrastive learning approach to learn representations from large amounts of textual data, which can be used for image classification, object detection, segmentation, among others.\n",
      "The original answer is as follows: - Pre-training methods directly from raw text have revolutionized NLP, and may hold potential for computer vision as well.\n",
      "- Natural language supervision for image representation learning is still rare due to lower performance on common benchmarks compared to alternative approaches.\n",
      "- CLIP, a contrastive language-image pre-training method, enables efficient learning from natural language supervision and is competitive with prior task-specific supervised models.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
