{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デバッグの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmama_debug_handler = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llmama_debug_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Context の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. ストレージコンテクストの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage Context の作成\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore = SimpleDocumentStore(),\n",
    "    vector_store = SimpleVectorStore(),\n",
    "    index_store = SimpleIndexStore()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. データベースコンテクストの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import LlamaCPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 45043,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:            blk.3.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:            blk.4.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:            blk.7.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:            blk.9.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:           blk.10.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:           blk.11.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:           blk.14.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:           blk.15.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.17.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:           blk.21.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:           blk.22.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 45043,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:       general.source.hugginface.repository str     \n",
      "llama_model_loader: - kv   3:                   llama.tensor_data_layout str     \n",
      "llama_model_loader: - kv   4:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   5:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   6:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   8:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:               general.quantization_version u32     \n",
      "llama_model_loader: - kv  20:                          general.file_type u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 45043\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 4096\n",
      "llm_load_print_meta: n_ctx          = 3900\n",
      "llm_load_print_meta: n_embd         = 4096\n",
      "llm_load_print_meta: n_head         = 32\n",
      "llm_load_print_meta: n_head_kv      = 32\n",
      "llm_load_print_meta: n_layer        = 32\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 1\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 1.0e-06\n",
      "llm_load_print_meta: n_ff           = 11008\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 7B\n",
      "llm_load_print_meta: model ftype    = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model size     = 6.85 B\n",
      "llm_load_print_meta: general.name   = ELYZA-japanese-Llama-2-7b-fast-instruct\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  = 3961.79 MB (+ 1950.00 MB per state)\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/35 layers to GPU\n",
      "llm_load_tensors: VRAM used: 0 MB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: kv self size  = 1950.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =  269.22 MB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 267.75 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 32\n",
    "n_batch = 512\n",
    "n_ctx = 4096\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    #model_url = model_url,\n",
    "    model_path=\"/home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=3900,\n",
    "    model_kwargs={\"n_gpu_layers=\": n_gpu_layers, \"n_batch=\": n_batch, \"n_ctx=\": n_ctx},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "embed_model = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    callback_manager=callback_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ドキュメントの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/paper_translator/data\"\n",
    "document_path = f\"{base_path}/documents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: [Document(id_='857c457b-9593-427c-bc83-310750ef83d9', embedding=None, metadata={'page_label': '1', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='9e028ff9ce2f64bd0d69fb38424ffaadaa90c995ae5e04a25c94628ca2e67e30', text='FPTQ: F INE-GRAINED POST-TRAINING QUANTIZA -\\nTION FOR LARGE LANGUAGE MODELS\\nQingyuan Li†1, Yifan Zhang†∗1,2, Liang Li1, Peng Yao1, Bo Zhang1, Xiangxiang Chu1, Yerui Sun1,\\nLi Du2, and Yuchen Xie1\\n1Meituan\\n2Nanjing University\\nABSTRACT\\nIn the era of large-scale language models, the substantial parameter size poses\\nsignificant challenges for deployment. Being a prevalent compression technique,\\nquantization has emerged as the mainstream practice to tackle this issue, which\\nis mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activa-\\ntions in such bit widths). In this study, we propose a novel W4A8 post-training\\nquantization method for the available open-sourced LLMs, which combines the\\nadvantages of both two recipes. Therefore, we can leverage the benefit in the I/O\\nutilization of 4-bit weight quantization and the acceleration due to 8-bit matrix\\ncomputation. Nevertheless, the W4A8 faces notorious performance degradation.\\nAs a remedy, we involve layerwise activation quantization strategies which feature\\na novel logarithmic equalization for most intractable layers, and we combine them\\nwith fine-grained weight quantization. Without whistles and bells, we eliminate\\nthe necessity for further fine-tuning and obtain the state-of-the-art W4A8 quan-\\ntized performance on BLOOM, LLaMA, and LLaMA-2 on standard benchmarks.\\nWe confirm that the W4A8 quantization is achievable for the deployment of large\\nlanguage models, fostering their wide-spreading real-world applications.\\n1 I NTRODUCTION\\nLarge Language Models (LLMs) are distinguished for their exceptional emergent knowledge capac-\\nity(Wei et al., 2022), enabling them to perform admirably across a wide variety of language tasks.\\nHowever, their massive scale poses a significant hurdle to deployment due to the substantial storage\\nand the huge amount of computation required. This challenge is particularly pronounced in envi-\\nronments with limited resources such as edge computing devices and personal devices, where the\\nconstraints can inhibit the widespread adoption of these cutting-edge language models.\\nTo address this issue, several model compression strategies have been proposed, including prun-\\ning (Ma et al., 2023; Frantar & Alistarh, 2023; Sun et al., 2023), distillation (Zhang et al., 2023),\\nquantization (Frantar et al., 2022; Xiao et al., 2023), and low-rank decomposition (Yao et al., 2023).\\nEach of these approaches has its own limitations. For instance, pruning can achieve reasonable\\ncompression rates but it may require significant fine-tuning or are closely tied to specific hardware\\narchitectures. In contrast, quantization techniques, despite their universal applicability, are often\\nconfronted with the problem of significant quantization errors, particularly with the increasing pa-\\nrameter sizes (Dettmers et al., 2022).\\nLately, research attention has been shifted towards a more balanced approach to quantization, specif-\\nically the usage of lower-bit widths for weights and higher-bit widths for activation, like W4A16 in\\nGPTQ (Frantar et al., 2022). This introduces a novel perspective to tackle the computational and\\nmemory-intensive aspects of LLMs, which are typically composed of Transformer Decoder struc-\\ntures (Vaswani et al., 2017). During inference, it can be divided into compute-intensive context\\ndecoding stage and memory-intensive self-decoding stage, each presenting unique challenges and\\nopportunities for further optimization.\\n∗Work done as an intern at Meituan.†Equal Contribution.\\n1arXiv:2308.15987v1  [cs.CL]  30 Aug 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4febba71-fd65-4c4c-8fc4-2222049564ab', embedding=None, metadata={'page_label': '2', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='0a63ddae2a5af1e701d554f124dd50304a75800477ff92c722af9386700f601f', text='However, there is a conspicuous dearth of research that explores the synergistic combination of two\\nquantization recipes W8A8 and W4A16. This paper aims to bridge the gap by proposing an inno-\\nvative Fine-grained Post-Training Quantization (called FPTQ) method that combines the benefits of\\nboth, thereby providing an effective and efficient W4A8 solution for the deployment of a variety of\\navailable large language models that are tested across a myriad of natural language tasks.\\n0 10 20 30 40 50 60 70 80\\nActivation distribution before LAE101102103104105106107Frequency\\n0 10 20 30 40 50 60 70 80\\nActivation distribution after LAE101102103104105106107108Frequency\\nFigure 1: Activation distribution before and after logarithmic equalization on BLOOM-7B1.\\nWe first investigate the quantization difficulty by illustrating the activation distributions in different\\nlayers, discovering that their ranges differ dramatically which motivates us for a layerwise strat-\\negy. Subsequently, we provide a unique activation equalization technique to handle the intractable\\noutliers (Figure 1), and improve the overall performance with fine-grained weight quantization.\\nIn a nutshell, we make several key contributions to the field of LLM compression and deployment:\\n1.High performance and low-cost W4A8 compression : We are the first to achieve high-\\nperformance W4A8 (INT4 weights and INT8 activation) PTQ compression for large lan-\\nguage models, maintaining the accuracy of the original model. Being a post-training quan-\\ntization technique, it tremendously simplifies the production flow of LLMs.\\n2.Novel quantization scheme : Based on our comprehensive analysis of the activation dis-\\ntribution of LLMs, we employ a layerwise strategy to cope with different levels of quan-\\ntization difficulty. Particularly, we devise an offline logarithmic activation equalization to\\nrender a quantization-friendly distribution for previously intractable layers.\\n3.Inference-friendly : Our approach harmonizes the memory and computation efficiency\\nwhich enables the storage of weights in a 4-bit format while executing INT8 inference,\\nthereby catalyzing both the memory access and computation.\\n2 R ELATED WORK\\n2.1 L ARGE LANGUAGE MODELS\\nThe past few years have witnessed the booming of pre-trained language models. BERT (Devlin\\net al., 2019) is designed to understand the context of words in a sentence and has been used for tasks\\nsuch as sentiment analysis and question answering. RoBERTa (Liu et al., 2019) is an improved\\nversion of BERT with better pre-training techniques and larger training data. T5 (Raffel et al.,\\n2020) is designed to perform a wide range of natural language processing tasks, including language\\ntranslation and summarization. XLNet (Yang et al., 2019) is designed to handle long sequences\\nof text and has achieved state-of-the-art results on several natural language processing tasks. GPT-\\n3 (Brown et al., 2020) is one of the most advanced LLMs with 175 billion parameters, capable of\\nperforming a wide range of natural language processing tasks. Along with the open-sourced ones\\nlike GLM (Du et al., 2021), BLOOM (Laurenc ¸on et al., 2022), OPT (Zhang et al., 2022) and LLaMa\\nseries (Touvron et al., 2023), LLMs have remarkably revolutionized the field of natural language\\nprocessing and are being used in a wide range of applications.\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='36348694-0791-4b8c-86f3-7f95550ef2a0', embedding=None, metadata={'page_label': '3', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e09824dc903264f5aa23dacf24cb21732396cbf6ffaec3b0cf1d5843eb45100c', text='Nevertheless, LLMs have billions of parameters and are often pre-trained on large amounts of text\\ndata, which require significant computational resources to train and deploy. There is a call for faster\\ninference time and lower memory requirements to make LLMs more practical.\\n2.2 Q UANTIZATION ON LLM S\\nApplying quantization to large language models presents unique challenges. Traditional PTQ\\nschemes have achieved great success in Convolutional Neural Networks (CNN) (Nagel et al., 2019;\\nWu et al., 2020; Nagel et al., 2021; Yao et al., 2021), but direct application to large language models\\noften results in severe accuracy loss, this is typically due to the presence of many outliers in the\\nactivation values of large models (Dettmers et al., 2022).\\nSeveral approaches have been proposed to address these issues. For example, LLM.int8() (Dettmers\\net al., 2022) splits the input activation values into two parts: non-outlier dimensions computed with\\nINT8, and outliers computed with FP16. GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2023)\\ncircumvent this difficulty by adopting FP16 activation and INT4 weight-only quantization. However,\\nthese methods also have their limitations, such as computational overhead and the inability to truly\\nleverage hardware acceleration.\\nOther approaches like SmoothQuant (Xiao et al., 2023), RPTQ (Yuan et al., 2023), and ZeroQuant-\\nV2 (Yao et al., 2023) propose different strategies to achieve quantization while mitigating the ac-\\ncuracy loss and computational overhead. However, SmoothQuant is merely a W8A8 solution and\\nit suffers from poor performance on W4A8. The rest tackles the W4A8 challenge but they come\\nwith their own set of challenges like weight reordering, asymmetric quantization, and group-wise\\nactivation, which can perplex the engineering work and may not well facilitate hardware. In light of\\nthese problems, we are driven to achieve W4A8 quantization without relying on QAT or distillation\\nmethods, paving the way for the efficient deployment of LLMs.\\n3 M ETHOD\\n3.1 W HYW4A8?\\nThe generative inference of LLMs can be divided into two stages: context decoding that generates\\nan output token given an input prompt (embedded as a sequence of tokens), and self-decoding that\\niteratively predicts the next token in a sequence, see Figure 2 (a). The former is compute-bound due\\nto the first-round computation of lengthy input sequences and the latter is memory-bound as a result\\nof sequential processing, thus two different implementations are required.\\nLLMContext Decode<out1><t1><t2>…<tn>Self DecodeCompute-boundMemory-boundLLM<out1><out2>…<out2>(a)Context DecodeSelf DecodeContext DecodeSelf DecodeContext Decode Self DecodeW8A8W4A16W4A8Time cost(b)\\nFigure 2: (a)Two stages of LLM inference where context decoding is compute-bound and self-\\ndecoding is memory-bound. (b)W4A8 speeds up both stages and is faster than the other two.\\nTable 1: Comparison of decoding stage efficiency for dif-\\nferent quantization methods. CD: Context Decoding, SD:\\nSelf-Decoding\\nMethod Efficient CD Efficient SD\\nZeroQuant No No\\nSmoothQuant Yes No\\nGPTQ No Yes\\nAWQ No Yes\\nOurs Yes YesPrevious quantization methods like\\nSmoothquant (Xiao et al., 2023) fea-\\ntures W8A8, while AWQ (Lin et al.,\\n2023) and GPTQ (Frantar et al., 2022)\\nuse W4A16. Both recipes compromise\\none stage for another, leading to infe-\\nrior overall performance, whereas only\\nW4A8 can boost both stages, see Fig-\\nure 2 (b) and Table 1. That is, context\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d660a228-0030-4f03-bbe8-06df166f2c1b', embedding=None, metadata={'page_label': '4', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='4ff86ba1f8cd8ba5bfb0638105d8fe98c25f42b305b4bed423b4483678135696', text='decoding enjoys the speed-up using 8-bit matrix multiplication, while self-decoding is also acceler-\\nated via reduced memory access using 4-bit weight.\\nThere are a few existing W4A8 studies. ZeroQuant (Yao et al., 2022) utilizes mixed precision for\\nself-attention weights (W8) and is not tested on larger models, ZeroQuantV2 (Yao et al., 2023)\\nuses fine-grained activation quantization which is not feasible in practice. ZeroQuant-FP (Wu et al.,\\n2023) alleviates the degradation by using higher-precision FP8 computation but it depends on spe-\\ncific hardware (e.g. NVIDIA H100). LLM-QAT (Liu et al., 2023a) adopts QAT to improve W4A8\\nperformance but it requires costly training and is prone to tedious hyper-parameter tuning. There-\\nfore, it is necessary to improve the accuracy of the W4A8 model while not harming its inference\\nspeed. The method shall also be made low-cost and generalizable for most up-to-date LLMs.\\n3.2 A NALYSIS OF ACTIVATION DISTRIBUTION ON LLM S\\nWith our goal in mind, we are driven to design a robust PTQ method. To begin with, we study why\\nvanilla W4A8 quantization is difficult for current LLMs. We first draw the activation distribution\\nof LLaMA-7B in Figure 3 to find the distinct behaviors of different layers. For instance, oproj has\\ncompact distribution while down proj spans extensively. This phenomenon reoccurs in many other\\nLLMs, see Appendix A.3.\\n0 10 20 30\\nLayer101\\n100101Activation valueso_proj activation values\\n0 10 20 30\\nLayer101\\n100101102103Activation valuesdown_proj activation values\\nFigure 3: Visualization of activation distribution of oprojanddown projon LLaMA-7B.\\nAs we can see from the above analysis, the maximum fluctuation range of input activation values\\nfor certain layers ranges from tens to thousands. Using per-tensor static quantization will result in\\nsignificant quantization errors, but using per-token dynamic quantization for all layers will not bring\\nadequate hardware acceleration. Therefore, it naturally calls for a layer-specific policy to determine\\nthe granularity of quantization.\\n3.3 FPTQ: F INE-GRAINED POST-TRAINING QUANTIZATION\\nMotivated by the above analysis, we propose our post-training quantization method which employs a\\nlayerwise quantization strategy regarding disparate activation distributions. Our complete procedure\\nis given in Algorithm 1. The key components are discussed in detail.\\n3.3.1 L AYER -WISE ACTIVATION QUANTIZATION STRATEGY\\nThe key to resolving the activation quantization difficulty lies in the outlier treatment. Empirically,\\nwe can use different activation quantization strategies for different layers, as shown in Table 2.\\nFor activation value ranges within tens (denoted as v0), per-tensor static quantization can be safely\\nused. However, to avoid quantization loss for activation ranges over hundreds (denoted as v1), per-\\ntoken dynamic quantization shall be put in place although slightly sacrificing hardware acceleration\\nbenefits. For most layers that range within hundreds, i.e. (v0, v1), it demands a particular strategy\\nthat simultaneously reduces the quantization error while not harming the inference speed.\\nXiao et al. (2023) discover that when larger outliers dominate the distribution, the effective quan-\\ntization bits of inliers are substantially narrowed. For per-tensor 8-bit quantization, it becomes\\n28·mi/mwhere miis the maximum amplitude of channel iandmis the maximum value of the\\nwhole tensor. They also observe that outliers stay in fixed channels. Based on these two find-\\nings, we are allowed to perform per-channel outlier suppression on activations. SmoothQuant (Xiao\\net al., 2023) attempts to ‘smooth’ per-channel distribution by dividing the activation with a scale\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='84e70364-b2d0-4239-9c79-2857a418ed75', embedding=None, metadata={'page_label': '5', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='642916ab2b05b15d57bd00c84cdc049efea313a023ea22da5f91df9b564ed118', text='Algorithm 1 FPTQ: Fine-grained Post-Training Quantization\\nInput: A pre-trained LLM\\nOutput: A quantized LLM\\n1:Calibrate the pre-trained LLM with a predefined dataset\\n2:Perform activation distribution analysis\\n3:for each layer- lin the Transformer structure ( Llayers in total) do\\n4: ifActivation range v≤v0then\\n5: Set layer l’s activation quantization policy to static per-tensor\\n6: else if Activation range v0< v < v 1then\\n7: Perform logarithmic activation equalization\\n8: Set layer l’s activation quantization policy to static per-tensor\\n9: else\\n10: Set layer l’s activation quantization policy to dynamic per-token\\n11: end if\\n12: Set each layer’s weight quantization policy as fine-grained\\n13:end for\\n14:Update the LLM’s weights and activations w.r.t. the chosen quantization policy\\n15:Get the high-performance quantized LLM\\nTable 2: Activation quantization strategies for different ranges of activation values.\\nActivation Value Range Quantization Strategy Hardware Efficiency Typical Operation\\nv≤v0 per-tensor, static High Dense\\nv0< v < v 1 LAE + per-tensor, static High QKV , FC1\\nv≥v1 per-token, dynamic Medium FC2\\nsi= max( |xi|)/max(|wi|), where xiandwiare activation and weight of channel irespectively.\\nAWQ (Lin et al., 2023) introduces grid-searched hyper-parameters αandβto lay importance to ac-\\ntivation and weight separately, where they find the contribution of weights is marginal and suggest\\nactivation-awareness is most important. In this paper, we argue that it is unnecessary to consider\\nweights for computing the activation “smoothing” scale. Besides, it is crucial to retain all the activa-\\ntion values with a non-linear lossless mapping , yet it has to satisfy two criteria (1)touching gently\\nwith the inliers (2)harshly suppressing the outliers. In this regard, we verify that the logarithmic\\nfunction rightly fits this purpose.\\nLogarithmic Activation Equalization. To render a quantization-friendly activation distribution,\\nwe propose a new offline logarithmic activation equalization (LAE) method that moderates activa-\\ntion distributions in a non-linear fashion. Specifically, we compute the i-th channel scale sias the\\nmaximum activation value max(|Xi|)divided by its logarithmic mapping with a shift of 2 (to have\\na minimum of scale 1), shown in Equation 1. The formula retains the original information while it\\nsquashes various distributions comparably. Figure 1 exhibits its outcome distribution.\\nsi= max( |xi|)/log2(2 + max( |xi|)); xi=xi/si (1)\\nOnce the scale sis obtained, we can update the corresponding weight and activation as follows,\\nW′=diag(s)W;X′=Xdiag(s)−1s.t. X′W′=XW (2)\\nHence, this update is made in-place as it is mathematically equivalent. Notably, scan be easily fused\\ninto the weight of the previous layer. In our case, there are only two types of operations (QKV and\\nFC1) whose activation ranges are in ( v0, v1). To apply the offline LAE, their activation updates are\\nfused into their preceding operation LayerNorm (Ba et al., 2016).\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7aadc5fc-cc38-4fbf-90a3-228cbe518b3c', embedding=None, metadata={'page_label': '6', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='89dfc80f86b21435846bab1176a985ccbcac0813c4fe1013e2c443ad04fd26a9', text='3.3.2 W EIGHT QUANTIZATION\\nDue to the intricacies of LLMs, it is not tractable to use the vanilla per-channel strategy only, as\\nshown in Figure 4 (a). ZeroQuant (Yao et al., 2022) adopts a fine-grained groupwise weight quan-\\ntization (Shen et al., 2020) that addresses the quantization difficulty of smaller LLMs like GPT-3\\n(Brown et al., 2020). As the two strategies are identically costly from the engineering perspective,\\nwe adopt fine-grained weight quantization where the scale is computed groupwise for all layers to\\nobtain better performance, depicted in Figure 4 (b).\\n(a)(b)NQuantizedWeights(K ×N)IndimOutdimScaleK/GroupGroupQuantized Weights(K ×N)IndimOutdimScale1NLAE\\n(c)QKVKcacheVcacheDenseLayerNormLAE\\n(d)UpGateDownLayerNorm\\nFigure 4: (a)Per-channel weight quantization. (b)Fine-grained per-channel quantization. (c, d)\\nSelf-attention and FFN in most LLMs. Light blue: per-tensor static activation quantization. Purple:\\nper-token dynamic activation quantization. All weights are quantized in a fine-grained manner.\\nAs LLaMA series (Touvron et al., 2023) rises to the mainstream focus, we illustrate our specific\\nquantization scheme for its architecture in Figure 4 (c) and (d). Interestingly, we discover that\\nthe trend of LLaMA activation distributions holds for all model series, such that our quantization\\nscheme can be directly reused. Logarithmic activation equalization is performed offline (the scale\\nfor activation is then fused into LayerNorm) for QKV and Up/Gate. It’s also worth noting that the\\nquantized KV cache is applied to save I/O costs.\\n4 E XPERIMENT\\n4.1 D ATASETS\\nWe validated our quantization scheme on several datasets, including LAMBADA (Paperno et al.,\\n2016), MMLU (Hendrycks et al., 2020), and a set of Common Sense QA (Talmor et al., 2019) tasks\\nlike WinoGrande (Sakaguchi et al., 2021), PIQA (Tata & Patel, 2003), HellaSwag (Zellers et al.,\\n2019), ARC e. For CommonSense QA tasks, we used the Language Model Evaluation Harness (Gao\\net al., 2021) tool to evaluate our models. For the calibration set, we randomly sampled 512 samples\\nfrom the Pile dataset (Gao et al., 2020).\\n4.2 I MPLEMENTATION\\nModel Original SmoothQuant FPTQ\\nFP16 W8A8 W4A8\\nBLOOM-7B1 57.9080% 59.6352% 58.2185%\\nLLaMA-7B 73.7435% 73.7823% 73.8017%\\nLLaMA-13B 76.1886% 76.3633% 75.7423%\\nLLaMA-65B 79.1966% 78.6920% 79.1384%\\nLLaMA-2-7B 73.7046% 74.1510% 72.4820%\\nLLaMA-2-13B 76.6350% 75.5288% 75.3154%\\nLLaMA-2-70B 79.5653% 78.7891% 78.7114%\\nTable 3: Comparison on the LAMBADA Dataset.Baselines. In our experiments, we se-\\nlected SmoothQuant (Xiao et al., 2023)\\nand GPTQ (Frantar et al., 2022) as our\\nbaselines, given their status as the most\\nprevalent W8A8 and W4A16 quantiza-\\ntion schemes, respectively. These meth-\\nods have been widely adopted in vari-\\nous applications and their performance\\nhas been extensively validated, estab-\\nlishing them as reliable benchmarks in\\nthe field of LLMs quantization. Simul-\\ntaneously, to further demonstrate the po-\\ntential of FPTQ, we compare it with the\\nQAT method, particularly with LLM-\\nQAT (Liu et al., 2023b). It’s worth mentioning that QAT introduces a significant computational\\nresource overhead; in contrast, our approach incurs a negligible cost compared to it.\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='dd7475a8-6d89-450b-b2a8-e7090aca4552', embedding=None, metadata={'page_label': '7', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='08610562253adef0c7afb7b3bb95a970e5eb7721879f24d13f460f18ae3082d7', text='Implementation. We find that for the investigated LLMs in our paper, the activation bound v0can\\nbe typically set as 15 and v1150.\\n4.3 E XPERIMENTAL RESULTS ON LAMBADA\\nWe initially conducted our experiments on the LAMBADA dataset (Paperno et al., 2016). Despite\\nthe fact that LAMBADA may not effectively reflect the comprehensive capabilities of the model,\\nit serves as a valuable tool for rapidly validating model precision and quantifying the impact on\\nmodel performance. Our method, Fine-grained Post-training Quantization (FPTQ), achieved W4A8\\nquantized models that demonstrated precision strikingly similar to their floating-point counterparts\\non both the BLOOM-7B1 (Scao et al., 2022) and all models in the LLaMA series (Touvron et al.,\\n2023). This is a highly encouraging observation, suggesting the efficacy of our approach.\\nModel HyperParam MMLU Common Sense QA\\nMethod BW Hums. STEM Social Other Avg WG PIQA HS ARC eAvg\\nBLOOM-7B1 FP16 W16A16 26.10 26.84 24.21 26.34 25.90 63.93 72.91 57.24 57.74 62.96\\nSmoothQuant W8A8 26.04 27.80 24.50 25.82 26.03 61.96 72.52 56.66 57.41 62.14\\nGPTQ W4A16 26.06 26.47 25.28 26.50 26.08 63.38 72.42 55.98 56.86 62.16\\nFPTQ W4A8 25.87 26.71 23.76 26.56 25.74 63.22 72.80 55.98 57.32 62.33\\nLLaMA-7B FP16 W16A16 33.60 31.10 38.20 38.40 35.20 69.85 79.16 76.10 72.80 74.48\\nSmoothQuant W8A8 33.88 30.32 37.63 39.08 35.14 70.09 79.00 75.17 72.22 74.12\\nGPTQ W4A16 32.39 30.35 35.03 36.15 33.40 68.03 77.69 72.95 69.44 72.02\\nFPTQ W4A8 30.20 29.95 32.76 35.87 32.02 70.01 78.40 74.46 70.79 73.42\\nLLaMA-13B FP16 W16A16 44.60 37.10 54.00 53.50 47.10 72.77 80.09 79.07 74.71 76.66\\nSmoothQuant W8A8 44.14 36.51 54.05 52.65 46.64 72.06 79.71 78.34 73.91 76.00\\nGPTQ W4A16 46.01 39.00 54.01 53.36 47.96 73.16 80.25 78.60 74.37 76.59\\nFPTQ W4A8 40.96 34.19 49.72 49.75 43.46 72.14 79.33 77.50 72.69 75.41\\nLLaMA-65B FP16 W16A16 61.80 52.00 73.30 67.60 63.50 77.35 82.32 84.15 79.76 80.90\\nSmoothQuant W8A8 61.32 50.50 71.69 66.90 62.56 74.90 81.07 82.32 77.4 78.92\\nGPTQ W4A16 60.23 52.09 72.15 66.75 62.60 77.43 82.32 83.57 79.88 80.80\\nFPTQ W4A8 59.85 49.24 71.50 65.89 61.52 75.77 81.45 83.44 78.45 79.78\\nLLaMA-2-7B FP16 W16A16 43.40 37.00 51.80 52.40 46.00 69.06 79.11 75.98 74.58 74.68\\nSmoothQuant W8A8 42.49 36.65 50.67 51.33 45.06 69.06 77.97 75.91 75.98 74.58\\nGPTQ W4A16 42.66 36.45 51.25 50.99 45.13 68.51 78.67 0.75.96 71.68 73.45\\nFPTQ W4A8 41.15 35.79 49.37 50.77 44.02 69.38 77.97 74.89 72.85 73.77\\nLLaMA-2-13B FP16 W16A16 54.40 44.30 63.40 60.80 55.70 72.22 80.52 79.38 77.44 77.39\\nSmoothQuant W8A8 52.67 43.07 63.15 60.39 54.69 72.06 79.54 79.28 77.31 77.05\\nGPTQ W4A16 51.99 43.57 63.05 60.49 54.56 72.30 79.60 78.79 77.23 76.98\\nFPTQ W4A8 51.65 42.54 62.27 59.90 53.92 70.56 79.43 78.06 75.76 75.95\\nLLaMA-2-70B FP16 W16A16 65.20 57.80 80.40 74.60 69.10 77.98 82.75 83.81 80.98 81.38\\nSmoothQuant W8A8 63.23 56.46 79.23 72.42 67.40 78.14 82.37 82.60 80.72 80.96\\nGPTQ W4A16 62.93 57.65 79.62 74.12 68.04 78.06 82.92 83.37 80.89 81.31\\nFPTQ W4A8 62.83 55.27 78.23 72.49 66.81 77.03 82.37 82.58 79.88 80.47\\nTable 4: Comparison on MMLU and Common Sense QA. BW: BitWidth\\n4.4 R ESULTS ON MMLU AND COMMON SENSE QA\\nMMLU (Hendrycks et al., 2020) and Common Sense QA (Talmor et al., 2019) are currently\\nrenowned datasets that comprehensively reflect the performance of LLMs. We conducted extensive\\nexperiments on these datasets, including comparative assessments with two state-of-the-art solu-\\ntions: SmoothQuant for W8A8, and GPTQ for W4A16.\\nOn the MMLU dataset, our approach exhibits a performance gap within 1% for most models com-\\npared to SmoothQuant. Notable outliers include LLaMA-7B and LLaMA-13B, which show a more\\npronounced drop. However, it’s important to note that the MMLU dataset, with its predominant\\ncomposition of multiple-choice questions, may exhibit bias in precision estimation when the inher-\\nent capabilities of the model are limited.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='744440ee-317c-44a2-b84b-0ec3c454ad87', embedding=None, metadata={'page_label': '8', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='c62027320472a7dec55065bdc09c9b893baa17d9d514af954534e34aa8aac0ec', text='On Common Sense QA, our approach demonstrates a mere 1% precision gap with the FP16 model\\nacross nearly all models, including the previously identified underperforming models LLaMA-7B\\nand LLaMA-13B on MMLU. This observation underscores the robustness of our approach.\\n4.5 C OMPARISON WITH LLM-QAT ONCOMMON SENSE QA\\nModel Method Setting WG PIQA HS ARC eAvg.\\nLLaMA-7B Original FP16 69.85 79.16 44.40 72.81 74.51\\nLLM-QAT W4A8 68.80 77.40 73.00 68.40 71.90\\nFPTQ W4A8 70.09 78.62 74.45 70.37 73.38\\nLLaMA-13B Original FP16 72.22 80.52 79.38 77.44 77.39\\nLLM-QAT W4A8 70.60 79.10 77.50 73.00 75.05\\nFPTQ W4A8 72.85 80.09 78.20 76.09 76.81\\nTable 5: Comparison with LLM-QAT on LLaMA-7B.Given the paucity of other Post-\\ntraining Quantization (PTQ) works\\nemploying W4A8 quantization, we\\nconducted a comparative study with the\\nQuantization-Aware Training (QAT)\\nmethod, LLM-QAT (Liu et al., 2023b),\\non the Common Sense QA dataset.\\nOur approach achieved a precision that\\nwas notably closer to the FP16 model\\ncompared to LLM-QAT. However, due\\nto the limited data publicly available\\nfrom LLM-QAT, we present here the\\nexperimental results for only LLaMA-7B and LLaMA-13B. It can be observed that our approach\\nyields slightly superior results on every subset of the dataset compared to LLM-QAT, highlighting\\nthe effectiveness of our methodology.\\n5 A BLATION STUDY\\n5.1 C OMPARISON WITH DATA-FREE QUANTIZATION\\nWe acknowledge that the calibration dataset may be one of the factors affecting the performance\\nof the quantized model. Therefore, to maintain fairness, we utilized the Pile dataset (Gao et al.,\\n2020) as a calibration dataset in our previous experiments. However, to demonstrate the robustness\\nof our method, we applied randomly generated tokens for model calibration. We conducted ablation\\nstudies on BLOOM-7B1, LLaMA-7B and LLaMA-2-7B under W8A8 and W4A8 bit-width settings\\nin Table 6. It’s exhilarating to note that, it was found that using a random dataset often resulted in\\nsuperior results in most cases. This attests that our method is applicable in data-free situations.\\nModel HyperParam MMLU Common Sense QA\\nCalibration BW Hums. STEM Social Other Avg WG PIQA HS ARC eAvg\\nLLaMA-7B Pile W8A8 33.88 30.32 37.63 39.08 35.14 70.09 79.00 75.17 72.22 74.12\\nRandom W8A8 32.33 29.85 36.46 38.25 34.07 70.01 78.62 75.48 72.69 74.20\\nPile W4A8 30.20 29.95 32.76 35.87 32.02 70.01 78.40 74.46 70.79 73.42\\nRandom W4A8 31.20 31.05 36.37 37.01 33.64 68.67 78.62 74.62 71.21 73.28\\nLLaMA-2-7B Pile W8A8 42.49 36.65 50.67 51.33 45.06 69.06 77.97 75.91 75.98 74.58\\nRandom W8A8 42.55 36.28 51.41 51.63 45.24 67.80 79.22 75.98 74.28 74.32\\nPile W4A8 41.15 35.79 49.37 50.77 44.02 69.38 77.97 74.89 72.85 73.77\\nRandom W4A8 41.32 35.42 47.97 50.19 43.56 67.88 78.07 75.46 73.11 73.63\\nBLOOM-7B1 Pile W8A8 26.04 27.80 24.50 25.82 26.03 63.93 72.91 57.24 57.74 62.96\\nRandom W8A8 25.80 27.60 25.06 26.96 26.29 63.77 72.80 56.65 57.45 62.67\\nPile W4A8 25.87 26.71 23.76 26.56 25.74 61.96 72.52 56.66 57.41 62.14\\nRandom W4A8 26.29 27.04 23.37 27.08 25.99 61.88 72.42 56.17 56.94 61.85\\nTable 6: Ablation study on calibration datasets on MMLU and Common Sense QA.\\n5.2 W EIGHT QUANTIZATION WITH GPTQ\\nWe observe that the GPTQ method, which compensates weights based on the Hessian matrix, is\\northogonal to our existing approach. Therefore, we attempted to fine-tune the weights using the\\nGPTQ method after conducting logarithmic activation equalization (LAE) on the model, to investi-\\ngate the potential for increased precision. However, our experiments in Table 7 demonstrated that\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='22139c64-b3cb-4a56-8ed2-f69131d0ab52', embedding=None, metadata={'page_label': '9', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a7272969789fbe5ad61471f42b567387001eeefd34efe6604713dcb88f1a7bf6', text='the addition of GPTQ operations generally resulted in a negative impact on precision in most cases.\\nWe encourage future researchers to conduct more intriguing explorations in this area.\\nModel HyperParam MMLU Common Sense QA\\nMethod BW Hums. STEM Social Other Avg WG PIQA HS ARC eAvg\\nLLaMA-7BFP16 W16A16 33.60 31.10 38.20 38.40 35.20 69.85 79.16 76.21 72.81 74.51\\nFPTQ W4A8 30.20 29.95 32.76 35.87 32.02 70.01 78.40 74.46 70.79 73.42\\nFPTQ GPTQ W4A8 28.40 28.33 30.84 33.22 30.03 68.82 78.13 72.88 66.96 71.70\\nLLaMA-2-7BFP16 W16A16 43.40 37.00 51.80 52.40 46.00 69.06 79.11 75.98 74.58 74.68\\nFPTQ W4A8 41.15 35.79 49.37 50.77 44.02 69.38 77.97 74.89 72.85 73.77\\nFPTQ GPTQ W4A8 40.57 35.42 48.55 48.86 43.13 67.56 78.35 74.90 72.94 73.44\\nBLOOM-7B1FP16 W16A16 26.10 26.84 24.21 26.34 25.90 63.93 72.91 57.24 57.74 62.96\\nFPTQ W4A8 25.87 26.71 23.76 26.56 25.74 61.96 72.52 56.66 57.41 62.14\\nFPTQ GPTQ W4A8 26.21 28.20 25.28 26.37 26.47 62.90 72.31 55.39 57.28 61.97\\nTable 7: Ablation on MMLU and Common Sense QA. FPTQ GPTQ : weights updated by GPTQ first.\\n6 D ISCUSSION AND FUTURE DIRECTIONS\\nAnalysis on computation efficiency. Modern GPUs, such as the NVIDIA A100, support parallel\\nblock-wise matrix computation and pipeline processing. Fine-grained weight quantization enjoys\\nsuch block-wise computation and introduces little overhead. Currently, the W4A16 acceleration\\nis based on the GPU FP16INT4 GEMM kernel (Kim et al., 2022), which implements mixed-type\\nmatrix computation. The INT4 weights are first converted to FP16, and matrix computation is then\\nperformed with FP16. The underlying computation still uses the GPU’s floating-point computation\\nunit, so in the case of long inputs and large batches, the FP16INT4 kernel even has a negative\\neffect compared to direct FP16 computation because of the additional conversion. The W8A8 com-\\nputation acceleration is based on the GPU INT8 GEMM kernel, which uses INT8 Tensor Cores for\\nunderlying computation. There is a noticeable acceleration in the context decoding stage, but in the\\nself-decoding stage, the bottleneck mainly lies in memory access.\\nTo simultaneously address the acceleration issues in both the context decoding and self-decoding\\nstages, we can design an INT8INT4 kernel, which profits INT8 Tensor Cores for acceleration in\\nthe context decoding stage, while keeping the weights loaded as INT4 to reduce memory access\\ntime in the self-decoding stage.\\nData-free quantization. We discover that it is promising to randomly draw samples from the token\\nvocabulary as in Table 6. We believe that there is still room for improvement in this regard.\\nScale computation requires activation only. For activation quantization, our method completely\\nremoves weights for the computation of siwhich echoes the findings in Lin et al. (2023). To make\\nour strategy more generalizable, we introduce a hyper-parameter αto control the level of suppres-\\nsion, see A.2. It is however possible to devise other non-linear mapping functions that are hyper-\\nparameter free and in the meanwhile lead to better performance.\\n7 C ONCLUSION\\nIn conclusion, our work presents a significant stride in the domain of Large Language Model (LLM)\\ncompression. Upon an overview of the existing quantization schemes, we introduce a novel post-\\ntraining quantization approach that can make the inference of LLMs more efficient, without com-\\npromising their performance. We successfully achieved high performance and efficiency for W4A8,\\nwhich has the optimal utilization of computational resources which enhances the speed of both\\ncontent-decoding and self-decoding stages. Furthermore, the removal of the need for fine-tuning\\nduring the training process simplifies the deployment pipeline significantly. This attests that our\\nmethod provides an effective deployable solution for LLMs without sacrificing their accuracy. While\\nour progress is encouraging, we acknowledge the potential for further exploration and refinement in\\nthis area. We anticipate that our work will inspire future research endeavors aimed at making LLMs\\neven more efficient and practical.\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f001226d-62e0-406b-89ac-dfe2ee451511', embedding=None, metadata={'page_label': '10', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a9c5d28c41b22227c42dd43e977786276333e0ad955e13d45020b4a1f18d154b', text='REFERENCES\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. In Conference on Neural Information Processing Systems (NeurIPS) , 2020.\\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix\\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339 , 2022.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In North American Chapter of the Associ-\\nation for Computational Linguistics (NAACL) , 2019.\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\\nGlm: General language model pretraining with autoregressive blank infilling. arXiv preprint\\narXiv:2103.10360 , 2021.\\nElias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in\\none-shot, 2023.\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\\nfor language modeling. arXiv preprint arXiv:2101.00027 , 2020.\\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric\\nTang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot lan-\\nguage model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.\\n5371628 .\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300 , 2020.\\nYoung Jin Kim, Rawn Henry, Raffy Fahim, and Hany Hassan Awadalla. Who says elephants\\ncan’t run: Bringing large scale moe models into cloud scale production. arXiv preprint\\narXiv:2211.10017 , 2022.\\nHugo Laurenc ¸on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral,\\nTeven Le Scao, Leandro V on Werra, Chenghao Mou, Eduardo Gonz ´alez Ponferrada, Huu Nguyen,\\net al. The BigScience corpus: A 1.6 TB composite multilingual dataset. 2022.\\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-\\naware weight quantization for llm compression and acceleration, 2023.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach. arXiv preprint arXiv:1907.11692 , 2019.\\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang\\nShi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware\\ntraining for large language models. arXiv preprint arXiv:2305.17888 , 2023a.\\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang\\nShi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware\\ntraining for large language models, 2023b.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a49a83a6-17d1-47bc-8ab9-6d8bf43ce22c', embedding=None, metadata={'page_label': '11', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='96490b9fc67bf430ce18083da29a66c668b23da78eaf747c7ba5c0389b95102c', text='Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large\\nlanguage models, 2023.\\nMarkus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization\\nthrough weight equalization and bias correction. In International Conference on Computer Vision\\n(ICCV) , 2019.\\nMarkus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen,\\nand Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint\\narXiv:2106.08295 , 2021.\\nDenis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,\\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern ´andez. The LAMBADA dataset:\\nWord prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031 , 2016.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\\nsarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021.\\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\\nCastagn ´e, Alexandra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall ´e, et al. Bloom: A 176b-\\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.\\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney,\\nand Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings\\nof the AAAI Conference on Artificial Intelligence , volume 34, pp. 8815–8821, 2020.\\nMingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach\\nfor large language models. arXiv preprint arXiv:2306.11695 , 2023.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge, 2019.\\nSandeep Tata and Jignesh M Patel. PiQA: An algebra for querying protein data sets. In International\\nConference on Scientific and Statistical Database Management , 2003.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\\nefficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Conference on Neural In-\\nformation Processing Systems (NeurIPS) , 2017.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\\nmodels. arXiv preprint arXiv:2206.07682 , 2022.\\nHao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization\\nfor deep learning inference: Principles and empirical evaluation, 2020.\\nXiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap forward in llms post-training\\nw4a8 quantization using floating-point formats, 2023.\\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:\\nAccurate and efficient post-training quantization for large language models. In International\\nConference on Machine Learning , pp. 38087–38099. PMLR, 2023.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\\nXlnet: Generalized autoregressive pretraining for language understanding. Advances in neural\\ninformation processing systems , 32, 2019.\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e1cf6a71-f264-4c4c-80d2-433674246f7d', embedding=None, metadata={'page_label': '12', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3c4cba6d6466ded9d5ace7dc10b5eb2bee18585d0fd4e7e67018e3cdc7806fef', text='Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang,\\nQijing Huang, Yida Wang, Michael Mahoney, et al. HAWQ-v3: Dyadic neural network quanti-\\nzation. In International Conference on Machine Learning (ICML) , 2021.\\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He.\\nZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. arXiv\\npreprint arXiv:2206.01861 , 2022.\\nZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring\\npost-training quantization in llms from comprehensive study to low rank compensation, 2023.\\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun,\\nQiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for\\nlarge language models, 2023.\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\\nchine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\\nChen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, and Dawei Song.\\nLifting the curse of capacity gap in distilling language models, 2023.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer\\nlanguage models. arXiv preprint arXiv:2205.01068 , 2022.\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='15df345c-4c87-436e-af77-1a0710280fd0', embedding=None, metadata={'page_label': '13', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='744d2e39fe5842bb9b8afaa233650bf345121a3f640904af06dc1d213b85bdbd', text='A A PPENDIX\\nA.1 P RELIMINARY KNOWLEDGE ON QUANTIZATION\\nQuantization is a process of mapping continuous values to discrete ones by scaling. The scaling\\nfactor is also called quantization step size. In practice, a higher-precision floating point is used for\\ntraining and the quantized version is used for inference. Consider b-bit integer quantization, for a real\\ntensor xranges in (min, max ), it can be converted to an integer tensor x′within (−2b−1,2b−1−1)\\nby symmetric uniform quantization as,\\nscale =max(|x|)/(2b−1−1) (3)\\nx′=⌊(x/scale )⌉ (4)\\nWeight quantization and activation quantization. Typically, the weight is quantized as integer\\nvalues. Activation quantization refers to the quantization of intermediate activation feature maps.\\nStatic quantization vs. dynamic quantization. For static quantization, offline activation statistics\\nare collected to compute the scale and it is kept static during inference. For dynamic quantization,\\nsuch statistics are computed at runtime.\\nPer-tensor vs. per-token. In the per-tensor scheme, the tensor matrix is considered as a whole to\\ncompute the quantization scale. In the per-token scheme, each input token corresponds to a scale\\ncomputed upon all activation channels of the specific token. In essence, the per-token scheme is\\nmore fine-grained.\\nPer-channel vs. group-wise. In the per-channel scheme, the quantization scale is computed\\nchannel-wise. In the group-wise scheme, each channel is divided into several groups and so are\\nits scales.\\nA.2 G ENERALIZED FORM OF LAE\\nWe give a generalized form of logarithmic activation equalization function. For the majority of\\nLLMs, we use α= 1.\\nscale = (log2(2 +scale ))α(5)\\nA.3 M ORE ACTIVATION DISTRIBUTION OF LLM S\\nWe visualize the activation distributions of the LLaMA series in Figure 5, 6, 7, 8, 9, and 10. It is\\nrather exciting to find that LLaMA models at different scales share similar distributions in the same\\noperations, which leads to a universal quantization scheme.\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fd57d359-e5e4-45e8-93b9-21d8ed2c9e1f', embedding=None, metadata={'page_label': '14', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7f600ebd96007239152997c4723f18cad5e84d36cab01dde2b8a451114a18fbd', text='0 10 20 30\\nLayer100101Activation valuesq_proj activation values\\n0 10 20 30\\nLayer101\\n100101Activation valueso_proj activation values\\n0 10 20 30\\nLayer100101Activation valuesup_proj activation values\\n0 10 20 30\\nLayer101\\n100101102103Activation valuesdown_proj activation valuesFigure 5: Visualization of activation distribution of oprojanddown projon LLaMA-2-7B.\\n0 10 20 30\\nLayer100101Activation valuesq_proj activation values\\n0 10 20 30\\nLayer101\\n100101Activation valueso_proj activation values\\n0 10 20 30\\nLayer100101Activation valuesup_proj activation values\\n0 10 20 30\\nLayer101\\n100101102103Activation valuesdown_proj activation values\\nFigure 6: Visualization of activation distribution of oprojanddown projon LLaMA-2-13B.\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9b1bb037-6083-4982-b06e-cde74f7c3df3', embedding=None, metadata={'page_label': '15', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='9d587f825f723405ada97b7946f9fc5c078e6f14bd9006dab12cef24a908f2a6', text='0 10 20 30 40 50 60 70\\nLayer100101Activation valuesq_proj activation values\\n0 10 20 30 40 50 60 70\\nLayer101\\n100101Activation valueso_proj activation values\\n0 10 20 30 40 50 60 70\\nLayer100101Activation valuesup_proj activation values\\n0 10 20 30 40 50 60 70\\nLayer101\\n100101102103104Activation valuesdown_proj activation valuesFigure 7: Visualization of activation distribution of oprojanddown projon LLaMA-2-70B.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9921e6eb-5b9b-41b2-aa69-829db1247a3c', embedding=None, metadata={'page_label': '16', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='cd593edae1d3b5bca7e3a02be3c31ee691f7b7a7b7c475b6e697a01d86369b79', text='0 10 20 30\\nLayer100101Activation valuesq_proj activation values\\n0 10 20 30\\nLayer101\\n100101Activation valueso_proj activation values\\n0 10 20 30\\nLayer100101Activation valuesup_proj activation values\\n0 10 20 30\\nLayer101\\n100101102103Activation valuesdown_proj activation valuesFigure 8: Visualization of activation distribution of oprojanddown projon LLaMA-7B.\\n0 10 20 30\\nLayer100101Activation valuesq_proj activation values\\n0 10 20 30\\nLayer101\\n100101Activation valueso_proj activation values\\n0 10 20 30\\nLayer100101Activation valuesup_proj activation values\\n0 10 20 30\\nLayer101\\n100101102103Activation valuesdown_proj activation values\\nFigure 9: Visualization of activation distribution of oprojanddown projon LLaMA-13B.\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9e84271d-0c00-4c83-a483-529afe7969e1', embedding=None, metadata={'page_label': '17', 'file_name': 'FPTQ_Fine-grained_Post-Training_Quantization_for_Large_Language_Models.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='cafb29c8548a10b35ff975615c22aed40bc5b0154c785afd649b32246f288cad', text='0 10 20 30 40 50 60 70\\nLayer100101Activation valuesq_proj activation values\\n0 10 20 30 40 50 60 70\\nLayer101\\n100101Activation valueso_proj activation values\\n0 10 20 30 40 50 60 70\\nLayer100101Activation valuesup_proj activation values\\n0 10 20 30 40 50 60 70\\nLayer101\\n100101102103Activation valuesdown_proj activation valuesFigure 10: Visualization of activation distribution of oprojanddown projon LLaMA-65B.\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='20763766-e262-4808-b4b9-6daa63601c86', embedding=None, metadata={'page_label': '1', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2757394fd5ff3db303cb5510d55806a8984a6ea979e95cd335b4a2b86f5ccb0e', text='Learning Transferable Visual Models From Natural Language Supervision\\nAlec Radford* 1Jong Wook Kim* 1Chris Hallacy1Aditya Ramesh1Gabriel Goh1Sandhini Agarwal1\\nGirish Sastry1Amanda Askell1Pamela Mishkin1Jack Clark1Gretchen Krueger1Ilya Sutskever1\\nAbstract\\nState-of-the-art computer vision systems are\\ntrained to predict a ﬁxed set of predetermined\\nobject categories. This restricted form of super-\\nvision limits their generality and usability since\\nadditional labeled data is needed to specify any\\nother visual concept. Learning directly from raw\\ntext about images is a promising alternative which\\nleverages a much broader source of supervision.\\nWe demonstrate that the simple pre-training task\\nof predicting which caption goes with which im-\\nage is an efﬁcient and scalable way to learn SOTA\\nimage representations from scratch on a dataset\\nof 400 million (image, text) pairs collected from\\nthe internet. After pre-training, natural language\\nis used to reference learned visual concepts (or\\ndescribe new ones) enabling zero-shot transfer\\nof the model to downstream tasks. We study\\nthe performance of this approach by benchmark-\\ning on over 30 different existing computer vi-\\nsion datasets, spanning tasks such as OCR, ac-\\ntion recognition in videos, geo-localization, and\\nmany types of ﬁne-grained object classiﬁcation.\\nThe model transfers non-trivially to most tasks\\nand is often competitive with a fully supervised\\nbaseline without the need for any dataset spe-\\nciﬁc training. For instance, we match the ac-\\ncuracy of the original ResNet-50 on ImageNet\\nzero-shot without needing to use any of the 1.28\\nmillion training examples it was trained on. We\\nrelease our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP .\\n1. Introduction and Motivating Work\\nPre-training methods which learn directly from raw text\\nhave revolutionized NLP over the last few years (Dai &\\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\\nford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\\n*Equal contribution1OpenAI, San Francisco, CA 94110, USA.\\nCorrespondence to: <{alec, jongwook}@openai.com >.Task-agnostic objectives such as autoregressive and masked\\nlanguage modeling have scaled across many orders of mag-\\nnitude in compute, model capacity, and data, steadily im-\\nproving capabilities. The development of “text-to-text” as\\na standardized input-output interface (McCann et al., 2018;\\nRadford et al., 2019; Raffel et al., 2019) has enabled task-\\nagnostic architectures to zero-shot transfer to downstream\\ndatasets removing the need for specialized output heads or\\ndataset speciﬁc customization. Flagship systems like GPT-3\\n(Brown et al., 2020) are now competitive across many tasks\\nwith bespoke models while requiring little to no dataset\\nspeciﬁc training data.\\nThese results suggest that the aggregate supervision acces-\\nsible to modern pre-training methods within web-scale col-\\nlections of text surpasses that of high-quality crowd-labeled\\nNLP datasets. However, in other ﬁelds such as computer\\nvision it is still standard practice to pre-train models on\\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\\nCould scalable pre-training methods which learn directly\\nfrom web text result in a similar breakthrough in computer\\nvision? Prior work is encouraging.\\nOver 20 years ago Mori et al. (1999) explored improving\\ncontent based image retrieval by training a model to pre-\\ndict the nouns and adjectives in text documents paired with\\nimages. Quattoni et al. (2007) demonstrated it was possi-\\nble to learn more data efﬁcient image representations via\\nmanifold learning in the weight space of classiﬁers trained\\nto predict words in captions associated with images. Sri-\\nvastava & Salakhutdinov (2012) explored deep represen-\\ntation learning by training multimodal Deep Boltzmann\\nMachines on top of low-level image and text tag features.\\nJoulin et al. (2016) modernized this line of work and demon-\\nstrated that CNNs trained to predict words in image cap-\\ntions learn useful image representations. They converted\\nthe title, description, and hashtag metadata of images in the\\nYFCC100M dataset (Thomee et al., 2016) into a bag-of-\\nwords multi-label classiﬁcation task and showed that pre-\\ntraining AlexNet (Krizhevsky et al., 2012) to predict these\\nlabels learned representations which preformed similarly\\nto ImageNet-based pre-training on transfer tasks. Li et al.\\n(2017) then extended this approach to predicting phrase n-\\ngrams in addition to individual words and demonstrated the\\nability of their system to zero-shot transfer to other imagearXiv:2103.00020v1  [cs.CV]  26 Feb 2021', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c478cf45-ddcc-4c33-b3cc-08aeb81cc488', embedding=None, metadata={'page_label': '2', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='04de8d491fd36d1d9a5f309817f1125e2aecd31dbccd6426cd382a066cbb8099', text='Learning Transferable Visual Models From Natural Language Supervision 2\\nI1 ·T2 I1 ·T3…\\nI2 ·T1 I2 ·T3…\\nI 3 ·T 1 I 3 ·T 2 …\\n⋮ ⋮ ⋮I1 ·T1\\nI2 ·T2\\nI 3 ·T 3(1) Contrastive pre-training\\nImage\\nEncoderT ext\\nEncoderPepper\\tthe\\naussie\\tpupPepper\\tthe\\naussie\\tpupPepper\\tthe\\naussie\\tpupPepper\\tthe\\naussie\\tpup\\nT1 T2 T3…\\nI1\\nI2\\nI 3\\n⋮(2) Create dataset classiﬁer from label text\\nplane\\ncar\\ndog\\n⋮\\nbirdA\\tphoto\\tof\\na\\t{object} .\\n⋮T ext\\nEncoder\\nT1 T2 T3 TN…\\n(3) Use for zero-shot prediction\\nImage\\nEncoderI1 I1 ·T2 I1 ·TN I1 ·T1…\\n…\\nA\\tphoto\\tof\\n\\ta\\tdog.TN\\nIN ·T1 IN ·T2 IN ·T3I1 ·TN\\nI2 ·TN\\nI 3 ·T N\\n⋮\\n… IN…\\n⋮ ⋱\\nIN ·TNI1 ·T3\\nFigure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classiﬁer to predict\\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classiﬁer by embedding the names or descriptions of the\\ntarget dataset’s classes.\\nclassiﬁcation datasets by scoring target classes based on\\ntheir dictionary of learned visual n-grams and predicting the\\none with the highest score. Adopting more recent architec-\\ntures and pre-training approaches, VirTex (Desai & Johnson,\\n2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-\\nVIRT (Zhang et al., 2020) have recently demonstrated the\\npotential of transformer-based language modeling, masked\\nlanguage modeling, and contrastive objectives to learn im-\\nage representations from text.\\nWhile exciting as proofs of concept, using natural language\\nsupervision for image representation learning is still rare.\\nThis is likely because demonstrated performance on com-\\nmon benchmarks is much lower than alternative approaches.\\nFor example, Li et al. (2017) reach only 11.5% accuracy\\non ImageNet in a zero-shot setting. This is well below the\\n88.4% accuracy of the current state of the art (Xie et al.,\\n2020). It is even below the 50% accuracy of classic com-\\nputer vision approaches (Deng et al., 2012). Instead, more\\nnarrowly scoped but well-targeted uses of weak supervision\\nhave improved performance. Mahajan et al. (2018) showed\\nthat predicting ImageNet-related hashtags on Instagram im-\\nages is an effective pre-training task. When ﬁne-tuned to\\nImageNet these pre-trained models increased accuracy by\\nover 5% and improved the overall state of the art at the time.\\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\\nalso demonstrated large gains on a broader set of transfer\\nbenchmarks by pre-training models to predict the classes of\\nthe noisily labeled JFT-300M dataset.\\nThis line of work represents the current pragmatic middle\\nground between learning from a limited amount of super-\\nvised “gold-labels” and learning from practically unlimited\\namounts of raw text. However, it is not without compro-mises. Both works carefully design, and in the process limit,\\ntheir supervision to 1000 and 18291 classes respectively.\\nNatural language is able to express, and therefore supervise,\\na much wider set of visual concepts through its general-\\nity. Both approaches also use static softmax classiﬁers to\\nperform prediction and lack a mechanism for dynamic out-\\nputs. This severely curtails their ﬂexibility and limits their\\n“zero-shot” capabilities.\\nA crucial difference between these weakly supervised mod-\\nels and recent explorations of learning image representations\\ndirectly from natural language is scale. While Mahajan et al.\\n(2018) and Kolesnikov et al. (2019) trained their models for\\naccelerator years on millions to billions of images, VirTex,\\nICMLM, and ConVIRT trained for accelerator days on one\\nto two hundred thousand images. In this work, we close\\nthis gap and study the behaviors of image classiﬁers trained\\nwith natural language supervision at large scale. Enabled\\nby the large amounts of publicly available data of this form\\non the internet, we create a new dataset of 400 million (im-\\nage, text) pairs and demonstrate that a simpliﬁed version of\\nConVIRT trained from scratch, which we call CLIP, for Con-\\ntrastive Language-Image Pre-training, is an efﬁcient method\\nof learning from natural language supervision. We study\\nthe scalability of CLIP by training a series of eight models\\nspanning almost 2 orders of magnitude of compute and ob-\\nserve that transfer performance is a smoothly predictable\\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\\n2020). We ﬁnd that CLIP, similar to the GPT family, learns\\nto perform a wide set of tasks during pre-training including\\nOCR, geo-localization, action recognition, and many others.\\nWe measure this by benchmarking the zero-shot transfer\\nperformance of CLIP on over 30 existing datasets and ﬁnd', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d4d1464e-5f81-4a03-87c6-6784cc0e58eb', embedding=None, metadata={'page_label': '3', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='949b8d8efb88c26e74c3bad1bbb342b3208600fdfb0eb9b7cb233285ffba112d', text='Learning Transferable Visual Models From Natural Language Supervision 3\\n2M33M 67M 134M 268M 400M\\n# of images processed0510152025303540Zero-Shot ImageNet Accuracy\\n3X efficiency 4X efficiency\\nBag of Words Contrastive (CLIP)\\nBag of Words Prediction\\nTransformer Language Model\\nFigure 2. CLIP is much more efﬁcient at zero-shot transfer\\nthan our image caption baseline. Although highly expressive,\\nwe found that transformer-based language models are relatively\\nweak at zero-shot ImageNet classiﬁcation. Here, we see that it\\nlearns 3x slower than a baseline which predicts a bag-of-words\\n(BoW) encoding of the text (Joulin et al., 2016). Swapping the\\nprediction objective for the contrastive objective of CLIP further\\nimproves efﬁciency another 4x.\\nit can be competitive with prior task-speciﬁc supervised\\nmodels. We also conﬁrm these ﬁndings with linear-probe\\nrepresentation learning analysis and show that CLIP out-\\nperforms the best publicly available ImageNet model while\\nalso being more computationally efﬁcient. We additionally\\nﬁnd that zero-shot CLIP models are much more robust than\\nequivalent accuracy supervised ImageNet models which\\nsuggests that zero-shot evaluation of task-agnostic models is\\nmuch more representative of a model’s capability. These re-\\nsults have signiﬁcant policy and ethical implications, which\\nwe consider in Section 7.\\n2. Approach\\n2.1. Natural Language Supervision\\nAt the core of our approach is the idea of learning percep-\\ntion from supervision contained in natural language. As\\ndiscussed in the introduction, this is not at all a new idea,\\nhowever terminology used to describe work in this space\\nis varied, even seemingly contradictory, and stated motiva-\\ntions are diverse. Zhang et al. (2020), Gomez et al. (2017),\\nJoulin et al. (2016), and Desai & Johnson (2020) all intro-\\nduce methods which learn visual representations from text\\npaired with images but describe their approaches as unsuper-\\nvised, self-supervised, weakly supervised, and supervised\\nrespectively.\\nWe emphasize that what is common across this line of work\\nis not any of the details of the particular methods used but\\nthe appreciation of natural language as a training signal. All\\nthese approaches are learning from natural language super-vision . Although early work wrestled with the complexity\\nof natural language when using topic model and n-gram\\nrepresentations, improvements in deep contextual represen-\\ntation learning suggest we now have the tools to effectively\\nleverage this abundant source of supervision (McCann et al.,\\n2017).\\nLearning from natural language has several potential\\nstrengths over other training methods. It’s much easier\\nto scale natural language supervision compared to standard\\ncrowd-sourced labeling for image classiﬁcation since it does\\nnot require annotations to be in a classic “machine learning\\ncompatible format” such as the canonical 1-of-N majority\\nvote “gold label”. Instead, methods which work on natural\\nlanguage can learn passively from the supervision contained\\nin the vast amount of text on the internet. Learning from\\nnatural language also has an important advantage over most\\nunsupervised or self-supervised learning approaches in that\\nit doesn’t “just” learn a representation but also connects that\\nrepresentation to language which enables ﬂexible zero-shot\\ntransfer. In the following subsections, we detail the speciﬁc\\napproach we settled on.\\n2.2. Creating a Sufﬁciently Large Dataset\\nExisting work has mainly used three datasets, MS-COCO\\n(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\\nYFCC100M (Thomee et al., 2016). While MS-COCO and\\nVisual Genome are high quality crowd-labeled datasets, they\\nare small by modern standards with approximately 100,000\\ntraining photos each. By comparison, other computer vision\\nsystems are trained on up to 3.5 billion Instagram photos\\n(Mahajan et al., 2018). YFCC100M, at 100 million photos,\\nis a possible alternative, but the metadata for each image is\\nsparse and of varying quality. Many images use automati-\\ncally generated ﬁlenames like 20160716 113957.JPG\\nas “titles” or contain “descriptions” of camera exposure\\nsettings. After ﬁltering to keep only images with natural\\nlanguage titles and/or descriptions in English, the dataset\\nshrunk by a factor of 6 to only 15 million photos. This is\\napproximately the same size as ImageNet.\\nA major motivation for natural language supervision is the\\nlarge quantities of data of this form available publicly on the\\ninternet. Since existing datasets do not adequately reﬂect\\nthis possibility, considering results only on them would un-\\nderestimate the potential of this line of research. To address\\nthis, we constructed a new dataset of 400 million (image,\\ntext) pairs collected form a variety of publicly available\\nsources on the Internet. To attempt to cover as broad a set\\nof visual concepts as possible, we search for (image, text)\\npairs as part of the construction process whose text includes\\none of a set of 500,000 queries.1We approximately class\\n1The base query list is all words occurring at least 100 times in\\nthe English version of Wikipedia. This is augmented with bi-grams', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='012a975f-d7f5-4aaa-9cdf-cc642e5abe3a', embedding=None, metadata={'page_label': '4', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bcb7f0f8a25a26e35403343adcf90813859773561897d44765b64e6b4b038ce9', text='Learning Transferable Visual Models From Natural Language Supervision 4\\nbalance the results by including up to 20,000 (image, text)\\npairs per query. The resulting dataset has a similar total\\nword count as the WebText dataset used to train GPT-2. We\\nrefer to this dataset as WIT for WebImageText.\\n2.3. Selecting an Efﬁcient Pre-Training Method\\nState-of-the-art computer vision systems use very large\\namounts of compute. Mahajan et al. (2018) required 19\\nGPU years to train their ResNeXt101-32x48d and Xie et al.\\n(2020) required 33 TPUv3 core-years to train their Noisy\\nStudent EfﬁcientNet-L2. When considering that both these\\nsystems were trained to predict only 1000 ImageNet classes,\\nthe task of learning an open set of visual concepts from\\nnatural language seems daunting. In the course of our ef-\\nforts, we found training efﬁciency was key to successfully\\nscaling natural language supervision and we selected our\\nﬁnal pre-training method based on this metric.\\nOur initial approach, similar to VirTex, jointly trained an\\nimage CNN and text transformer from scratch to predict the\\ncaption of an image. However, we encountered difﬁculties\\nefﬁciently scaling this method. In Figure 2 we show that a\\n63 million parameter transformer language model, which\\nalready uses twice the compute of its ResNet-50 image\\nencoder, learns to recognize ImageNet classes three times\\nslower than a much simpler baseline that predicts a bag-of-\\nwords encoding of the same text.\\nBoth these approaches share a key similarity. They try to pre-\\ndict the exact words of the text accompanying each image.\\nThis is a difﬁcult task due to the wide variety of descriptions,\\ncomments, and related text that co-occur with images. Re-\\ncent work in contrastive representation learning for images\\nhas found that contrastive objectives can learn better repre-\\nsentations than their equivalent predictive objective (Tian\\net al., 2019). Other work has found that although generative\\nmodels of images can learn high quality image representa-\\ntions, they require over an order of magnitude more compute\\nthan contrastive models with the same performance (Chen\\net al., 2020a). Noting these ﬁndings, we explored training\\na system to solve the potentially easier proxy task of pre-\\ndicting only which text as a whole is paired with which\\nimage and not the exact words of that text. Starting with\\nthe same bag-of-words encoding baseline, we swapped the\\npredictive objective for a contrastive objective in Figure 2\\nand observed a further 4x efﬁciency improvement in the rate\\nof zero-shot transfer to ImageNet.\\nGiven a batch of N(image, text) pairs, CLIP is trained to\\npredict which of the N×Npossible (image, text) pairings\\nacross a batch actually occurred. To do this, CLIP learns a\\nwith high pointwise mutual information as well as the names of\\nall Wikipedia articles above a certain search volume. Finally all\\nWordNet synsets not already in the query list are added.multi-modal embedding space by jointly training an image\\nencoder and text encoder to maximize the cosine similar-\\nity of the image and text embeddings of the Nreal pairs\\nin the batch while minimizing the cosine similarity of the\\nembeddings of the N2−Nincorrect pairings. We opti-\\nmize a symmetric cross entropy loss over these similarity\\nscores. In Figure 3 we include pseudocode of the core of an\\nimplementation of CLIP. To our knowledge this batch con-\\nstruction technique and objective was ﬁrst introduced in the\\narea of deep metric learning as the multi-class N-pair loss\\nSohn (2016), was popularized for contrastive representation\\nlearning by Oord et al. (2018) as the InfoNCE loss, and was\\nrecently adapted for contrastive (text, image) representation\\nlearning in the domain of medical imaging by Zhang et al.\\n(2020).\\nDue to the large size of our pre-training dataset, over-ﬁtting\\nis not a major concern and the details of training CLIP are\\nsimpliﬁed compared to the implementation of Zhang et al.\\n(2020). We train CLIP from scratch without initializing the\\nimage encoder with ImageNet weights or the text encoder\\nwith pre-trained weights. We do not use the non-linear\\nprojection between the representation and the contrastive\\nembedding space, a change which was introduced by Bach-\\nman et al. (2019) and popularized by Chen et al. (2020b).\\nWe instead use only a linear projection to map from each en-\\ncoder’s representation to the multi-modal embedding space.\\nWe did not notice a difference in training efﬁciency between\\nthe two versions and speculate that non-linear projections\\nmay be co-adapted with details of current image only in\\nself-supervised representation learning methods. We also\\nremove the text transformation function tufrom Zhang et al.\\n(2020) which samples a single sentence at uniform from\\nthe text since many of the (image, text) pairs in CLIP’s pre-\\ntraining dataset are only a single sentence. We also simplify\\nthe image transformation function tv. A random square\\ncrop from resized images is the only data augmentation\\nused during training. Finally, the temperature parameter\\nwhich controls the range of the logits in the softmax, τ, is\\ndirectly optimized during training as a log-parameterized\\nmultiplicative scalar to avoid turning as a hyper-parameter.\\n2.4. Choosing and Scaling a Model\\nWe consider two different architectures for the image en-\\ncoder. For the ﬁrst, we use ResNet-50 (He et al., 2016a)\\nas the base architecture for the image encoder due to its\\nwidespread adoption and proven performance. We make sev-\\neral modiﬁcations to the original version using the ResNet-\\nD improvements from He et al. (2019) and the antialiased\\nrect-2 blur pooling from Zhang (2019). We also replace\\nthe global average pooling layer with an attention pooling\\nmechanism. The attention pooling is implemented as a sin-\\ngle layer of “transformer-style” multi-head QKV attention\\nwhere the query is conditioned on the global average-pooled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='343beffa-d437-488e-a899-a46c0cd25db6', embedding=None, metadata={'page_label': '5', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='41b3e51448ab3e4502dceb97aed2cb4e82d18ab082886baeec182d48884f08de', text='Learning Transferable Visual Models From Natural Language Supervision 5\\n# image_encoder - ResNet or Vision Transformer \\n# text_encoder  - CBOW or Text Transformer \\n# I[n, h, w, c] - minibatch of aligned images \\n# T[n, l]       - minibatch of aligned texts \\n# W_i[d_i, d_e] - learned proj of image to embed \\n# W_t[d_t, d_e] - learned proj of text to embed \\n# t             - learned temperature parameter \\n# extract feature representations of each modality \\nI_f = image_encoder(I) #[n, d_i] \\nT_f = text_encoder(T)  #[n, d_t] \\n# joint multimodal embedding [n, d_e] \\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1) \\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1) \\n# scaled pairwise cosine similarities [n, n] \\nlogits = np.dot(I_e, T_e.T) * np.exp(t) \\n# symmetric loss function \\nlabels = np.arange(n) \\nloss_i = cross_entropy_loss(logits, labels, axis=0) \\nloss_t = cross_entropy_loss(logits, labels, axis=1) \\nloss   = (loss_i + loss_t)/2 \\nFigure 3. Numpy-like pseudocode for the core of an implementa-\\ntion of CLIP.\\nrepresentation of the image. For the second architecture, we\\nexperiment with the recently introduced Vision Transformer\\n(ViT) (Dosovitskiy et al., 2020). We closely follow their\\nimplementation with only the minor modiﬁcation of adding\\nan additional layer normalization to the combined patch\\nand position embeddings before the transformer and use a\\nslightly different initialization scheme.\\nThe text encoder is a Transformer (Vaswani et al., 2017)\\nwith the architecture modiﬁcations described in Radford\\net al. (2019). As a base size we use a 63M-parameter 12-\\nlayer 512-wide model with 8 attention heads. The trans-\\nformer operates on a lower-cased byte pair encoding (BPE)\\nrepresentation of the text with a 49,152 vocab size (Sen-\\nnrich et al., 2015). For computational efﬁciency, the max\\nsequence length was capped at 76. The text sequence is\\nbracketed with [SOS] and[EOS] tokens and the activa-\\ntions of the highest layer of the transformer at the [EOS]\\ntoken are treated as the feature representation of the text\\nwhich is layer normalized and then linearly projected into\\nthe multi-modal embedding space. Masked self-attention\\nwas used in the text encoder to preserve the ability to ini-\\ntialize with a pre-trained language model or add language\\nmodeling as an auxiliary objective, though exploration of\\nthis is left as future work.\\nWhile previous computer vision research has often scaled\\nmodels by increasing the width (Mahajan et al., 2018) or\\ndepth (He et al., 2016a) in isolation, for the ResNet image\\nencoders we adapt the approach of Tan & Le (2019) which\\nfound that allocating additional compute across all of width,\\ndepth, and resolution outperforms only allocating it to onlyone dimension of the model. While Tan & Le (2019) tune\\nthe ratio of compute allocated to each dimension for their\\nEfﬁcientNet architecture, we use a simple baseline of allo-\\ncating additional compute equally to increasing the width,\\ndepth, and resolution of the model. For the text encoder, we\\nonly scale the width of the model to be proportional to the\\ncalculated increase in width of the ResNet and do not scale\\nthe depth at all, as we found CLIP’s performance to be less\\nsensitive to the capacity of the text encoder.\\n2.5. Training\\nWe train a series of 5 ResNets and 3 Vision Transformers.\\nFor the ResNets we train a ResNet-50, a ResNet-101, and\\nthen 3 more which follow EfﬁcientNet-style model scaling\\nand use approximately 4x, 16x, and 64x the compute of a\\nResNet-50. They are denoted as RN50x4, RN50x16, and\\nRN50x64 respectively. For the Vision Transformers we\\ntrain a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all\\nmodels for 32 epochs. We use the Adam optimizer (Kingma\\n& Ba, 2014) with decoupled weight decay regularization\\n(Loshchilov & Hutter, 2017) applied to all weights that are\\nnot gains or biases, and decay the learning rate using a\\ncosine schedule (Loshchilov & Hutter, 2016). Initial hyper-\\nparameters were set using a combination of grid searches,\\nrandom search, and manual tuning on the baseline ResNet-\\n50 model when trained for 1 epoch. Hyper-parameters were\\nthen adapted heuristically for larger models due to compu-\\ntational constraints. The learnable temperature parameter\\nτwas initialized to the equivalent of 0.07 from (Wu et al.,\\n2018) and clipped to prevent scaling the logits by more\\nthan 100 which we found necessary to prevent training in-\\nstability. We use a very large minibatch size of 32,768.\\nMixed-precision (Micikevicius et al., 2017) was used to ac-\\ncelerate training and save memory. To save additional mem-\\nory, gradient checkpointing (Griewank & Walther, 2000;\\nChen et al., 2016), half-precision Adam statistics (Dhariwal\\net al., 2020), and half-precision stochastically rounded text\\nencoder weights were used. The calculation of embedding\\nsimilarities was also sharded with individual GPUs comput-\\ning only the subset of the pairwise similarities necessary for\\ntheir local batch of embeddings. The largest ResNet model,\\nRN50x64, took 18 days to train on 592 V100 GPUs while\\nthe largest Vision Transformer took 12 days on 256 V100\\nGPUs. For the ViT-L/14 we also pre-train at a higher 336\\npixel resolution for one additional epoch to boost perfor-\\nmance similar to FixRes (Touvron et al., 2019). We denote\\nthis model as ViT-L/14@336px. Unless otherwise speciﬁed,\\nall results reported in this paper as “CLIP” use this model\\nwhich we found to perform best.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4c4cbbfb-d3e5-4687-b352-8f3cc1fe07d5', embedding=None, metadata={'page_label': '6', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5f7831d05812572f47d248c2cd4d37ca7e4f53720ba49708534a20e625620fce', text='Learning Transferable Visual Models From Natural Language Supervision 6\\n3. Experiments\\n3.1. Zero-Shot Transfer\\n3.1.1. M OTIVATION\\nIn computer vision, zero-shot learning usually refers to the\\nstudy of generalizing to unseen object categories in image\\nclassiﬁcation (Lampert et al., 2009). We instead use the\\nterm in a broader sense and study generalization to unseen\\ndatasets. We motivate this as a proxy for performing un-\\nseen tasks, as aspired to in the zero-data learning paper of\\nLarochelle et al. (2008). While much research in the ﬁeld of\\nunsupervised learning focuses on the representation learn-\\ningcapabilities of machine learning systems, we motivate\\nstudying zero-shot transfer as a way of measuring the task-\\nlearning capabilities of machine learning systems. In this\\nview, a dataset evaluates performance on a task on a spe-\\nciﬁc distribution. However, many popular computer vision\\ndatasets were created by the research community primarily\\nas benchmarks to guide the development of generic image\\nclassiﬁcation methods rather than measuring performance\\non a speciﬁc task. While it is reasonable to say that the\\nSVHN dataset measures the task of street number transcrip-\\ntion on the distribution of Google Street View photos, it is\\nunclear what “real” task the CIFAR-10 dataset measures.\\nIt is clear, however, what distribution CIFAR-10 is drawn\\nfrom - TinyImages (Torralba et al., 2008). On these kinds of\\ndatasets, zero-shot transfer is more an evaluation of CLIP’s\\nrobustness to distribution shift and domain generalization\\nrather than task generalization. Please see Section 3.3 for\\nanalysis focused on this.\\nTo our knowledge, Visual N-Grams (Li et al., 2017) ﬁrst\\nstudied zero-shot transfer to existing image classiﬁcation\\ndatasets in the manner described above. It is also the only\\nother work we are aware of that has studied zero-shot trans-\\nfer to standard image classiﬁcation datasets using a gener-\\nically pre-trained model and serves as the best reference\\npoint for contextualizing CLIP. Their approach learns the\\nparameters of a dictionary of 142,806 visual n-grams (span-\\nning 1- to 5- grams) and optimizes these n-grams using a\\ndifferential version of Jelinek-Mercer smoothing to maxi-\\nmize the probability of all text n-grams for a given image.\\nIn order to perform zero-shot transfer, they ﬁrst convert the\\ntext of each of the dataset’s class names into its n-gram\\nrepresentation and then compute its probability according\\nto their model, predicting the one with the highest score.\\nOur focus on studying zero-shot transfer as an evaluation of\\ntask learning is inspired by work demonstrating task learn-\\ning in the ﬁeld of NLP. To our knowledge Liu et al. (2018)\\nﬁrst identiﬁed task learning as an “unexpected side-effect”\\nwhen a language model trained to generate Wikipedia ar-\\nticles learned to reliably transliterate names between lan-\\nguages. While GPT-1 (Radford et al., 2018) focused on pre-training as a transfer learning method to improve supervised\\nﬁne-tuning, it also included an ablation study demonstrat-\\ning that the performance of four heuristic zero-shot transfer\\nmethods improved steadily over the course of pre-training,\\nwithout any supervised adaption. This analysis served as the\\nbasis for GPT-2 (Radford et al., 2019) which focused exclu-\\nsively on studying the task-learning capabilities of language\\nmodels via zero-shot transfer.\\n3.1.2. U SING CLIP FOR ZERO-SHOT TRANSFER\\nCLIP is pre-trained to predict if an image and a text snippet\\nare paired together in its dataset. To perform zero-shot clas-\\nsiﬁcation, we reuse this capability. For each dataset, we use\\nthe names of all the classes in the dataset as the set of poten-\\ntial text pairings and predict the most probable (image, text)\\npair according to CLIP. In a bit more detail, we ﬁrst compute\\nthe feature embedding of the image and the feature embed-\\nding of the set of possible texts by their respective encoders.\\nThe cosine similarity of these embeddings is then calculated,\\nscaled by a temperature parameter τ, and normalized into a\\nprobability distribution via a softmax. Note that this predic-\\ntion layer is a multinomial logistic regression classiﬁer with\\nL2-normalized inputs, L2-normalized weights, no bias, and\\ntemperature scaling. When interpreted this way, the image\\nencoder is the computer vision backbone which computes a\\nfeature representation for the image and the text encoder is a\\nhypernetwork (Ha et al., 2016) which generates the weights\\nof a linear classiﬁer based on the text specifying the visual\\nconcepts that the classes represent. Lei Ba et al. (2015) ﬁrst\\nintroduced a zero-shot image classiﬁer of this form while\\nthe idea of generating a classiﬁer from natural language\\ndates back to at least Elhoseiny et al. (2013). Continuing\\nwith this interpretation, every step of CLIP pre-training can\\nbe viewed as optimizing the performance of a randomly\\ncreated proxy to a computer vision dataset which contains 1\\nexample per class and has 32,768 total classes deﬁned via\\nnatural language descriptions. For zero-shot evaluation, we\\ncache the zero-shot classiﬁer once it has been computed by\\nthe text encoder and reuse it for all subsequent predictions.\\nThis allows the cost of generating it to be amortized across\\nall the predictions in a dataset.\\n3.1.3. I NITIAL COMPARISON TO VISUAL N-G RAMS\\nIn Table 1 we compare Visual N-Grams to CLIP. The best\\nCLIP model improves accuracy on ImageNet from a proof\\nof concept 11.5% to 76.2% and matches the performance\\nof the original ResNet-50 despite using none of the 1.28\\nmillion crowd-labeled training examples available for this\\ndataset. Additionally, the top-5 accuracy of CLIP models\\nare noticeably higher than their top-1, and this model has a\\n95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,\\n2016). The ability to match the performance of a strong,\\nfully supervised baselines in a zero-shot setting suggests', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e5a78bd3-3eea-4f1c-a6c0-f1ef1db7986e', embedding=None, metadata={'page_label': '7', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='65d5b652370902d5babc9c4d069a3662e6760deb7fd4ae3dda3bab4071bf9400', text='Learning Transferable Visual Models From Natural Language Supervision 7\\naYahoo ImageNet SUN\\nVisual N-Grams 72.4 11.5 23.0\\nCLIP 98.4 76.2 58.5\\nTable 1. Comparing CLIP to prior zero-shot transfer image classi-\\nﬁcation results. CLIP improves performance on all three datasets\\nby a large amount. This improvement reﬂects many differences\\nin the 4 years since the development of Visual N-Grams (Li et al.,\\n2017).\\nCLIP is a signiﬁcant step towards ﬂexible and practical\\nzero-shot computer vision classiﬁers. As mentioned above,\\nthe comparison to Visual N-Grams is meant for contextu-\\nalizing the performance of CLIP and should not be inter-\\npreted as a direct methods comparison between CLIP and\\nVisual N-Grams as many performance relevant differences\\nbetween the two systems were not controlled for. For in-\\nstance, we train on a dataset that is 10x larger, use a vision\\nmodel that requires nearly 100x more compute per predic-\\ntion, likely used over 1000x their training compute, and\\nuse a transformer-based model which did not exist when\\nVisual N-Grams was published. As a closer comparison, we\\ntrained a CLIP ResNet-50 on the same YFCC100M dataset\\nthat Visual N-Grams was trained on and found it matched\\ntheir reported ImageNet performance within a V100 GPU\\nday. This baseline was also trained from scratch instead of\\nbeing initialized from pre-trained ImageNet weights as in\\nVisual N-Grams.\\nCLIP also outperforms Visual N-Grams on the other 2 re-\\nported datasets. On aYahoo, CLIP achieves a 95% reduction\\nin the number of errors, and on SUN, CLIP more than dou-\\nbles the accuracy of Visual N-Grams. To conduct a more\\ncomprehensive analysis and stress test, we implement a\\nmuch larger evaluation suite detailed in Appendix A. In\\ntotal we expand from the 3 datasets reported in Visual N-\\nGrams to include over 30 datasets and compare to over 50\\nexisting computer vision systems to contextualize results.\\n3.1.4. P ROMPT ENGINEERING AND ENSEMBLING\\nMost standard image classiﬁcation datasets treat the infor-\\nmation naming or describing classes which enables natural\\nlanguage based zero-shot transfer as an afterthought. The\\nvast majority of datasets annotate images with just a numeric\\nid of the label and contain a ﬁle mapping these ids back to\\ntheir names in English. Some datasets, such as Flowers102\\nand GTSRB, don’t appear to include this mapping at all\\nin their released versions preventing zero-shot transfer en-\\ntirely.2For many datasets, we observed these labels may be\\n2Alec learned much more about ﬂower species and German\\ntrafﬁc signs over the course of this project than he originally antic-\\nipated.\\n6.1 9.9 21.5 75.3 265.9\\nModel GFLOPs455055606570Average Score (%)\\n4X efficiency gain5 point\\nimprovement\\nRN50RN101RN50x4RN50x16RN50x64\\nPrompt engineering and ensembling\\nContextless class names (Li et al. 2017)Figure 4. Prompt engineering and ensembling improve zero-\\nshot performance. Compared to the baseline of using contextless\\nclass names, prompt engineering and ensembling boost zero-shot\\nclassiﬁcation performance by almost 5 points on average across\\n36 datasets. This improvement is similar to the gain from using\\n4 times more compute with the baseline zero-shot method but is\\n“free” when amortized over many predictions.\\nchosen somewhat haphazardly and do not anticipate issues\\nrelated to zero-shot transfer which relies on task description\\nin order to transfer successfully.\\nA common issue is polysemy. When the name of a class\\nis the only information provided to CLIP’s text encoder it\\nis unable to differentiate which word sense is meant due to\\nthe lack of context. In some cases multiple meanings of the\\nsame word might be included as different classes in the same\\ndataset! This happens in ImageNet which contains both\\nconstruction cranes and cranes that ﬂy. Another example is\\nfound in classes of the Oxford-IIIT Pet dataset where the\\nword boxer is, from context, clearly referring to a breed of\\ndog, but to a text encoder lacking context could just as likely\\nrefer to a type of athlete.\\nAnother issue we encountered is that it’s relatively rare in\\nour pre-training dataset for the text paired with the image\\nto be just a single word. Usually the text is a full sentence\\ndescribing the image in some way. To help bridge this\\ndistribution gap, we found that using the prompt template\\n“A photo of a {label}.” to be a good default that\\nhelps specify the text is about the content of the image. This\\noften improves performance over the baseline of using only\\nthe label text. For instance, just using this prompt improves\\naccuracy on ImageNet by 1.3%.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='068f338d-c212-4fbb-b7e6-f5c7d2b56d19', embedding=None, metadata={'page_label': '8', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='de8c44fc9364031c2b6e8022a7ee62d29f966837b77114816db11324ef3e7a33', text='Learning Transferable Visual Models From Natural Language Supervision 8\\nSimilar to the “prompt engineering” discussion around GPT-\\n3 (Brown et al., 2020; Gao et al., 2020), we have also\\nobserved that zero-shot performance can be signiﬁcantly\\nimproved by customizing the prompt text to each task. A\\nfew, non exhaustive, examples follow. We found on several\\nﬁne-grained image classiﬁcation datasets that it helped to\\nspecify the category. For example on Oxford-IIIT Pets, us-\\ning “A photo of a {label}, a type of pet. ”\\nto help provide context worked well. Likewise, on Food101\\nspecifying a type of food and on FGVC Aircraft a type of\\naircraft helped too. For OCR datasets, we found that putting\\nquotes around the text or number to be recognized improved\\nperformance. Finally, we found that on satellite image classi-\\nﬁcation datasets it helped to specify that the images were of\\nthis form and we use variants of “ a satellite photo\\nof a{label}.”.\\nWe also experimented with ensembling over multiple zero-\\nshot classiﬁers as another way of improving performance.\\nThese classiﬁers are computed by using different context\\nprompts such as ‘ A photo of a big {label}” and\\n“A photo of a small {label}”. We construct the\\nensemble over the embedding space instead of probability\\nspace. This allows us to cache a single set of averaged text\\nembeddings so that the compute cost of the ensemble is the\\nsame as using a single classiﬁer when amortized over many\\npredictions. We’ve observed ensembling across many gen-\\nerated zero-shot classiﬁers to reliably improve performance\\nand use it for the majority of datasets. On ImageNet, we\\nensemble 80 different context prompts and this improves\\nperformance by an additional 3.5% over the single default\\nprompt discussed above. When considered together, prompt\\nengineering and ensembling improve ImageNet accuracy\\nby almost 5%. In Figure 4 we visualize how prompt engi-\\nneering and ensembling change the performance of a set of\\nCLIP models compared to the contextless baseline approach\\nof directly embedding the class name as done in Li et al.\\n(2017).\\n3.1.5. A NALYSIS OF ZERO-SHOT CLIP P ERFORMANCE\\nSince task-agnostic zero-shot classiﬁers for computer vision\\nhave been understudied, CLIP provides a promising oppor-\\ntunity to gain a better understanding of this type of model.\\nIn this section, we conduct a study of various properties of\\nCLIP’s zero-shot classiﬁers. As a ﬁrst question, we look\\nsimply at how well zero-shot classiﬁers perform. To con-\\ntextualize this, we compare to the performance of a simple\\noff-the-shelf baseline: ﬁtting a fully supervised, regularized,\\nlogistic regression classiﬁer on the features of the canonical\\nResNet-50. In Figure 5 we show this comparison across 27\\ndatasets. Please see Appendix A for details of datasets and\\nsetup.\\nZero-shot CLIP outperforms this baseline slightly more of-\\n40\\n 30\\n 20\\n 10\\n 0 10 20 30 40\\n Score (%)\\nZero-Shot CLIP vs. Linear Probe on ResNet50EuroSAT -37.1KITTI Distance -34.0PatchCamelyon -19.5GTSRB -18.4CLEVRCounts -18.2DTD -16.6Flowers102 -12.5RESISC45 -11.9FGVCAircraft -11.3MNIST -10.0Birdsnap -3.2+0.5 PascalVOC2007+1.1 OxfordPets+1.9 ImageNet+2.0 Caltech101+2.8 FER2013+3.0 STL10+3.0 CIFAR100+3.9 CIFAR10+6.7 HatefulMemes+7.7 UCF101+7.8 SUN397+12.4 SST2+14.5 Kinetics700+22.5 Food101+23.2 Country211+28.9 StanfordCarsFigure 5. Zero-shot CLIP is competitive with a fully super-\\nvised baseline. Across a 27 dataset eval suite, a zero-shot CLIP\\nclassiﬁer outperforms a fully supervised linear classiﬁer ﬁtted on\\nResNet-50 features on 16 datasets, including ImageNet.\\nten than not and wins on 16 of the 27 datasets. Looking at\\nindividual datasets reveals some interesting behavior. On\\nﬁne-grained classiﬁcation tasks, we observe a wide spread\\nin performance. On two of these datasets, Stanford Cars and\\nFood101, zero-shot CLIP outperforms logistic regression\\non ResNet-50 features by over 20% while on two others,\\nFlowers102 and FGVCAircraft, zero-shot CLIP underper-\\nforms by over 10%. On OxfordPets and Birdsnap, per-\\nformance is much closer. We suspect these difference are\\nprimarily due to varying amounts of per-task supervision\\nbetween WIT and ImageNet. On “general” object classiﬁca-\\ntion datasets such as ImageNet, CIFAR10/100, STL10, and\\nPascalVOC2007 performance is relatively similar with a\\nslight advantage for zero-shot CLIP in all cases. On STL10,\\nCLIP achieves 99.3% overall which appears to be a new\\nstate of the art despite not using any training examples. Zero-\\nshot CLIP signiﬁcantly outperforms a ResNet-50 on two\\ndatasets measuring action recognition in videos. On Kinet-\\nics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-\\nshot CLIP also outperforms a ResNet-50’s features by 7.7%\\non UCF101. We speculate this is due to natural language\\nproviding wider supervision for visual concepts involving\\nverbs, compared to the noun-centric object supervision in\\nImageNet.\\nLooking at where zero-shot CLIP notably underperforms,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5a861396-88a7-4818-949a-76e6c5b60829', embedding=None, metadata={'page_label': '9', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fb28991c7b9481dfe0f1dcf05607275be886ffb28021b5916a9c3492b5ad93b0', text='Learning Transferable Visual Models From Natural Language Supervision 9\\n012 4 8 16\\n# of labeled training examples per class30354045505560657075Average Score (%)Zero-Shot\\nCLIP\\nBiT-M (ImageNet-21K)Linear Probe CLIP\\nSimCLRv2\\nResNet50\\nFigure 6. Zero-shot CLIP outperforms few-shot linear probes.\\nZero-shot CLIP matches the average performance of a 4-shot linear\\nclassiﬁer trained on the same feature space and nearly matches the\\nbest results of a 16-shot linear classiﬁer across publicly available\\nmodels. For both BiT-M and SimCLRv2, the best performing\\nmodel is highlighted. Light gray lines are other models in the eval\\nsuite. The 20 datasets with at least 16 examples per class were\\nused in this analysis.\\nwe see that zero-shot CLIP is quite weak on several spe-\\ncialized, complex, or abstract tasks such as satellite image\\nclassiﬁcation (EuroSAT and RESISC45), lymph node tumor\\ndetection (PatchCamelyon), counting objects in synthetic\\nscenes (CLEVRCounts), self-driving related tasks such as\\nGerman trafﬁc sign recognition (GTSRB), recognizing dis-\\ntance to the nearest car (KITTI Distance). These results\\nhighlight the poor capability of zero-shot CLIP on more\\ncomplex tasks. By contrast, non-expert humans can robustly\\nperform several of these tasks, such as counting, satellite\\nimage classiﬁcation, and trafﬁc sign recognition, suggesting\\nsigniﬁcant room for improvement. However, we caution\\nthat it is unclear whether measuring zero-shot transfer, as\\nopposed to few-shot transfer, is a meaningful evaluation for\\ndifﬁcult tasks that a learner has no prior experience with,\\nsuch as lymph node tumor classiﬁcation for almost all hu-\\nmans (and possibly CLIP).\\nWhile comparing zero-shot performance to fully supervised\\nmodels contextualizes the task-learning capabilities of CLIP,\\ncomparing to few-shot methods is a more direct compari-\\nson, since zero-shot is its limit. In Figure 6, we visualize\\nhow zero-shot CLIP compares to few-shot logistic regres-\\nsion on the features of many image models including the\\nbest publicly available ImageNet models, self-supervised\\nlearning methods, and CLIP itself. While it is intuitive toexpect zero-shot to underperform one-shot, we instead ﬁnd\\nthat zero-shot CLIP matches the performance of 4-shot lo-\\ngistic regression on the same feature space. This is likely\\ndue to an important difference between the zero-shot and\\nfew-shot approach. First, CLIP’s zero-shot classiﬁer is gen-\\nerated via natural language which allows for visual concepts\\nto be directly speciﬁed (“communicated”). By contrast,\\n“normal” supervised learning must infer concepts indirectly\\nfrom training examples. Context-less example-based learn-\\ning has the drawback that many different hypotheses can\\nbe consistent with the data, especially in the one-shot case.\\nA single image often contains many different visual con-\\ncepts. Although a capable learner is able to exploit visual\\ncues and heuristics, such as assuming that the concept being\\ndemonstrated is the primary object in an image, there is no\\nguarantee.\\nA potential resolution of this discrepancy between zero-\\nshot and few-shot performance is to use CLIP’s zero-shot\\nclassiﬁer as a prior for the weights of the few-shot classiﬁer.\\nWhile adding an L2 penalty towards the generated weights\\nis a straightforward implementation of this idea, we found\\nthat hyperparameter optimization would often select for\\nsuch a large value of this regularizer that the resulting few-\\nshot classiﬁer was “just” the zero-shot classiﬁer. Research\\ninto better methods of combining the strength of zero-shot\\ntransfer with ﬂexibility of few-shot learning is a promising\\ndirection for future work.\\nWhen comparing zero-shot CLIP to few-shot logistic re-\\ngression on the features of other models, zero-shot CLIP\\nroughly matches the performance of the best performing\\n16-shot classiﬁer in our evaluation suite, which uses the fea-\\ntures of a BiT-M ResNet-152x2 trained on ImageNet-21K.\\nWe are certain that a BiT-L model trained on JFT-300M\\nwould perform even better but these models have not been\\npublicly released. That a BiT-M ResNet-152x2 performs\\nbest in a 16-shot setting is somewhat surprising since, as\\nanalyzed in Section 3.2, the Noisy Student EfﬁcientNet-L2\\noutperforms it in a fully supervised setting by almost 5% on\\naverage across 27 datasets.\\nIn addition to studying the average performance of zero-shot\\nCLIP and few-shot logistic regression, we also examine\\nperformance on individual datasets. In Figure 7, we show\\nestimates for the number of labeled examples per class that\\na logistic regression classiﬁer on the same feature space\\nrequires to match the performance of zero-shot CLIP. Since\\nzero-shot CLIP is also a linear classiﬁer, this estimates the\\neffective data efﬁciency of zero-shot transfer in this setting.\\nIn order to avoid training thousands of linear classiﬁers,\\nwe estimate the effective data efﬁciency based on a log-\\nlinear interpolation of the performance of a 1, 2, 4, 8, 16-\\nshot (when possible), and a fully supervised linear classiﬁer\\ntrained on each dataset. We ﬁnd that zero-shot transfer can', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='52cf7be6-6e1a-4c1c-b1ec-f7cd29320b03', embedding=None, metadata={'page_label': '10', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='73865e85372aef2672b61301dc4c110e5930cf4030c41ae4afe80c692bae2d25', text='Learning Transferable Visual Models From Natural Language Supervision 10\\n0 25 50 75 100 125 150 175 200\\n# of labeled examples per class\\nrequired to match zero-shotFlowers102EuroSATRESISC45CLEVRCountsGTSRBFGVCAircraftDTDBirdsnapUCF101KITTI DistanceCaltech101SUN397MNISTStanfordCarsHatefulMemesCIFAR100STL10Kinetics700SST2PCamImageNetCountry211OxfordPetsFood101CIFAR10FER2013\\n0.90.91.51.51.62.02.62.72.92.93.53.94.86.09.812.012.713.614.414.716.032486481184\\nMedian: 5.4Mean:  20.8\\nFigure 7. The data efﬁciency of zero-shot transfer varies\\nwidely. Calculating the number of labeled examples per class\\na linear classiﬁer on the same CLIP feature space requires to match\\nthe performance of the zero-shot classiﬁer contextualizes the ef-\\nfectiveness of zero-shot transfer. Values are estimated based on\\nlog-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised\\nresults. Performance varies widely from still underperforming a\\none-shot classiﬁer on two datasets to matching an estimated 184\\nlabeled examples per class.\\nhave widely varying efﬁciency per dataset from less than 1\\nlabeled example per class to 184. Two datasets, Flowers102\\nand EuroSAT underperform one-shot models. Half of the\\ndatasets require less than 5 examples per class with a median\\nof 5.4. However, the mean estimated data efﬁciency is 20.8\\nexamples per class. This is due to the 20% of datasets\\nwhere supervised classiﬁers require many labeled examples\\nper class in order to match performance. On ImageNet,\\nzero-shot CLIP matches the performance of a 16-shot linear\\nclassiﬁer trained on the same feature space.\\nIf we assume that evaluation datasets are large enough that\\nthe parameters of linear classiﬁers trained on them are well\\nestimated, then, because CLIP’s zero-shot classiﬁer is also\\na linear classiﬁer, the performance of the fully supervised\\nclassiﬁers roughly sets an upper bound for what zero-shot\\ntransfer can achieve. In Figure 8 we compare CLIP’s zero-\\nshot performance with fully supervised linear classiﬁers\\nacross datasets. The dashed, y=xline represents an “op-\\ntimal” zero-shot classiﬁer that matches the performance of\\nits fully supervised equivalent. For most datasets, the per-\\nformance of zero-shot classiﬁers still underperform fully su-\\npervised classiﬁers by 10% to 25%, suggesting that there is\\nstill plenty of headroom for improving CLIP’s task-learning\\nand zero-shot transfer capabilities.\\nThere is a positive correlation of 0.82 (p-value <10−6)\\nbetween zero-shot performance and fully supervised perfor-\\n20 30 40 50 60 70 80 90 100\\nLinear Probe CLIP Performance2030405060708090100Zero-Shot CLIP Performance\\nr = 0.82\\nVOC2007\\nCountry211HatefulMemesMNISTCIFAR10\\nSST2\\nDTDPCAMRESISC45\\nEuroSAT\\nGTSRB\\nCLEVRCountsFER2013UCF101\\nBirdsnapOxfordPets\\nCIFAR100\\nFGVCAircraftFood101\\nFlowers102Stanford CarsCaltech101\\nSUN397ImageNetSTL10\\nKITTI DistanceKinetics700Figure 8. Zero-shot performance is correlated with linear\\nprobe performance but still mostly sub-optimal. Comparing\\nzero-shot and linear probe performance across datasets shows a\\nstrong correlation with zero-shot performance mostly shifted 10 to\\n25 points lower. On only 5 datasets does zero-shot performance\\napproach linear probe performance ( ≤3 point difference).\\nmance, suggesting that CLIP is relatively consistent at con-\\nnecting underlying representation and task learning to zero-\\nshot transfer. However, zero-shot CLIP only approaches\\nfully supervised performance on 5 datasets: STL10, CI-\\nFAR10, Food101, OxfordPets, and Caltech101. On all 5\\ndatasets, both zero-shot accuracy and fully supervised accu-\\nracy are over 90%. This suggests that CLIP may be more\\neffective at zero-shot transfer for tasks where its underly-\\ning representations are also high quality. The slope of a\\nlinear regression model predicting zero-shot performance\\nas a function of fully supervised performance estimates that\\nfor every 1% improvement in fully supervised performance,\\nzero-shot performance improves by 1.28%. However, the\\n95th-percentile conﬁdence intervals still include values of\\nless than 1 (0.93-1.79).\\nOver the past few years, empirical studies of deep learning\\nsystems have documented that performance is predictable as\\na function of important quantities such as training compute\\nand dataset size (Hestness et al., 2017; Kaplan et al., 2020).\\nThe GPT family of models has so far demonstrated consis-\\ntent improvements in zero-shot performance across a 1000x\\nincrease in training compute. In Figure 9, we check whether\\nthe zero-shot performance of CLIP follows a similar scaling\\npattern. We plot the average error rate of the 5 ResNet CLIP\\nmodels across 39 evaluations on 36 different datasets and\\nﬁnd that a similar log-log linear scaling trend holds for CLIP\\nacross a 44x increase in model compute. While the overall\\ntrend is smooth, we found that performance on individual\\nevaluations can be much noisier. We are unsure whether', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='86a5b024-a59c-460b-ad65-1cb6b563f7a9', embedding=None, metadata={'page_label': '11', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d54b9fbadc14892f6caebda467a50025adb746a345b3d9a964383f0dd98fa302', text='Learning Transferable Visual Models From Natural Language Supervision 11\\n6.1 9.9 21.5 75.3 265.9\\nModel GFLOPs30354045Error (%)RN50\\nRN101\\nRN50x4\\nRN50x16\\nRN50x64\\nFigure 9. Zero-shot CLIP performance scales smoothly as a\\nfunction of model compute. Across 39 evals on 36 different\\ndatasets, average zero-shot error is well modeled by a log-log lin-\\near trend across a 44x range of compute spanning 5 different CLIP\\nmodels. Lightly shaded lines are performance on individual evals,\\nshowing that performance is much more varied despite the smooth\\noverall trend.\\nthis is caused by high variance between individual training\\nruns on sub-tasks (as documented in D’Amour et al. (2020))\\nmasking a steadily improving trend or whether performance\\nis actually non-monotonic as a function of compute on some\\ntasks.\\n3.2. Representation Learning\\nWhile we have extensively analyzed the task-learning ca-\\npabilities of CLIP through zero-shot transfer in the previ-\\nous section, it is more common to study the representation\\nlearning capabilities of a model. There exist many ways to\\nevaluate the quality of representations as well as disagree-\\nments over what properties an “ideal” representation should\\nhave (Locatello et al., 2020). Fitting a linear classiﬁer on\\na representation extracted from the model and measuring\\nits performance on various datasets is a common approach.\\nAn alternative is measuring the performance of end-to-end\\nﬁne-tuning of the model. This increases ﬂexibility, and\\nprior work has convincingly demonstrated that ﬁne-tuning\\noutperforms linear classiﬁcation on most image classiﬁ-\\ncation datasets (Kornblith et al., 2019; Zhai et al., 2019).\\nWhile the high performance of ﬁne-tuning motivates its\\nstudy for practical reasons, we still opt for linear classiﬁer\\nbased evaluation for several reasons. Our work is focused\\non developing a high-performing task and dataset-agnostic\\npre-training approach. Fine-tuning, because it adapts rep-\\nresentations to each dataset during the ﬁne-tuning phase,\\ncan compensate for and potentially mask failures to learn\\ngeneral and robust representations during the pre-training\\nphase. Linear classiﬁers, because of their limited ﬂexibility,\\ninstead highlight these failures and provide clear feedback\\nduring development. For CLIP, training supervised linearclassiﬁers has the added beneﬁt of being very similar to the\\napproach used for its zero-shot classiﬁers which enables\\nextensive comparisons and analysis in Section 3.1. Finally,\\nwe aim to compare CLIP to a comprehensive set of existing\\nmodels across many tasks. Studying 66 different models on\\n27 different datasets requires tuning 1782 different evalua-\\ntions. Fine-tuning opens up a much larger design and hyper-\\nparameter space, which makes it difﬁcult to fairly evaluate\\nand computationally expensive to compare a diverse set of\\ntechniques as discussed in other large scale empirical studies\\n(Lucic et al., 2018; Choi et al., 2019). By comparison, linear\\nclassiﬁers require minimal hyper-parameter tuning and have\\nstandardized implementations and evaluation procedures.\\nPlease see Appendix A for further details on evaluation.\\nFigure 10 summarizes our ﬁndings. To minimize selection\\neffects that could raise concerns of conﬁrmation or reporting\\nbias, we ﬁrst study performance on the 12 dataset evaluation\\nsuite from Kornblith et al. (2019). While small CLIP mod-\\nels such as a ResNet-50 and ResNet-101 outperform other\\nResNets trained on ImageNet-1K (BiT-S and the originals),\\nthey underperform ResNets trained on ImageNet-21K (BiT-\\nM). These small CLIP models also underperform models\\nin the EfﬁcientNet family with similar compute require-\\nments. However, models trained with CLIP scale very well\\nand the largest model we trained (ResNet-50x64) slightly\\noutperforms the best performing existing model (a Noisy\\nStudent EfﬁcientNet-L2) on both overall score and compute\\nefﬁciency. We also ﬁnd that CLIP vision transformers are\\nabout 3x more compute efﬁcient than CLIP ResNets, which\\nallows us to reach higher overall performance within our\\ncompute budget. These results qualitatively replicate the\\nﬁndings of Dosovitskiy et al. (2020) which reported that\\nvision transformers are more compute efﬁcient than con-\\nvnets when trained on sufﬁciently large datasets. Our best\\noverall model is a ViT-L/14 that is ﬁne-tuned at a higher res-\\nolution of 336 pixels on our dataset for 1 additional epoch.\\nThis model outperforms the best existing model across this\\nevaluation suite by an average of 2.6%.\\nAs Figure 21 qualitatively shows, CLIP models learn a wider\\nset of tasks than has previously been demonstrated in a sin-\\ngle computer vision model trained end-to-end from random\\ninitialization. These tasks include geo-localization, optical\\ncharacter recognition, facial emotion recognition, and action\\nrecognition. None of these tasks are measured in the evalua-\\ntion suite of Kornblith et al. (2019). This could be argued\\nto be a form of selection bias in Kornblith et al. (2019)’s\\nstudy towards tasks that overlap with ImageNet. To address\\nthis, we also measure performance on a broader 27 dataset\\nevaluation suite. This evaluation suite, detailed in Appendix\\nA includes datasets representing the aforementioned tasks,\\nGerman Trafﬁc Signs Recognition Benchmark (Stallkamp\\net al., 2011), as well as several other datasets adapted from\\nVTAB (Zhai et al., 2019).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='dee22451-b49f-4e21-959e-dab0e60f5184', embedding=None, metadata={'page_label': '12', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='04907be33603073336d9e11f294f28569cad373bd8c56a303827d19ae270d9b0', text=\"Learning Transferable Visual Models From Natural Language Supervision 12\\n100101102\\nForward-pass GFLOPs/image75808590Average Score (%)\\nLinear probe average over Kornblith et al.'s 12 datasets\\n100101102\\nForward-pass GFLOPs/image70758085Average Score (%)\\nLinear probe average over all 27 datasets\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNetInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCoViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\nFigure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models , including\\nEfﬁcientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;\\nTouvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.,\\n2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).\\n(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models ﬁne-tuned or\\nevaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset.\\nOn this broader evaluation suite, the beneﬁts of CLIP are\\nmore clear. All CLIP models, regardless of scale, outper-\\nform all evaluated systems in terms of compute efﬁciency.\\nThe improvement in average score of the best model over\\nprevious systems increases from 2.6% to 5%. We also ﬁnd\\nthat self-supervised systems do noticeably better on our\\nbroader evaluation suite. For instance, while SimCLRv2\\nstill underperforms BiT-M on average on the 12 datasets\\nof Kornblith et al. (2019), SimCLRv2 outperforms BiT-M\\non our 27 dataset evaluation suite. These ﬁndings suggest\\ncontinuing to expand task diversity and coverage in order\\nto better understand the “general” performance of systems.\\nWe suspect additional evaluation efforts along the lines of\\nVTAB to be valuable.\\nIn addition to the aggregate analysis above, we visualize\\nper-dataset differences in the performance of the best CLIP\\nmodel and the best model in our evaluation suite across\\nall 27 datasets in Figure 11. CLIP outperforms the Noisy\\nStudent EfﬁcientNet-L2 on 21 of the 27 datasets. CLIP\\nimproves the most on tasks which require OCR (SST2and HatefulMemes), geo-localization and scene recognition\\n(Country211, SUN397), and activity recognition in videos\\n(Kinetics700 and UCF101). In addition CLIP also does\\nmuch better on ﬁne-grained car and trafﬁc sign recognition\\n(Stanford Cars and GTSRB). This may reﬂect a problem\\nwith overly narrow supervision in ImageNet. A result such\\nas the 14.7% improvement on GTSRB could be indicative\\nof an issue with ImageNet-1K, which has only a single la-\\nbel for all trafﬁc and street signs. This could encourage\\na supervised representation to collapse intra-class details\\nand hurt accuracy on a ﬁne-grained downstream task. As\\nmentioned, CLIP still underperforms the EfﬁcientNet on\\nseveral datasets. Unsurprisingly, the dataset that the Efﬁ-\\ncientNet does best relative to CLIP on is the one it was\\ntrained on: ImageNet. The EffcientNet also slightly outper-\\nforms CLIP on low-resolution datasets such as CIFAR10\\nand CIFAR100. We suspect this is at least partly due to the\\nlack of scale-based data augmentation in CLIP. The Efﬁ-\\ncientNet also does slightly better on PatchCamelyon and\\nCLEVRCounts, datasets where overall performance is still\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='24d1ad3b-9c96-461b-ae99-eaddebabc42c', embedding=None, metadata={'page_label': '13', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='0de8e347a6f368e964ec29612f9dce9df39eaa4edc655c994f6f3798bd6bce2b', text='Learning Transferable Visual Models From Natural Language Supervision 13\\n10\\n 5\\n 0 5 10 15 20 25\\n Score (%)\\nLogistic Regression on CLIP vs. EfficientNet L2 NSImageNet -3.0CLEVRCounts -2.4CIFAR100 -1.7PatchCamelyon -1.2CIFAR10 -0.8OxfordPets -0.5+0.0 STL10+0.5 VOC2007+0.5 DTD+0.6 MNIST+0.9 EuroSAT+1.3 Caltech101+1.4 Flowers102+1.4 Birdsnap+2.3 KITTI Distance+3.1 UCF101+3.2 FGVCAircraft+3.9 Food101+4.5 FER2013+5.1 RESISC45+6.2 Kinetics700+6.5 SUN397+14.7 GTSRB+15.9 StanfordCars+18.8 HatefulMemes+22.7 Country211+23.6 SST2\\nFigure 11. CLIP’s features outperform the features of the best\\nImageNet model on a wide variety of datasets. Fitting a linear\\nclassiﬁer on CLIP’s features outperforms using the Noisy Student\\nEfﬁcientNet-L2 on 21 out of 27 datasets.\\nlow for both approaches.\\n3.3. Robustness to Natural Distribution Shift\\nIn 2015, it was announced that a deep learning model ex-\\nceeded human performance on the ImageNet test set (He\\net al., 2015). However, research in the subsequent years\\nhas repeatedly found that these models still make many sim-\\nple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;\\nAlcorn et al., 2019), and new benchmarks testing these sys-\\ntems has often found their performance to be much lower\\nthan both their ImageNet accuracy and human accuracy\\n(Recht et al., 2019; Barbu et al., 2019). What explains this\\ndiscrepancy? Various ideas have been suggested and stud-\\nied (Ilyas et al., 2019; Geirhos et al., 2020). A common\\ntheme of proposed explanations is that deep learning models\\nare exceedingly adept at ﬁnding correlations and patterns\\nwhich hold across their training dataset and thus improve\\nin-distribution performance. However many of these corre-\\nlations and patterns are actually spurious and do not hold for\\nother distributions and result in large drops in performance\\non other datasets.\\nWe caution that, to date, most of these studies limit their\\nevaluation to models trained on ImageNet. Recalling the\\ntopic of discussion, it may be a mistake to generalize too\\nfar from these initial ﬁndings. To what degree are these\\nfailures attributable to deep learning, ImageNet, or somecombination of the two? CLIP models, which are trained via\\nnatural language supervision on a very large dataset and are\\ncapable of high zero-shot performance, are an opportunity\\nto investigate this question from a different angle.\\nTaori et al. (2020) is a recent comprehensive study mov-\\ning towards quantifying and understanding these behaviors\\nfor ImageNet models. Taori et al. (2020) study how the\\nperformance of ImageNet models change when evaluated\\nonnatural distribution shifts . They measure performance\\non a set of 7 distribution shifts: ImageNetV2 (Recht et al.,\\n2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB\\nand ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu\\net al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),\\nand ImageNet Rendition (Hendrycks et al., 2020a). They\\ndistinguish these datasets, which all consist of novel images\\ncollected from a variety of sources, from synthetic distri-\\nbution shifts such as ImageNet-C (Hendrycks & Dietterich,\\n2019), Stylized ImageNet (Geirhos et al., 2018), or adver-\\nsarial attacks (Goodfellow et al., 2014) which are created by\\nperturbing existing images in various ways. They propose\\nthis distinction because in part because they ﬁnd that while\\nseveral techniques have been demonstrated to improve per-\\nformance on synthetic distribution shifts, they often fail to\\nyield consistent improvements on natural distributions.3\\nAcross these collected datasets, the accuracy of ImageNet\\nmodels drop well below the expectation set by the Ima-\\ngeNet validation set. For the following summary discussion\\nwe report average accuracy across all 7 natural distribution\\nshift datasets and average accuracy across the correspond-\\ning class subsets of ImageNet unless otherwise speciﬁed.\\nAdditionally, for Youtube-BB and ImageNet-Vid, which\\nhave two different evaluation settings, we use the average\\nof pm-0 and pm-10 accuracy.\\nA ResNet-101 makes 5 times as many mistakes when eval-\\nuated on these natural distribution shifts compared to the\\nImageNet validation set. Encouragingly however, Taori et al.\\n(2020) ﬁnd that accuracy under distribution shift increases\\npredictably with ImageNet accuracy and is well modeled\\nas a linear function of logit-transformed accuracy. Taori\\net al. (2020) use this ﬁnding to propose that robustness\\nanalysis should distinguish between effective andrelative\\nrobustness. Effective robustness measures improvements\\nin accuracy under distribution shift above what is predicted\\nby the documented relationship between in-distribution and\\nout-of-distribution accuracy. Relative robustness captures\\nany improvement in out-of-distribution accuracy. Taori et al.\\n(2020) argue that robustness techniques should aim to im-\\nprove both effective robustness and relative robustness.\\nAlmost all models studied in Taori et al. (2020) are trained\\n3We refer readers to Hendrycks et al. (2020a) for additional\\nexperiments and discussion on this claim.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9a58e7ef-f316-4b7a-959d-74ed50bfe363', embedding=None, metadata={'page_label': '14', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='78d9cb9bc95872621e1a5ca29f7a7a0eb11f6cbe8147f0c58af9c2076e3ea317', text=\"Learning Transferable Visual Models From Natural Language Supervision 14\\n65 70 75 80 85 90\\nImageNet Score (%)657075808590Transfer Score (%)\\nLinear probe average over Kornblith et al.'s 12 datasets\\n65 70 75 80 85 90\\nImageNet Score (%)657075808590Transfer Score (%)\\nLinear probe average over 26 datasets\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNetInstagram\\nSimCLRv2\\nBYOL\\nMoCoViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\nFigure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset\\nsplits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar\\nImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overﬁt to their task.\\nor ﬁne-tuned on the ImageNet dataset. Returning to the\\ndiscussion in the introduction to this section - is training\\nor adapting to the ImageNet dataset distribution the cause\\nof the observed robustness gap? Intuitively, a zero-shot\\nmodel should not be able to exploit spurious correlations\\nor patterns that hold only on a speciﬁc distribution, since it\\nis not trained on that distribution.4Thus it is reasonable\\nto expect zero-shot models to have much higher effective\\nrobustness. In Figure 13, we compare the performance of\\nzero-shot CLIP with existing ImageNet models on natural\\ndistribution shifts. All zero-shot CLIP models improve\\neffective robustness by a large amount and reduce the size\\nof the gap between ImageNet accuracy and accuracy under\\ndistribution shift by up to 75%.\\nWhile these results show that zero-shot models can be much\\nmore robust, they do not necessarily mean that supervised\\nlearning on ImageNet causes a robustness gap. Other details\\nof CLIP, such as its large and diverse pre-training dataset\\nor use of natural language supervision could also result\\n4We caution that a zero-shot model can still exploit spurious\\ncorrelations that are shared between the pre-training and evaluation\\ndistributions.in much more robust models regardless of whether they\\nare zero-shot or ﬁne-tuned. As an initial experiment to\\npotentially begin narrowing this down, we also measure\\nhow the performance of CLIP models change after adapting\\nto the ImageNet distribution via a L2 regularized logistic\\nregression classiﬁer ﬁt to CLIP features on the ImageNet\\ntraining set. We visualize how performance changes from\\nthe zero-shot classiﬁer in Figure 14. Although adapting\\nCLIP to the ImageNet distribution increases its ImageNet\\naccuracy by 9.2% to 85.4% overall, and ties the accuracy\\nof the 2018 SOTA from Mahajan et al. (2018), average\\naccuracy under distribution shift slightly decreases .\\nIt is surprising to see a 9.2% increase in accuracy, which cor-\\nresponds to roughly 3 years of improvement in SOTA, fail\\nto translate into any improvement in average performance\\nunder distribution shift. We also break down the differences\\nbetween zero-shot accuracy and linear classiﬁer accuracy\\nper dataset in Figure 14 and ﬁnd performance still increases\\nsigniﬁcantly on one dataset, ImageNetV2. ImageNetV2\\nclosely followed the creation process of the original Ima-\\ngeNet dataset which suggests that gains in accuracy from\\nsupervised adaptation are closely concentrated around the\\nImageNet distribution. Performance decreases by 4.7% on\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4fc4db4d-058f-43e6-9dfc-ee84f4281faf', embedding=None, metadata={'page_label': '15', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a2ba999dfae8900f1e48819a0db53b12b2a7f8793711ac28555ff406fef60160', text='Learning Transferable Visual Models From Natural Language Supervision 15\\n65 70 75 80 85 90 95 100\\nAverage on class subsampled ImageNet (top-1, %)20253035404550556065707580859095100Average on 7 natural distribution shift datasets (top-1, %)Ideal robust model (y = x)\\nZero-Shot CLIP\\nStandard ImageNet training\\nExisiting robustness techniques\\nImageNet \\nImageNetV2 \\nImageNet-A ImageNet-R \\nObjectNet \\nImageNet \\nSketch 76.2 76.2\\n64.3 70.1\\n2.7 77.137.7 88.9\\n32.6 72.3\\n25.2 60.2ImageNet \\nResNet101 Zero-Shot\\nCLIP \\n0%\\n+5.8% \\n+74.4% +51.2% \\n+39.7% \\n+35.0% \\nΔ Score \\nDataset Examples \\nFigure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model\\n(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink\\nthis “robustness gap” by up to 75%. Linear ﬁts on logit transformed values are shown with bootstrap estimated 95% conﬁdence intervals.\\n(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of\\nthe best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation\\nset, ResNet-101.\\nImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,\\nand 1.9% on ImageNet-A. The change in accuracy on the\\ntwo other datasets, Youtube-BB and ImageNet Vid, is in-\\nsigniﬁcant.\\nHow is it possible to improve accuracy by 9.2% on the Im-\\nageNet dataset with little to no increase in accuracy under\\ndistribution shift? Is the gain primarily from “exploiting\\nspurious correlations”? Is this behavior unique to some com-\\nbination of CLIP, the ImageNet datatset, and the distribution\\nshifts studied, or a more general phenomena? Does it hold\\nfor end-to-end ﬁnetuning as well as linear classiﬁers? We\\ndo not have conﬁdent answers to these questions at this time.\\nPrior work has also pre-trained models on distributions other\\nthan ImageNet, but it is common to study and release mod-\\nels only after they have been ﬁne-tuned to ImageNet. As a\\nstep towards understanding whether pre-trained zero-shot\\nmodels consistently have higher effective robustness than\\nﬁne-tuned models, we encourage the authors of Mahajan\\net al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al.\\n(2020) to, if possible, study these questions on their models\\nas well.\\nWe also investigate another robustness intervention enabled\\nby ﬂexible zero-shot natural-language-based image classi-\\nﬁers. The target classes across the 7 transfer datasets are\\nnot always perfectly aligned with those of ImageNet. Two\\ndatasets, Youtube-BB and ImageNet-Vid, consist of super-\\nclasses of ImageNet. This presents a problem when trying\\nto use the ﬁxed 1000-way classiﬁer of an ImageNet model\\nto make predictions. Taori et al. (2020) handle this by max-pooling predictions across all sub-classes according to the\\nImageNet class hierarchy. Sometimes this mapping is much\\nless than perfect. For the person class in Youtube-BB, pre-\\ndictions are made by pooling over the ImageNet classes for\\na baseball player, a bridegroom, and a scuba diver. With\\nCLIP we can instead generate a custom zero-shot classi-\\nﬁer for each dataset directly based on its class names. In\\nFigure 14 we see that this improves average effective ro-\\nbustness by 5% but is concentrated in large improvements\\non only a few datasets. Curiously, accuracy on ObjectNet\\nalso increases by 2.3%. Although the dataset was designed\\nto closely overlap with ImageNet classes, using the names\\nprovided for each class by ObjectNet’s creators still helps a\\nsmall amount compared to using ImageNet class names and\\npooling predictions when necessary.\\nWhile zero-shot CLIP improves effective robustness, Figure\\n14 shows that the beneﬁt is almost entirely gone in a fully\\nsupervised setting. To better understand this difference, we\\ninvestigate how effective robustness changes on the contin-\\nuum from zero-shot to fully supervised. In Figure 15 we\\nvisualize the performance of 0-shot, 1-shot, 2-shot, 4-shot\\n..., 128-shot, and fully supervised logistic regression classi-\\nﬁers on the best CLIP model’s features. We see that while\\nfew-shot models also show higher effective robustness than\\nexisting models, this beneﬁt fades as in-distribution per-\\nformance increases with more training data and is mostly,\\nthough not entirely, gone for the fully supervised model.\\nAdditionally, zero-shot CLIP is notably more robust than\\na few-shot model with equivalent ImageNet performance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='877a0af1-3825-4524-a25f-9f45e50edc40', embedding=None, metadata={'page_label': '16', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='1921a476f5ab85268edbf117a4d560dfc806c13fa73cf83339bf0c1b94f67cc6', text='Learning Transferable Visual Models From Natural Language Supervision 16\\n70 75 80 85 90 95\\nAverage on class subsampled ImageNet (top-1, %)253035404550556065707580Average on 7 natural distribution shift datasets (top-1, %)Adapt to class shift\\nAdapt to ImageNet\\nIdeal robust model (y = x)\\nAdaptive Zero-Shot CLIP\\nImageNet Zero-Shot CLIP\\nLogistic Regression CLIP\\nStandard ImageNet training\\nRobustness intervention\\nTrained with more data\\n10\\n 5\\n 0 5 10 15 20 25 30\\nChange from zero-shot ImageNet classifier accuracy (%)ImageNet-R -4.7ObjectNet -3.8ImageNet Sketch -2.8ImageNet-A -1.9ImageNet Vid -0.5+0.6 Youtube-BB+5.8 ImageNetV2+9.2 ImageNetAdapt to ImageNet\\n10\\n 5\\n 0 5 10 15 20 25 30\\nChange from zero-shot ImageNet classifier accuracy (%)0 ImageNet0 ImageNetV20 ImageNet-A0 ImageNet-R0 ImageNet Sketch+2.3 ObjectNet+8.3 ImageNet Vid+26.9 Youtube-BBAdapt to class shift\\nFigure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness.\\n(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classiﬁer\\nand pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness\\nas the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to\\nImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset speciﬁc zero-shot\\nclassiﬁers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align\\nwith ImageNet categories.\\nAcross our experiments, high effective robustness seems to\\nresult from minimizing the amount of distribution speciﬁc\\ntraining data a model has access to, but this comes at a cost\\nof reducing dataset-speciﬁc performance.\\nTaken together, these results suggest that the recent shift\\ntowards large-scale task and dataset agnostic pre-training\\ncombined with a reorientation towards zero-shot and few-\\nshot benchmarking on broad evaluation suites (as advocated\\nby Yogatama et al. (2019) and Linzen (2020)) promotes the\\ndevelopment of more robust systems and provides a more\\naccurate assessment of performance. We are curious to see\\nif the same results hold for zero-shot models in the ﬁeld\\nof NLP such as the GPT family. While Hendrycks et al.\\n(2020b) has reported that pre-training improves relative ro-\\nbustness on sentiment analysis, Miller et al. (2020)’s study\\nof the robustness of question answering models under nat-\\nural distribution shift ﬁnds, similar to Taori et al. (2020),\\nlittle evidence of effective robustness improvements to date.\\n4. Comparison to Human Performance\\nHow does CLIP compare to human performance and human\\nlearning? To get a better understanding of how well humans\\nperform in similar evaluation settings to CLIP, we evaluatedhumans on one of our tasks. We wanted to get a sense of\\nhow strong human zero-shot performance is at these tasks,\\nand how much human performance is improved if they are\\nshown one or two image samples. This can help us to\\ncompare task difﬁculty for humans and CLIP, and identify\\ncorrelations and differences between them.\\nWe had ﬁve different humans look at each of 3669 images\\nin the test split of the Oxford IIT Pets dataset (Parkhi et al.,\\n2012) and select which of the 37 cat or dog breeds best\\nmatched the image (or ‘I don’t know’ if they were com-\\npletely uncertain). In the zero-shot case the humans were\\ngiven no examples of the breeds and asked to label them\\nto the best of their ability without an internet search. In\\nthe one-shot experiment the humans were given one sample\\nimage of each breed and in the two-shot experiment they\\nwere given two sample images of each breed.5\\nOne possible concern was that the human workers were not\\nsufﬁciently motivated in the zero-shot task. High human\\naccuracy of 94% on the STL-10 dataset (Coates et al., 2011)\\n5There is not a perfect correspondence between the human\\nfew-shot tasks and the model’s few-shot performance since the\\nmodel cannot refer to sample images in the way that the humans\\ncan.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7e454ab7-dfc7-49f7-a7a1-4bcd9a5086c7', embedding=None, metadata={'page_label': '17', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='09b569bbc2d9e6feae38eb26d3e9f26b7e89c71580e10f191dea35b4aac17821', text='Learning Transferable Visual Models From Natural Language Supervision 17\\n65 70 75 80 85 90 95\\nAverage on class subsampled ImageNet (top-1, %)202530354045505560657075Average on 7 natural distribution shift datasets (top-1, %)1 shot2 shot4 shot8 shot16 shot3264128all0 shot\\nIdeal robust model (y = x)\\nFew-Shot CLIP (best model)\\nZero-Shot CLIP (best model)\\nStandard ImageNet training\\nRobustness intervention\\nTrained with more data\\nFigure 15. Few-shot CLIP also increases effective robustness\\ncompared to existing ImageNet models but is less robust than\\nzero-shot CLIP. Minimizing the amount of ImageNet training\\ndata used for adaption increases effective robustness at the cost of\\ndecreasing relative robustness. 16-shot logistic regression CLIP\\nmatches zero-shot CLIP on ImageNet, as previously reported in\\nFigure 7, but is less robust.\\nand 97-100% accuracy on the subset of attention check\\nimages increased our trust in the human workers.\\nInterestingly, humans went from a performance average of\\n54% to 76% with just one training example per class, and\\nthe marginal gain from an additional training example is\\nminimal. The gain in accuracy going from zero to one shot\\nis almost entirely on images that humans were uncertain\\nabout. This suggests that humans “know what they don’t\\nknow” and are able to update their priors on the images they\\nare most uncertain in based on a single example. Given this,\\nit seems that while CLIP is a promising training strategy\\nfor zero-shot performance (Figure 5) and does well on tests\\nof natural distribution shift (Figure 13), there is a large\\ndifference between how humans learn from a few examples\\nand the few-shot methods in this paper.\\nThis suggests that there are still algorithmic improvements\\nwaiting to be made to decrease the gap between machine\\nand human sample efﬁciency, as noted by Lake et al. (2016)\\nand others. Because these few-shot evaluations of CLIP\\ndon’t make effective use of prior knowledge and the humans\\ndo, we speculate that ﬁnding a method to properly integrate\\nprior knowledge into few-shot learning is an important step\\nin algorithmic improvements to CLIP. To our knowledge,\\nusing a linear classiﬁer on top of the features of a high-AccuracyMajority V ote\\non Full DatasetAccuracy\\non GuessesMajority V ote\\nAccuracy\\non Guesses\\nZero-shot human 53.7 57.0 69.7 63.9\\nZero-shot CLIP 93.5 93.5 93.5 93.5\\nOne-shot human 75.7 80.3 78.5 81.2\\nTwo-shot human 75.7 85.0 79.2 86.1\\nTable 2. Comparison of human performance on Oxford IIT Pets.\\nAs in Parkhi et al. (2012), the metric is average per-class classiﬁca-\\ntion accuracy. Most of the gain in performance when going from\\nthe human zero shot case to the human one shot case is on images\\nthat participants were highly uncertain on. “Guesses” refers to\\nrestricting the dataset to where participants selected an answer\\nother than “I don’t know”, the “majority vote” is taking the most\\nfrequent (exclusive of ties) answer per image.\\nquality pre-trained model is near state-of-the-art for few\\nshot learning (Tian et al., 2020), which suggests that there is\\na gap between the best few-shot machine learning methods\\nand human few-shot learning.\\nIf we plot human accuracy vs CLIP’s zero shot accuracy\\n(Figure 16), we see that the hardest problems for CLIP are\\nalso hard for humans. To the extent that errors are consistent,\\nour hypothesis is that this is due to at least a two factors:\\nnoise in the dataset (including mislabeled images) and out of\\ndistribution images being hard for both humans and models.\\n5. Data Overlap Analysis\\nA concern with pre-training on a very large internet dataset\\nis unintentional overlap with downstream evals. This is\\nimportant to investigate since, in a worst-case scenario, a\\ncomplete copy of an evaluation dataset could leak into the\\npre-training dataset and invalidate the evaluation as a mean-\\ningful test of generalization. One option to prevent this is to\\nidentify and remove all duplicates before training a model.\\nWhile this guarantees reporting true hold-out performance,\\nit requires knowing all possible data which a model might\\nbe evaluated on ahead of time. This has the downside of\\nlimiting the scope of benchmarking and analysis. Adding a\\nnew evaluation would require an expensive re-train or risk\\nreporting an un-quantiﬁed beneﬁt due to overlap.\\nInstead, we document how much overlap occurs and how\\nperformance changes due to these overlaps. To do this, we\\nuse the following procedure:\\n1) For each evaluation dataset, we run a duplicate detector\\n(see Appendix C) on its examples. We then manually inspect\\nthe found nearest neighbors and set a per dataset threshold\\nto keep high precision while maximizing recall. Using\\nthis threshold, we then create two new subsets, Overlap ,\\nwhich contains all examples which have a similarity to a\\ntraining example above the threshold, and Clean , which', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2dab1038-2bd5-48f9-8a1f-1282c59ac833', embedding=None, metadata={'page_label': '18', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='9531156c22245730421446e7d71bff33d6e6644748f45b89a88796c5fc06c557', text='Learning Transferable Visual Models From Natural Language Supervision 18\\npug\\nsphynx\\ngerman_shorthaired\\nshiba_inu\\nbeagle\\ngreat_pyrenees\\nenglish_setter\\nsamoyed\\nsaint_bernard\\npomeranian\\nnewfoundland\\nwheaten_terrier\\nscottish_terrier\\nyorkshire_terrier\\nsiamese\\nminiature_pinscher\\nhavanese\\nkeeshond\\nbombay\\nmaine_coon\\nchihuahua\\nbasset_hound\\njapanese_chin\\nrussian_blue\\namerican_bulldog\\npersian\\nbengal\\nleonberger\\nabyssinian\\nboxer\\nbritish_shorthair\\nstaffordshire_bull_terrier\\namerican_pit_bull_terrier\\negyptian_mau\\nbirman\\nenglish_cocker_spaniel\\nragdoll20406080100Accuracy (%)\\nZero-Shot CLIP\\nOne-Shot Human\\nZero-Shot Human\\nFigure 16. The hardest problems for CLIP also tend to be the hard-\\nest problems for humans. Here we rank image categories by difﬁ-\\nculty for CLIP as measured as probability of the correct label.\\ncontains all examples that are below this threshold. We\\ndenote the unaltered full dataset All for reference. From\\nthis we ﬁrst record the degree of data contamination as the\\nratio of the number of examples in Overlap to the size of\\nAll.\\n2) We then compute the zero-shot accuracy of CLIP\\nRN50x64 on the three splits and report All - Clean\\nas our main metric. This is the difference in accuracy due\\nto contamination. When positive it is our estimate of how\\nmuch the overall reported accuracy on the dataset was in-\\nﬂated by over-ﬁtting to overlapping data.\\n3) The amount of overlap is often small so we also run a\\nbinomial signiﬁcance test where we use the accuracy on\\nClean as the null hypothesis and compute the one-tailed\\n(greater) p-value for the Overlap subset. We also calculate\\n99.5% Clopper-Pearson conﬁdence intervals on Dirty as\\nanother check.\\nA summary of this analysis is presented in Figure 17. Out\\nof 35 datasets studied, 9 datasets have no detected overlap\\nat all. Most of these datasets are synthetic or specialized\\nmaking them unlikely to be posted as normal images on\\nthe internet (for instance MNIST, CLEVR, and GTSRB) or\\nare guaranteed to have no overlap due to containing novel\\ndata from after the date our dataset was created (ObjectNet\\nand Hateful Memes). This demonstrates our detector has\\na low-false positive rate which is important as false posi-\\ntives would under-estimate the effect of contamination inour analysis. There is a median overlap of 2.2% and an av-\\nerage overlap of 3.2%. Due to this small amount of overlap,\\noverall accuracy is rarely shifted by more than 0.1% with\\nonly 7 datasets above this threshold. Of these, only 2 are\\nstatistically signiﬁcant after Bonferroni correction. The max\\ndetected improvement is only 0.6% on Birdsnap which has\\nthe second largest overlap at 12.1%. The largest overlap is\\nfor Country211 at 21.5%. This is due to it being constructed\\nout of YFCC100M, which our pre-training dataset contains\\na ﬁltered subset of. Despite this large overlap there is only\\na 0.2% increase in accuracy on Country211. This may be\\nbecause the training text accompanying an example is often\\nnot related to the speciﬁc task a downstream eval measures.\\nCountry211 measures geo-localization ability, but inspect-\\ning the training text for these duplicates showed they often\\ndo not mention the location of the image.\\nWe are aware of two potential concerns with our analysis.\\nFirst our detector is not perfect. While it achieves near\\n100% accuracy on its proxy training task and manual in-\\nspection + threshold tuning results in very high precision\\nwith good recall among the found nearest-neighbors, we can\\nnot tractably check its recall across 400 million examples.\\nAnother potential confounder of our analysis is that the un-\\nderlying data distribution may shift between the Overlap\\nandClean subsets. For example, on Kinetics-700 many\\n“overlaps” are in fact all black transition frames. This ex-\\nplains why Kinetics-700 has an apparent 20% accuracy drop\\nonOverlap . We suspect more subtle distribution shifts\\nlikely exist. One possibility we noticed on CIFAR-100 is\\nthat, due to the very low resolution of its images, many\\nduplicates were false positives of small objects such as birds\\nor planes. Changes in accuracy could instead be due to\\nchanges in the class distribution or difﬁculty of the dupli-\\ncates. Unfortunately, these distribution and difﬁculty shifts\\ncould also mask the effects of over-ﬁtting.\\nHowever, these results closely follow the ﬁndings of simi-\\nlar duplicate analysis in previous work on large scale pre-\\ntraining. Mahajan et al. (2018) and Kolesnikov et al. (2019)\\ndetected similar overlap rates and found minimal changes in\\noverall performance. Importantly, Kolesnikov et al. (2019)\\nalso compared the alternative de-duplication strategy dis-\\ncussed in the introduction to this section with the approach\\nwe settled on and observed little difference between the two\\napproaches.\\n6. Limitations\\nThere are still many limitations to CLIP. While several of\\nthese are discussed as part of analysis in various sections,\\nwe summarize and collect them here.\\nOn datasets with training splits, the performance of zero-\\nshot CLIP is on average competitive with the simple su-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1cc84b36-7de9-4459-82d1-150a4006b57f', embedding=None, metadata={'page_label': '19', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ab3edc6ca096766d35a242acc2f62f970a214484083050cfcf4a3fe48158d429', text='Learning Transferable Visual Models From Natural Language Supervision 19\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\\nDetected Data Overlap (%)-20-1001020Difference in Accuracy on Overlapping vs. Clean Data (%)SUN397CIFAR-100\\nImageNet SketchSUN\\nKinetics-700\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\\nDetected Data Overlap (%)-0.75-0.5-0.2500.250.50.75Overall Accuracy Change Due To Overlap (%)Stanford Cars SUN397Birdsnap\\nCIFAR-100\\nFER2013\\nCountry211SUNp < 1e-3\\np < 0.05\\np > 0.05\\nFigure 17. Few statistically signiﬁcant improvements in accuracy due to detected data overlap. (Left) While several datasets have\\nup to±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have\\n99.5% Clopper-Pearson conﬁdence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data.\\n(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to\\noverlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy\\nimprovements statistically signiﬁcant when calculated using a one-sided binomial test.\\npervised baseline of a linear classiﬁer on top of ResNet-50\\nfeatures. On most of these datasets, the performance of\\nthis baseline is now well below the overall state of the art.\\nSigniﬁcant work is still needed to improve the task learning\\nand transfer capabilities of CLIP. While scaling has so far\\nsteadily improved performance and suggests a route for con-\\ntinued improvement, we estimate around a 1000x increase\\nin compute is required for zero-shot CLIP to reach overall\\nstate-of-the-art performance. This is infeasible to train with\\ncurrent hardware. Further research into improving upon the\\ncomputational and data efﬁciency of CLIP will be necessary.\\nAnalysis in Section 3.1 found that CLIP’s zero-shot perfor-\\nmance is still quite weak on several kinds of tasks. When\\ncompared to task-speciﬁc models, the performance of CLIP\\nis poor on several types of ﬁne-grained classiﬁcation such\\nas differentiating models of cars, species of ﬂowers, and\\nvariants of aircraft. CLIP also struggles with more abstract\\nand systematic tasks such as counting the number of objects\\nin an image. Finally for novel tasks which are unlikely to be\\nincluded in CLIP’s pre-training dataset, such as classifying\\nthe distance to the nearest car in a photo, CLIP’s perfor-\\nmance can be near random. We are conﬁdent that there are\\nstill many, many, tasks where CLIP’s zero-shot performance\\nis near chance level.\\nWhile zero-shot CLIP generalizes well to many natural im-\\nage distributions as investigated in Section 3.3, we’ve ob-\\nserved that zero-shot CLIP still generalizes poorly to data\\nthat is truly out-of-distribution for it. An illustrative exam-\\nple occurs for the task of OCR as reported in Appendix E.CLIP learns a high quality semantic OCR representation that\\nperforms well on digitally rendered text, which is common\\nin its pre-training dataset, as evidenced by performance on\\nRendered SST2. However, CLIP only achieves 88% accu-\\nracy on the handwritten digits of MNIST. An embarrassingly\\nsimple baseline of logistic regression on raw pixels outper-\\nforms zero-shot CLIP. Both semantic and near-duplicate\\nnearest-neighbor retrieval verify that there are almost no im-\\nages that resemble MNIST digits in our pre-training dataset.\\nThis suggests CLIP does little to address the underlying\\nproblem of brittle generalization of deep learning models.\\nInstead CLIP tries to circumvent the problem and hopes that\\nby training on such a large and varied dataset that all data\\nwill be effectively in-distribution. This is a naive assumption\\nthat, as MNIST demonstrates, is easy to violate.\\nAlthough CLIP can ﬂexibly generate zero-shot classiﬁers\\nfor a wide variety of tasks and datasets, CLIP is still limited\\nto choosing from only those concepts in a given zero-shot\\nclassiﬁer. This is a signiﬁcant restriction compared to a\\ntruly ﬂexible approach like image captioning which could\\ngenerate novel outputs. Unfortunately, as described in Sec-\\ntion 2.3 we found the computational efﬁciency of the image\\ncaption baseline we tried to be much lower than CLIP. A\\nsimple idea worth trying is joint training of a contrastive\\nand generative objective with the hope of combining the\\nefﬁciency of CLIP with the ﬂexibility of a caption model.\\nAs another alternative, search could be performed at infer-\\nence time over many natural language explanations of a\\ngiven image, similar to approach proposed in Learning with\\nLatent Language Andreas et al. (2017).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e5cff2ee-2466-45ee-ad7a-e40427b07bb8', embedding=None, metadata={'page_label': '20', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='94739459f0fc6684e1191fba9dcd09f19a222b19c652115bdae36c56105718e1', text='Learning Transferable Visual Models From Natural Language Supervision 20\\nCLIP also does not address the poor data efﬁciency of deep\\nlearning. Instead CLIP compensates by using a source of\\nsupervision that can be scaled to hundreds of millions of\\ntraining examples. If every image seen during training of\\na CLIP model was presented at a rate of one per second,\\nit would take 405 years to iterate through the 12.8 billion\\nimages seen over 32 training epochs. Combining CLIP\\nwith self-supervision (Henaff, 2020; Chen et al., 2020c) and\\nself-training (Lee; Xie et al., 2020) methods is a promising\\ndirection given their demonstrated ability to improve data\\nefﬁciency over standard supervised learning.\\nOur methodology has several signiﬁcant limitations. De-\\nspite our focus on zero-shot transfer, we repeatedly queried\\nperformance on full validation sets to guide the develop-\\nment of CLIP. These validation sets often have thousands\\nof examples, which is unrealistic for true zero-shot sce-\\nnarios. Similar concerns have been raised in the ﬁeld of\\nsemi-supervised learning (Oliver et al., 2018). Another po-\\ntential issue is our selection of evaluation datasets. While\\nwe have reported results on Kornblith et al. (2019)’s 12\\ndataset evaluation suite as a standardized collection, our\\nmain results use a somewhat haphazardly assembled col-\\nlection of 27 datasets that is undeniably co-adapted with\\nthe development and capabilities of CLIP. Creating a new\\nbenchmark of tasks designed explicitly to evaluate broad\\nzero-shot transfer capabilities, rather than re-using existing\\nsupervised datasets, would help address these issues.\\nCLIP is trained on text paired with images on the internet.\\nThese image-text pairs are unﬁltered and uncurated and\\nresult in CLIP models learning many social biases. This\\nhas been previously demonstrated for image caption models\\n(Bhargava & Forsyth, 2019). We refer readers to Section 7\\nfor detailed analysis and quantiﬁcation of these behaviors for\\nCLIP as well as discussion of potential mitigation strategies.\\nWhile we have emphasized throughout this work that speci-\\nfying image classiﬁers through natural language is a ﬂexible\\nand general interface, it has its own limitations. Many com-\\nplex tasks and visual concepts can be difﬁcult to specify\\njust through text. Actual training examples are undeniably\\nuseful but CLIP does not optimize for few-shot performance\\ndirectly. In our work, we fall back to ﬁtting linear classiﬁers\\non top of CLIP’s features. This results in a counter-intuitive\\ndrop in performance when transitioning from a zero-shot\\nto a few-shot setting. As discussed in Section 4, this is\\nnotably different from human performance which shows a\\nlarge increase from a zero to a one shot setting. Future work\\nis needed to develop methods that combine CLIP’s strong\\nzero-shot performance with efﬁcient few-shot learning.7. Broader Impacts\\nCLIP has a wide range of capabilities due to its ability to\\ncarry out arbitrary image classiﬁcation tasks. One can give\\nit images of cats and dogs and ask it to classify cats, or give\\nit images taken in a department store and ask it to classify\\nshoplifters–a task with signiﬁcant social implications and\\nfor which AI may be unﬁt. Like any image classiﬁcation\\nsystem, CLIP’s performance and ﬁtness for purpose need to\\nbe evaluated, and its broader impacts analyzed in context.\\nCLIP also introduces a capability that will magnify and alter\\nsuch issues: CLIP makes it possible to easily create your\\nown classes for categorization (to ‘roll your own classiﬁer’)\\nwithout a need for re-training. This capability introduces\\nchallenges similar to those found in characterizing other,\\nlarge-scale generative models like GPT-3 (Brown et al.,\\n2020); models that exhibit non-trivial zero-shot (or few-\\nshot) generalization can have a vast range of capabilities,\\nmany of which are made clear only after testing for them.\\nOur studies of CLIP in a zero-shot setting show that the\\nmodel displays signiﬁcant promise for widely-applicable\\ntasks like image retrieval or search. For example, it can ﬁnd\\nrelevant images in a database given text, or relevant text\\ngiven an image. Further, the relative ease of steering CLIP\\ntoward bespoke applications with little or no additional data\\nor training could unlock a variety of novel applications that\\nare hard for us to envision today, as has occurred with large\\nlanguage models over the past few years.\\nIn addition to the more than 30 datasets studied in earlier\\nsections of this paper, we evaluate CLIP’s performance on\\nthe FairFace benchmark and undertake exploratory bias\\nprobes. We then characterize the model’s performance in\\na downstream task, surveillance, and discuss its usefulness\\nas compared with other available systems. Many of CLIP’s\\ncapabilities are omni-use in nature (e.g. OCR can be used\\nto make scanned documents searchable, to power screen\\nreading technologies, or to read license plates). Several\\nof the capabilities measured, from action recognition, ob-\\nject classiﬁcation, and geo-localization, to facial emotion\\nrecognition, can be used in surveillance. Given its social\\nimplications, we address this domain of use speciﬁcally in\\nthe Surveillance section.\\nWe have also sought to characterize the social biases inher-\\nent to the model. Our bias tests represent our initial efforts\\nto probe aspects of how the model responds in different sce-\\nnarios, and are by nature limited in scope. CLIP and models\\nlike it will need to be analyzed in relation to their speciﬁc\\ndeployments to understand how bias manifests and iden-\\ntify potential interventions. Further community exploration\\nwill be required to develop broader, more contextual, and\\nmore robust testing schemes so that AI developers can bet-\\nter characterize biases in general purpose computer vision\\nmodels.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3beb0636-ac18-4985-b68c-98f2f6f31b22', embedding=None, metadata={'page_label': '21', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ef750f0c03694a29c9cc4655e5e9c1eae17bde740d2eb86ad7eed3238f15adbc', text='Learning Transferable Visual Models From Natural Language Supervision 21\\nModel Race Gender Age\\nFairFace Model 93.7 94.2 59.7\\nLinear Probe CLIP 93.4 96.5 63.8\\nZero-Shot CLIP 58.3 95.9 57.1\\nLinear Probe Instagram 90.8 93.2 54.2\\nTable 3. Percent accuracy on Race, Gender, and Age classiﬁcation\\nof images in FairFace category ‘White’Model Race Gender Age\\nFairFace Model 75.4 94.4 60.7\\nLinear Probe CLIP 92.8 97.7 63.1\\nZero-Shot CLIP 91.3 97.2 54.3\\nLinear Probe Instagram 87.2 93.9 54.1\\nTable 4. Percent accuracy on Race, Gender, and Age classiﬁcation\\nof images in FairFace categories ‘Black,’ ‘Indian,’ ‘East Asian,’\\n‘Southeast Asian,’ ‘Middle Eastern,’ and ‘Latino’ (grouped to-\\ngether as FairFace category ‘Non-White’)\\nMiddle Southeast East\\nModel Gender Black White Indian Latino Eastern Asian Asian Average\\nMale 96.9 96.4 98.7 96.5 98.9 96.2 96.9 97.2\\nLinear Probe CLIP Female 97.9 96.7 97.9 99.2 97.2 98.5 97.3 97.8\\n97.4 96.5 98.3 97.8 98.4 97.3 97.1 97.5\\nMale 96.3 96.4 97.7 97.2 98.3 95.5 96.8 96.9\\nZero-Shot CLIP Female 97.1 95.3 98.3 97.8 97.5 97.2 96.4 97.0\\n96.7 95.9 98.0 97.5 98.0 96.3 96.6\\nMale 92.5 94.8 96.2 93.1 96.0 92.7 93.4 94.1\\nLinear Probe Instagram Female 90.1 91.4 95.0 94.8 95.0 94.1 94.3 93.4\\n91.3 93.2 95.6 94.0 95.6 93.4 93.9\\nTable 5. Percent accuracy on gender classiﬁcation of images by FairFace race category\\n7.1. Bias\\nAlgorithmic decisions, training data, and choices about how\\nclasses are deﬁned and taxonomized (which we refer to in-\\nformally as “class design”) can all contribute to and amplify\\nsocial biases and inequalities resulting from the use of AI\\nsystems (Noble, 2018; Bechmann & Bowker, 2019; Bowker\\n& Star, 2000). Class design is particularly relevant to mod-\\nels like CLIP, since any developer can deﬁne a class and the\\nmodel will provide some result.\\nIn this section, we provide preliminary analysis of some\\nof the biases in CLIP, using bias probes inspired by those\\noutlined in Buolamwini & Gebru (2018) and K ¨arkk¨ainen\\n& Joo (2019). We also conduct exploratory bias research\\nintended to ﬁnd speciﬁc examples of biases in the model,\\nsimilar to that conducted by Solaiman et al. (2019).\\nWe start by analyzing the performance of Zero-Shot CLIP on\\nthe face image dataset FairFace (K ¨arkk¨ainen & Joo, 2019)6\\n6FairFace is a face image dataset designed to balance age, gen-\\nder, and race, in order to reduce asymmetries common in previous\\nface datasets. It categorizes gender into 2 groups: female and male\\nand race into 7 groups: White, Black, Indian, East Asian, Southeast\\nAsian, Middle Eastern, and Latino. There are inherent problems\\nwith race and gender classiﬁcations, as e.g. Bowker & Star (2000)as an initial bias probe, then probe the model further to\\nsurface additional biases and sources of biases, including\\nclass design.\\nWe evaluated two versions of CLIP on the FairFace dataset:\\na zero-shot CLIP model (“ZS CLIP”), and a logistic regres-\\nsion classiﬁer ﬁtted to FairFace’s dataset on top of CLIP’s\\nfeatures (“LR CLIP”). We ﬁnd that LR CLIP gets higher\\naccuracy on the FairFace dataset than both the ResNext-101\\n32x48d Instagram model (“Linear Probe Instagram”) (Ma-\\nhajan et al., 2018) and FairFace’s own model on most of the\\nclassiﬁcation tests we ran7. ZS CLIP’s performance varies\\nby category and is worse than that of FairFace’s model for a\\nfew categories, and better for others. (See Table 3 and Table\\n4).\\nand Keyes (2018) have shown. While FairFace’s dataset reduces\\nthe proportion of White faces, it still lacks representation of entire\\nlarge demographic groups, effectively erasing such categories. We\\nuse the 2 gender categories and 7 race categories deﬁned in the\\nFairFace dataset in a number of our experiments not in order to\\nreinforce or endorse the use of such reductive categories, but in\\norder to enable us to make comparisons to prior work.\\n7One challenge with this comparison is that the FairFace model\\nuses binary classes for race (“White” and “Non-White”), instead\\nof breaking down races into ﬁner-grained sub-groups.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3473e7e3-d4d4-493e-831f-c1a2eb97b773', embedding=None, metadata={'page_label': '22', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7b4f06472b24b51c5be815c092ad565f6bcee6d6d1030ef58cfe458c8bf6c51a', text='Learning Transferable Visual Models From Natural Language Supervision 22\\nMiddle Southeast East\\nCategory Black White Indian Latino Eastern Asian Asian\\nCrime-related Categories 16.4 24.9 24.4 10.8 19.7 4.4 1.3\\nNon-human Categories 14.4 5.5 7.6 3.7 2.0 1.9 0.0\\nTable 6. Percent of images classiﬁed into crime-related and non-human categories by FairFace Race category. The label set included 7\\nFairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories.\\nCategory Label Set 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 over 70\\nDefault Label Set 30.3 35.0 29.5 16.3 13.9 18.5 19.1 16.2 10.4\\nDefault Label Set + ‘child’ category 2.3 4.3 14.7 15.0 13.4 18.2 18.6 15.5 9.4\\nTable 7. Percent of images classiﬁed into crime-related and non-human categories by FairFace Age category, showing comparison between\\nresults obtained using a default label set and a label set to which the label ’child’ has been added. The default label set included 7 FairFace\\nrace categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories.\\nAdditionally, we test the performance of the LR CLIP and\\nZS CLIP models across intersectional race and gender cate-\\ngories as they are deﬁned in the FairFace dataset. We ﬁnd\\nthat model performance on gender classiﬁcation is above\\n95% for all race categories. Table 5 summarizes these re-\\nsults.\\nWhile LR CLIP achieves higher accuracy than the Linear\\nProbe Instagram model on the FairFace benchmark dataset\\nfor gender, race and age classiﬁcation of images by intersec-\\ntional categories, accuracy on benchmarks offers only one\\napproximation of algorithmic fairness, as Raji et al. (2020)\\nhave shown, and often fails as a meaningful measure of fair-\\nness in real world contexts. Even if a model has both higher\\naccuracy and lower disparities in performance on different\\nsub-groups, this does not mean it will have lower disparities\\nin impact (Scheuerman et al., 2019). For example, higher\\nperformance on underrepresented groups might be used by\\na company to justify their use of facial recognition, and to\\nthen deploy it ways that affect demographic groups dispro-\\nportionately. Our use of facial classiﬁcation benchmarks to\\nprobe for biases is not intended to imply that facial classi-\\nﬁcation is an unproblematic task, nor to endorse the use of\\nrace, age, or gender classiﬁcation in deployed contexts.\\nWe also probed the model using classiﬁcation terms with\\nhigh potential to cause representational harm, focusing on\\ndenigration harms in particular (Crawford, 2017). We car-\\nried out an experiment in which the ZS CLIP model was\\nrequired to classify 10,000 images from the FairFace dataset.\\nIn addition to the FairFace classes, we added in the follow-\\ning classes: ‘animal’, ‘gorilla’, ‘chimpanzee’, ‘orangutan’,\\n‘thief’, ‘criminal’ and ‘suspicious person’. The goal of this\\nexperiment was to check if harms of denigration dispropor-\\ntionately impact certain demographic subgroups.We found that 4.9% (conﬁdence intervals between 4.6%\\nand 5.4%) of the images were misclassiﬁed into one of\\nthe non-human classes we used in our probes (‘animal’,\\n‘chimpanzee’, ‘gorilla’, ‘orangutan’). Out of these, ‘Black’\\nimages had the highest misclassiﬁcation rate (approximately\\n14%; conﬁdence intervals between [12.6% and 16.4%])\\nwhile all other races had misclassiﬁcation rates under 8%.\\nPeople aged 0-20 years had the highest proportion being\\nclassiﬁed into this category at 14% .\\nWe also found that 16.5% of male images were misclassiﬁed\\ninto classes related to crime (‘thief’, ‘suspicious person’ and\\n‘criminal’) as compared to 9.8% of female images. Inter-\\nestingly, we found that people aged 0-20 years old were\\nmore likely to fall under these crime-related classes (approx-\\nimately 18%) compared to images of people in different\\nage ranges (approximately 12% for people aged 20-60 and\\n0% for people over 70). We found signiﬁcant disparities in\\nclassiﬁcations across races for crime related terms, which is\\ncaptured in Table 6.\\nGiven that we observed that people under 20 were the most\\nlikely to be classiﬁed in both the crime-related and non-\\nhuman animal categories, we carried out classiﬁcation for\\nthe images with the same classes but with an additional\\ncategory ‘child’ added to the categories. Our goal here\\nwas to see if this category would signiﬁcantly change the\\nbehaviour of the model and shift how the denigration harms\\nare distributed by age. We found that this drastically reduced\\nthe number of images of people under 20 classiﬁed in either\\ncrime-related categories or non-human animal categories\\n(Table 7). This points to how class design has the potential\\nto be a key factor determining both the model performance\\nand the unwanted biases or behaviour the model may exhibit\\nwhile also asks overarching questions about the use of face', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e799d0d8-3ae2-4929-85d5-aeb6e74ff277', embedding=None, metadata={'page_label': '23', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='60e7f68c9401be1a04a7e36e2fe6fcb12ae3a7a95684dd88eaf713cfdb4d9d08', text='Learning Transferable Visual Models From Natural Language Supervision 23\\nimages to automatically classify people along such lines\\n(y Arcas et al., 2017).\\nThe results of these probes can change based on the class\\ncategories one chooses to include as well as the speciﬁc\\nlanguage one uses to describe each class. Poor class design\\ncan lead to poor real world performance; this concern is\\nparticularly relevant to a model like CLIP, given how easily\\ndevelopers can design their own classes.\\nWe also carried out experiments similar to those outlined by\\nSchwemmer et al. (2020) to test how CLIP treated images\\nof men and women differently using images of Members\\nof Congress. As part of these experiments, we studied\\nhow certain additional design decisions such as deciding\\nthresholds for labels can impact the labels output by CLIP\\nand how biases manifest.\\nWe carried out three experiments - we tested for accuracy\\non gender classiﬁcation and we tested for how labels were\\ndifferentially distributed across two different label sets. For\\nour ﬁrst label set, we used a label set of 300 occupations and\\nfor our second label set we used a combined set of labels that\\nGoogle Cloud Vision, Amazon Rekognition and Microsoft\\nAzure Computer Vision returned for all the images.\\nWe ﬁrst simply looked into gender prediction performance\\nof the model on the images of Members of Congress, in\\norder to check to see if the model correctly recognized\\nmen as men and women as women given the image of a\\nperson who appeared to be in an ofﬁcial setting/position of\\npower. We found that the model got 100% accuracy on the\\nimages. This is slightly better performance than the model’s\\nperformance on the FairFace dataset. We hypothesize that\\none of the reasons for this is that all the images in the\\nMembers of Congress dataset were high-quality and clear,\\nwith the people clearly centered, unlike those in the FairFace\\ndataset.\\nIn order to study how the biases in returned labels depend on\\nthe thresholds set for label probability, we did an experiment\\nin which we set threshold values at 0.5% and 4.0%. We\\nfound that the lower threshold led to lower quality of labels.\\nHowever, even the differing distributions of labels under\\nthis threshold can hold signals for bias. For example, we\\nﬁnd that under the 0.5% threshold labels such as ‘nanny’\\nand ‘housekeeper’ start appearing for women whereas labels\\nsuch as ‘prisoner’ and ‘mobster’ start appearing for men.\\nThis points to gendered associations similar to those that\\nhave previously been found for occupations (Schwemmer\\net al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016).\\nAt the higher 4% threshold, the labels with the highest prob-\\nability across both genders include “lawmaker”, “legislator”\\nand “congressman”. However, the presence of these biases\\namongst lower probability labels nonetheless point to larger\\nquestions about what ‘sufﬁciently’ safe behaviour may looklike for deploying such systems.\\nWhen given the combined set of labels that Google Cloud\\nVision (GCV), Amazon Rekognition and Microsoft returned\\nfor all the images, similar to the biases Schwemmer et al.\\n(2020) found in GCV systems, we found our system also\\ndisproportionately attached labels to do with hair and ap-\\npearance in general to women more than men. For ex-\\nample, labels such as ‘brown hair’, ‘blonde’ and ‘blond’\\nappeared signiﬁcantly more often for women. Additionally,\\nCLIP attached some labels that described high status occu-\\npations disproportionately more often to men such as ‘ex-\\necutive’ and ‘doctor’. Out of the only four occupations that\\nit attached more often to women, three were ‘newscaster’,\\n‘television presenter’ and ‘newsreader’ and the fourth was\\n‘Judge’. This is again similar to the biases found in GCV\\nand points to historical gendered differences (Schwemmer\\net al., 2020).\\nInterestingly, when we lowered the threshold to 0.5% for\\nthis set of labels, we found that the labels disproportionately\\ndescribing men also shifted to appearance oriented words\\nsuch as ‘suit’, ‘tie’ and ‘necktie’ (Figure 18). Many occupa-\\ntion oriented words such as ‘military person’ and ‘executive’\\n- which were not used to describe images of women at the\\nhigher 4% threshold - were used for both men and women\\nat the lower 0.5% threshold, which could have caused the\\nchange in labels for men. The reverse was not true. Descrip-\\ntive words used to describe women were still uncommon\\namongst men.\\nDesign decisions at every stage of building a model impact\\nhow biases manifest and this is especially true for CLIP\\ngiven the ﬂexibility it offers. In addition to choices about\\ntraining data and model architecture, decisions about things\\nlike class designs and thresholding values can alter the labels\\na model outputs and as a result heighten or lower certain\\nkinds of harm, such as those described by Crawford (2017).\\nPeople designing and developing models and AI systems\\nhave considerable power. Decisions about things like class\\ndesign are a key determiner not only of model performance,\\nbut also of how and in what contexts model biases manifest.\\nThese experiments are not comprehensive. They illus-\\ntrate potential issues stemming from class design and other\\nsources of bias, and are intended to spark inquiry.\\n7.2. Surveillance\\nWe next sought to characterize model performance in re-\\nlation to a downstream task for which there is signiﬁcant\\nsocietal sensitivity: surveillance. Our analysis aims to better\\nembody the characterization approach described above and\\nto help orient the research community towards the potential\\nfuture impacts of increasingly general purpose computer\\nvision models and aid the development of norms and checks', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='197ba727-cea3-4eaf-96f1-acde7d8aaff3', embedding=None, metadata={'page_label': '24', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d9258cfebd2b6c0d4c6cdac4ae53ddb4e94e9920cff970e8cdcd3ed6d4a8dbde', text='Learning Transferable Visual Models From Natural Language Supervision 24\\n0 20 40 60 80 100\\nFrequency (%)blousepurplenewsreaderbangspinkpixie cutblack hairbob cutmagentahotlaughingblazerspokespersonblondepublic speakingsenior citizenlookingfemaleladywomanTop labels,\\nimages of women\\nWomen\\nMen\\n0 20 40 60 80 100\\nFrequency (%)yellownecktiekidfrownshouldertiedisplayelderphotographwalkingmilitary officerphotosuitfacial expressionheadblackplayerfacemalemanTop labels,\\nimages of men\\nWomen\\nMen\\nFigure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google\\nCloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were\\nidentiﬁed with χ2tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a\\ncertain label by gender.\\naround such systems. Our inclusion of surveillance is not\\nintended to indicate enthusiasm for this domain - rather, we\\nthink surveillance is an important domain to try to make\\npredictions about given its societal implications (Zuboff,\\n2015; Browne, 2015).\\nWe measure the model’s performance on classiﬁcation of\\nimages from CCTV cameras and zero-shot celebrity identiﬁ-\\ncation. We ﬁrst tested model performance on low-resolution\\nimages captured from surveillance cameras (e.g. CCTV\\ncameras). We used the VIRAT dataset (Oh et al., 2011) and\\ndata captured by Varadarajan & Odobez (2009), which both\\nconsist of real world outdoor scenes with non-actors.\\nGiven CLIP’s ﬂexible class construction, we tested 515\\nsurveillance images captured from 12 different video se-\\nquences on self-constructed general classes for coarse and\\nﬁne grained classiﬁcation. Coarse classiﬁcation required the\\nmodel to correctly identify the main subject of the image (i.e.\\ndetermine if the image was a picture of an empty parking\\nlot, school campus, etc.). For ﬁne-grained classiﬁcation, the\\nmodel had to choose between two options constructed to\\ndetermine if the model could identify the presence/absence\\nof smaller features in the image such as a person standing\\nin the corner.\\nFor coarse classiﬁcation, we constructed the classes by hand-\\ncaptioning the images ourselves to describe the contents\\nof the image and there were always at least 6 options forthe model to choose from. Additionally, we carried out a\\n‘stress test’ where the class set included at least one more\\ncaption for something that was ‘close’ to the image (for\\nexample, ‘parking lot with white car’ vs. ‘parking lot with\\nred car’). We found that the model had a top-1 accuracy\\nof 91.8% on the CCTV images for the initial evaluation.\\nThe accuracy dropped signiﬁcantly to 51.1% for the second\\nevaluation, with the model incorrectly choosing the ‘close’\\nanswer 40.7% of the time.\\nFor ﬁne-grained detection, the zero-shot model performed\\npoorly, with results near random. Note that this experiment\\nwas targeted only towards detecting the presence or absence\\nof small objects in image sequences.\\nWe also tested CLIP’s zero-shot performance for ‘in the\\nwild’ identity detection using the CelebA dataset8. We did\\nthis to evaluate the model’s performance for identity detec-\\ntion using just the publicly available data it was pre-trained\\non. While we tested this on a dataset of celebrities who have\\na larger number of images on the internet, we hypothesize\\nthat the number of images in the pre-training data needed\\nfor the model to associate faces with names will keep de-\\ncreasing as models get more powerful (see Table 8), which\\nhas signiﬁcant societal implications (Garvie, 2019). This\\n8Note: The CelebA dataset is more representative of faces with\\nlighter skin tones. Due to the nature of the dataset, we were not\\nable to control for race, gender, age, etc.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ba8f6804-a20d-4681-9f94-f8a795a02dba', embedding=None, metadata={'page_label': '25', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='18c80e8ec27ee14e3159c909ef8443fe0b69cb24e2d640af8709f87905c1ed05', text='Learning Transferable Visual Models From Natural Language Supervision 25\\nModel 100 Classes 1k Classes 2k Classes\\nCLIP L/14 59.2 43.3 42.2\\nCLIP RN50x64 56.4 39.5 38.4\\nCLIP RN50x16 52.7 37.4 36.3\\nCLIP RN50x4 52.8 38.1 37.3\\nTable 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy\\nmirrors recent developments in natural language processing,\\nin which recent large language models trained on Internet\\ndata often exhibit a surprising ability to provide informa-\\ntion related to relatively minor public ﬁgures (Brown et al.,\\n2020).\\nWe found that the model had 59.2% top-1 accuracy out\\nof 100 possible classes for ‘in the wild’ 8k celebrity im-\\nages. However, this performance dropped to 43.3% when\\nwe increased our class sizes to 1k celebrity names. This\\nperformance is not competitive when compared to produc-\\ntion level models such as Google’s Celebrity Recognition\\n(Google). However, what makes these results noteworthy is\\nthat this analysis was done using only zero-shot identiﬁca-\\ntion capabilities based on names inferred from pre-training\\ndata - we didn’t use any additional task-speciﬁc dataset, and\\nso the (relatively) strong results further indicate that before\\ndeploying multimodal models, people will need to carefully\\nstudy them for behaviors in a given context and domain.\\nCLIP offers signiﬁcant beneﬁt for tasks that have relatively\\nlittle data given its zero-shot capabilities. However, large\\ndatasets and high performing supervised models exist for\\nmany in-demand surveillance tasks such as facial recogni-\\ntion. As a result, CLIP’s comparative appeal for such uses\\nis low. Additionally, CLIP is not designed for common\\nsurveillance-relevant tasks like object detection and seman-\\ntic segmentation. This means it has limited use for certain\\nsurveillance tasks when models that are designed with these\\nuses in mind such as Detectron2 (Wu et al., 2019) are widely\\navailable.\\nHowever, CLIP does unlock a certain aspect of usability\\ngiven how it removes the need for training data. Thus, CLIP\\nand similar models could enable bespoke, niche surveillance\\nuse cases for which no well-tailored models or datasets exist,\\nand could lower the skill requirements to build such appli-\\ncations. As our experiments show, ZS CLIP displays non-\\ntrivial, but not exceptional, performance on a few surveil-\\nlance relevant tasks today.\\n7.3. Future Work\\nThis preliminary analysis is intended to illustrate some of\\nthe challenges that general purpose computer vision models\\npose and to give a glimpse into their biases and impacts.We hope that this work motivates future research on the\\ncharacterization of the capabilities, shortcomings, and biases\\nof such models, and we are excited to engage with the\\nresearch community on such questions.\\nWe believe one good step forward is community exploration\\nto further characterize the capabilities of models like CLIP\\nand - crucially - identify application areas where they have\\npromising performance and areas where they may have\\nreduced performance9. This process of characterization can\\nhelp researchers increase the likelihood models are used\\nbeneﬁcially by:\\n•Identifying potentially beneﬁcial downstream uses of\\nmodels early in the research process, enabling other\\nresearchers to think about applications.\\n•Surfacing tasks with signiﬁcant sensitivity and a large\\nset of societal stakeholders, which may call for inter-\\nvention by policymakers.\\n•Better characterizing biases in models, alerting other\\nresearchers to areas of concern and areas for interven-\\ntions.\\n•Creating suites of tests to evaluate systems like CLIP\\non, so we can better characterize model capabilities\\nearlier in the development cycle.\\n•Identifying potential failure modes and areas for further\\nwork.\\nWe plan to contribute to this work, and hope this analysis\\nprovides some motivating examples for subsequent research.\\n8. Related Work\\nAny model that leverages written, spoken, signed or any\\nother form of human language as part of its training signal\\nis arguably using natural language as a source of supervi-\\nsion. This is an admittedly extremely broad area and covers\\nmost work in the ﬁeld of distributional semantics including\\ntopic models (Blei et al., 2003), word, sentence, and para-\\ngraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &\\nMikolov, 2014), and language models (Bengio et al., 2003).\\nIt also includes much of the broader ﬁeld of NLP that deals\\nwith predicting or modeling sequences of natural language\\nin some way. Work in NLP intentionally leveraging natural\\nlanguage supervision in the form of explanations, feedback,\\ninstructions, and advice for tasks such as classiﬁcation (as\\nopposed to the commonly used representation of supervision\\nas a set of arbitrarily encoded discrete category labels) has\\n9A model could be unﬁt for use due to inadequate performance\\nor due to the inappropriateness of AI use in the application area\\nitself.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='60219bdb-5605-49a9-9750-b7f0d35760f8', embedding=None, metadata={'page_label': '26', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='738eb4a39ad346493b23aa702ae7fb536767f3ce9887bdf106eb186154a8fbd1', text='Learning Transferable Visual Models From Natural Language Supervision 26\\nbeen explored in many creative and advanced ways. Dialog\\nbased learning (Weston, 2016; Li et al., 2016; Hancock et al.,\\n2019) develops techniques to learn from interactive natural\\nlanguage feedback in dialog. Several papers have leveraged\\nsemantic parsing to convert natural language explanations\\ninto features (Srivastava et al., 2017) or additional training\\nlabels (Hancock et al., 2018). More recently, ExpBERT\\n(Murty et al., 2020) uses feature representations produced\\nby conditioning a deep contextual language model on nat-\\nural language explanations and descriptions of relations to\\nimprove performance on the task of relation extraction.\\nCLIP is an example of using natural language as a training\\nsignal for learning about a domain other than language. In\\nthis context, the earliest use of the term natural language\\nsupervision that we are aware of is the work of Ramanathan\\net al. (2013) which showed that natural language descrip-\\ntions could be used along side other sources of supervision\\nto improve performance on the task of video event under-\\nstanding. However, as mentioned in the introduction and\\napproach section, methods of leveraging natural language\\ndescriptions in computer vision well predate the use of this\\nspeciﬁc term, especially for image retrieval (Mori et al.,\\n1999) and object classiﬁcation (Wang et al., 2009). Other\\nearly work leveraged tags (but not natural language) asso-\\nciated with images for the task of semantic segmentation\\n(Barnard et al., 2003). More recently, He & Peng (2017)\\nand Liang et al. (2020) demonstrated using natural language\\ndescriptions and explanations to improve ﬁne-grained vi-\\nsual classiﬁcation of birds. Others have investigated how\\ngrounded language can be used to improve visual represen-\\ntations and classiﬁers on the ShapeWorld dataset (Kuhnle\\n& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).\\nFinally, techniques which combine natural language with\\nreinforcement learning environments (Narasimhan et al.,\\n2015) have demonstrated exciting emergent behaviors such\\nas systematically accomplishing zero-shot tasks (Hill et al.,\\n2019).\\nCLIP’s pre-training task optimizes for text-image retrieval.\\nThis areas of research dates back to the mid-90s with the\\npreviously mentioned Mori et al. (1999) as representative of\\nearly work. While initial efforts focused primarily on predic-\\ntive objectives over time research shifted towards learning\\njoint multi-modal embedding spaces with techniques like\\nkernel Canonical Correlation Analysis and various ranking\\nobjectives (Weston et al., 2010; Socher & Fei-Fei, 2010;\\nHodosh et al., 2013). Over time work explored many combi-\\nnations of training objective, transfer, and more expressive\\nmodels and steadily improved performance (Frome et al.,\\n2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,\\n2014; Faghri et al., 2017).\\nOther work has leveraged natural language supervision for\\ndomains other than images. Stroud et al. (2020) exploreslarge scale representation learning by training a system to\\npair descriptive text with videos instead of images. Several\\nworks have explored using dense spoken natural language\\nsupervision for videos (Miech et al., 2019; 2020b). When\\nconsidered together with CLIP, these works suggest that\\nlarge scale natural language supervision is a promising way\\nto learn high quality perceptual systems for many domains.\\nAlayrac et al. (2020) extended this line of work to an addi-\\ntional modality by adding raw audio as an additional super-\\nvision source and demonstrated beneﬁts from combining all\\nthree sources of supervision.\\nAs part of our work on CLIP we also construct a new dataset\\nof image-text pairs. Modern work on image-text retrieval\\nhas relied on a set of crowd-sourced sentence level im-\\nage caption evaluation datasets like Pascal1K (Rashtchian\\net al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K\\n(Young et al., 2014). However, these datasets are still rel-\\natively small and limit achievable performance. Several\\nmethods have been proposed to create larger datasets au-\\ntomatically with Ordonez et al. (2011) as a notable early\\nexample. In the deep learning era, Mithun et al. (2018)\\ndemonstrated an additional set of (image, text) pairs col-\\nlected from the internet could improve retrieval performance\\nand several new automatically constructed datasets such as\\nConceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,\\n2020), and OCR-CC (Yang et al., 2020) have been created.\\nHowever, these datasets still use signiﬁcantly more aggres-\\nsive ﬁltering or are designed for a speciﬁc task such as OCR\\nand as a result are still much smaller than WIT with between\\n1 and 10 million training examples.\\nA related idea to CLIP is webly supervised learning. This\\nline of work queries image search engines to build image\\ndatasets by querying for terms and uses the queries as the\\nlabels for the returned images (Fergus et al., 2005). Classi-\\nﬁers trained on these large but noisily labeled datasets can\\nbe competitive with those trained on smaller carefully la-\\nbeled datasets. These image-query pairs are also often used\\nto improve performance on standard datasets as additional\\ntraining data (Chen & Gupta, 2015). CLIP also uses search\\nqueries as part of its dataset creation process. However\\nCLIP only uses full text sequences co-occuring with images\\nas supervision rather than just the queries, which are often\\nonly a single word or short n-gram. We also restrict this step\\nin CLIP to text only querying for sub-string matches while\\nmost webly supervised work uses standard image search\\nengines which have their own complex retrieval and ﬁlter-\\ning pipelines that often involve computer vision systems.\\nOf this line of work, Learning Everything about Anything:\\nWebly-Supervised Visual Concept Learning (Divvala et al.,\\n2014) has a notably similar ambition and goal as CLIP.\\nFinally, CLIP is related to a recent burst of activity on learn-\\ning joint models of vision and language (Lu et al., 2019; Tan', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='39adabbe-47ba-43d6-99a2-9bc4e03b2188', embedding=None, metadata={'page_label': '27', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='99b625d619ec2cbc9adab44c04bb1e8028f0304a728efaa57c9404ba18debf18', text='Learning Transferable Visual Models From Natural Language Supervision 27\\n& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.,\\n2020). This line of work focuses on richly connecting vision\\nand language in order to solve complex downstream tasks\\nsuch as visual question answering, visual commonsense\\nreasoning, or multimodal entailment. These approaches\\nleverage impressively engineered models which combine 3\\n(or more) pre-trained subsystems, typically an image feature\\nmodel, a region proposal / object detection model, and a\\npre-trained masked language model such as BERT. These\\nsystems are then jointly ﬁne-tuned via various training objec-\\ntives on image-text pairs and applied to the aforementioned\\ntasks and achieve impressive results. CLIP is instead fo-\\ncused on learning visual models from scratch via natural\\nlanguage supervision and does not densely connect the two\\ndomains with a joint attention model. The only interaction\\nin a CLIP model between the image and text domain is a\\nsingle dot product in a learned joint embedding space. We\\nare excited to see CLIP hybridized with this line of work.\\n9. Conclusion\\nWe have investigated whether it is possible to transfer the\\nsuccess of task-agnostic web-scale pre-training in NLP to\\nanother domain. We ﬁnd that adopting this formula re-\\nsults in similar behaviors emerging in the ﬁeld of computer\\nvision and discuss the social implications of this line of\\nresearch. In order to optimize their training objective, CLIP\\nmodels learn to perform a wide variety of tasks during pre-\\ntraining. This task learning can then be leveraged via natural\\nlanguage prompting to enable zero-shot transfer to many\\nexisting datasets. At sufﬁcient scale, the performance of this\\napproach can be competitive with task-speciﬁc supervised\\nmodels although there is still room for much improvement.\\nACKNOWLEDGMENTS\\nWe’d like to thank the millions of people involved in creating\\nthe data CLIP is trained on. We’d also like to thank Susan\\nZhang for her work on image conditional language models\\nwhile at OpenAI, Ishaan Gulrajani for catching an error in\\nthe pseudocode, and Irene Solaiman, Miles Brundage, and\\nGillian Hadﬁeld for their thoughtful feedback on the broader\\nimpacts section of the paper. We are also grateful to the\\nAcceleration and Supercomputing teams at OpenAI for their\\ncritical work on software and hardware infrastructure this\\nproject used. Finally, we’d also like to thank the developers\\nof the many software packages used throughout this project\\nincluding, but not limited, to Numpy (Harris et al., 2020),\\nSciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\\nFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\\npandas (pandas development team, 2020), and scikit-learn\\n(Pedregosa et al., 2011).References\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\\nTensorﬂow: A system for large-scale machine learning. In\\n12th{USENIX}symposium on operating systems design\\nand implementation ( {OSDI}16), pp. 265–283, 2016.\\nAlayrac, J.-B., Recasens, A., Schneider, R., Arandjelovi ´c,\\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\\nand Zisserman, A. Self-supervised multimodal versatile\\nnetworks. arXiv preprint arXiv:2006.16228 , 2020.\\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\\nS., and Nguyen, A. Strike (with) a pose: Neural networks\\nare easily fooled by strange poses of familiar objects. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition , pp. 4845–4854, 2019.\\nAndreas, J., Klein, D., and Levine, S. Learning with latent\\nlanguage. arXiv preprint arXiv:1711.00482 , 2017.\\nAssiri, Y . Stochastic optimization of plain convolutional\\nneural networks with simple methods. arXiv preprint\\narXiv:2001.08856 , 2020.\\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning\\nrepresentations by maximizing mutual information across\\nviews. In Advances in Neural Information Processing\\nSystems , pp. 15535–15545, 2019.\\nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\\nfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A\\nlarge-scale bias-controlled dataset for pushing the lim-\\nits of object recognition models. In Advances in Neural\\nInformation Processing Systems , pp. 9453–9463, 2019.\\nBarnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,\\nD. M., and Jordan, M. I. Matching words and pictures.\\nJournal of machine learning research , 3(Feb):1107–1135,\\n2003.\\nBechmann, A. and Bowker, G. C. Unsupervised by any\\nother name: Hidden layers of knowledge production in\\nartiﬁcial intelligence on social media. Big Data & Society ,\\n6(1):205395171881956, January 2019. doi: 10.1177/\\n2053951718819569. URL https://doi.org/10.\\n1177/2053951718819569 .\\nBengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A\\nneural probabilistic language model. Journal of machine\\nlearning research , 3(Feb):1137–1155, 2003.\\nBhargava, S. and Forsyth, D. Exposing and correcting the\\ngender bias in image captioning datasets and models.\\narXiv preprint arXiv:1912.00578 , 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='65e88c7f-dd78-4dd1-b6b0-baa63a138231', embedding=None, metadata={'page_label': '28', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e32f8a4a27713264b11ba4aca6255914514048c981fa36964685777d089b1684', text='Learning Transferable Visual Models From Natural Language Supervision 28\\nBlei, D. M., Ng, A. Y ., and Jordan, M. I. Latent dirichlet\\nallocation. Journal of machine Learning research , 3(Jan):\\n993–1022, 2003.\\nBolukbasi, T., Chang, K.-W., Zou, J. Y ., Saligrama, V ., and\\nKalai, A. T. Man is to computer programmer as woman\\nis to homemaker? debiasing word embeddings. Advances\\nin neural information processing systems , 29:4349–4357,\\n2016.\\nBowker, G. C. and Star, S. L. Sorting things out: Classiﬁca-\\ntion and its consequences . MIT press, 2000.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\narXiv preprint arXiv:2005.14165 , 2020.\\nBrowne, S. Dark Matters: Surveillance of Blackness . Duke\\nUniversity Press, 2015.\\nBulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\\nvisual representations with caption annotations. arXiv\\ne-prints , pp. arXiv–2008, 2020.\\nBuolamwini, J. and Gebru, T. Gender shades: Intersec-\\ntional accuracy disparities in commercial gender classi-\\nﬁcation. In Conference on fairness, accountability and\\ntransparency , pp. 77–91, 2018.\\nCarreira, J., Noland, E., Hillier, C., and Zisserman, A. A\\nshort note on the kinetics-700 human action dataset. arXiv\\npreprint arXiv:1907.06987 , 2019.\\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\\nD., and Sutskever, I. Generative pretraining from pixels.\\nInInternational Conference on Machine Learning , pp.\\n1691–1703. PMLR, 2020a.\\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training\\ndeep nets with sublinear memory cost. arXiv preprint\\narXiv:1604.06174 , 2016.\\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\\nsimple framework for contrastive learning of visual rep-\\nresentations. arXiv preprint arXiv:2002.05709 , 2020b.\\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\\nHinton, G. Big self-supervised models are strong semi-\\nsupervised learners. arXiv preprint arXiv:2006.10029 ,\\n2020c.\\nChen, X. and Gupta, A. Webly supervised learning of\\nconvolutional networks. In Proceedings of the IEEE\\nInternational Conference on Computer Vision , pp. 1431–\\n1439, 2015.Chen, X., Fan, H., Girshick, R., and He, K. Improved\\nbaselines with momentum contrastive learning. arXiv\\npreprint arXiv:2003.04297 , 2020d.\\nChen, Y .-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\\nCheng, Y ., and Liu, J. Uniter: Learning universal image-\\ntext representations. arXiv preprint arXiv:1909.11740 ,\\n2019.\\nCheng, G., Han, J., and Lu, X. Remote sensing image scene\\nclassiﬁcation: Benchmark and state of the art. Proceed-\\nings of the IEEE , 105(10):1865–1883, 2017.\\nChoi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J.,\\nand Dahl, G. E. On empirical comparisons of optimiz-\\ners for deep learning. arXiv preprint arXiv:1910.05446 ,\\n2019.\\nCoates, A., Ng, A., and Lee, H. An analysis of single-\\nlayer networks in unsupervised feature learning. In Pro-\\nceedings of the fourteenth international conference on\\nartiﬁcial intelligence and statistics , pp. 215–223, 2011.\\nCrawford, K. The trouble with bias. NIPS 2017\\nKeynote , 2017. URL https://www.youtube.com/\\nwatch?v=fMym_BKWQzk .\\nDai, A. M. and Le, Q. V . Semi-supervised sequence learning.\\nInAdvances in neural information processing systems ,\\npp. 3079–3087, 2015.\\nD’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\\nJ., Hoffman, M. D., et al. Underspeciﬁcation presents\\nchallenges for credibility in modern machine learning.\\narXiv preprint arXiv:2011.03395 , 2020.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\\nFei, L. ImageNet: A Large-Scale Hierarchical Image\\nDatabase. In CVPR09 , 2009.\\nDeng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\\nand Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www.\\nimage-net.org/challenges/LSVRC/2012/ .\\nDesai, K. and Johnson, J. Virtex: Learning visual rep-\\nresentations from textual annotations. arXiv preprint\\narXiv:2006.06666 , 2020.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805 ,\\n2018.\\nDhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\\nand Sutskever, I. Jukebox: A generative model for music.\\narXiv preprint arXiv:2005.00341 , 2020.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='82779b78-72c6-4807-8679-f4e798ceaac5', embedding=None, metadata={'page_label': '29', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='0591cd4f22a939218ed8c6c3b7f376f7ed0f25d0f7523ac6d20be50f3435800e', text='Learning Transferable Visual Models From Natural Language Supervision 29\\nDivvala, S. K., Farhadi, A., and Guestrin, C. Learning\\neverything about anything: Webly-supervised visual con-\\ncept learning. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition , pp. 3270–\\n3277, 2014.\\nDodge, S. and Karam, L. A study and comparison of human\\nand deep learning recognition performance under visual\\ndistortions. In 2017 26th international conference on\\ncomputer communication and networks (ICCCN) , pp. 1–\\n7. IEEE, 2017.\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al. An image is worth 16x16\\nwords: Transformers for image recognition at scale. arXiv\\npreprint arXiv:2010.11929 , 2020.\\nElhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\\nﬁer: Zero-shot learning using purely textual descriptions.\\nInProceedings of the IEEE International Conference on\\nComputer Vision , pp. 2584–2591, 2013.\\nFaghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\\nproving visual-semantic embeddings with hard negatives.\\narXiv preprint arXiv:1707.05612 , 2017.\\nFergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\\ning object categories from google’s image search. In\\nTenth IEEE International Conference on Computer Vision\\n(ICCV’05) Volume 1 , volume 2, pp. 1816–1823. IEEE,\\n2005.\\nFrome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\\nRanzato, M., and Mikolov, T. Devise: A deep visual-\\nsemantic embedding model. In Advances in neural infor-\\nmation processing systems , pp. 2121–2129, 2013.\\nGan, Z., Chen, Y .-C., Li, L., Zhu, C., Cheng, Y ., and Liu, J.\\nLarge-scale adversarial training for vision-and-language\\nrepresentation learning. arXiv preprint arXiv:2006.06195 ,\\n2020.\\nGao, T., Fisch, A., and Chen, D. Making pre-trained lan-\\nguage models better few-shot learners. arXiv preprint\\narXiv:2012.15723 , 2020.\\nGarvie, C., May 2019. URL https://www.\\nflawedfacedata.com/ .\\nGeiger, A., Lenz, P., and Urtasun, R. Are we ready for\\nautonomous driving? the kitti vision benchmark suite. In\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , 2012.\\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\\nmann, F. A., and Brendel, W. Imagenet-trained cnns arebiased towards texture; increasing shape bias improves ac-\\ncuracy and robustness. arXiv preprint arXiv:1811.12231 ,\\n2018.\\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,\\nBrendel, W., Bethge, M., and Wichmann, F. A. Short-\\ncut learning in deep neural networks. arXiv preprint\\narXiv:2004.07780 , 2020.\\nGomez, L., Patel, Y ., Rusi ˜nol, M., Karatzas, D., and Jawahar,\\nC. Self-supervised learning of visual features through\\nembedding images into text topic spaces. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition , pp. 4230–4239, 2017.\\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\\ning and harnessing adversarial examples. arXiv preprint\\narXiv:1412.6572 , 2014.\\nGoodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\\nMirza, M., Hamner, B., Cukierski, W., Tang, Y ., Thaler,\\nD., Lee, D.-H., et al. Challenges in representation learn-\\ning: A report on three machine learning contests. Neural\\nNetworks , 64:59–63, 2015.\\nGoogle. Google cloud api: Celebrity recognition. URL\\nhttps://cloud.google.com/vision/docs/\\ncelebrity-recognition .\\nGriewank, A. and Walther, A. Algorithm 799: revolve: an\\nimplementation of checkpointing for the reverse or ad-\\njoint mode of computational differentiation. ACM Trans-\\nactions on Mathematical Software (TOMS) , 26(1):19–45,\\n2000.\\nGrill, J.-B., Strub, F., Altch ´e, F., Tallec, C., Richemond,\\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733 , 2020.\\nHa, D., Dai, A., and Le, Q. V . Hypernetworks. arXiv\\npreprint arXiv:1609.09106 , 2016.\\nHancock, B., Bringmann, M., Varma, P., Liang, P., Wang,\\nS., and R ´e, C. Training classiﬁers with natural language\\nexplanations. In Proceedings of the conference. Associ-\\nation for Computational Linguistics. Meeting , volume\\n2018, pp. 1884. NIH Public Access, 2018.\\nHancock, B., Bordes, A., Mazare, P.-E., and Weston, J.\\nLearning from dialogue after deployment: Feed yourself,\\nchatbot! arXiv preprint arXiv:1901.05415 , 2019.\\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\\nKerkwijk, M. H., Brett, M., Haldane, A., Fern ´andez del', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5f533987-c817-47d8-9d17-17c7557e92ad', embedding=None, metadata={'page_label': '30', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='0074ed125ab50e6d6c200ab4e83160ffb05049f56951cbbfa2da5a5e682832f7', text='Learning Transferable Visual Models From Natural Language Supervision 30\\nR´ıo, J., Wiebe, M., Peterson, P., G ´erard-Marchant, P.,\\nSheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\\nGohlke, C., and Oliphant, T. E. Array programming\\nwith NumPy. Nature , 585:357–362, 2020. doi: 10.1038/\\ns41586-020-2649-2.\\nHays, J. and Efros, A. A. Im2gps: estimating geographic\\ninformation from a single image. In 2008 ieee confer-\\nence on computer vision and pattern recognition , pp. 1–8.\\nIEEE, 2008.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\\ninto rectiﬁers: Surpassing human-level performance on\\nimagenet classiﬁcation. In Proceedings of the IEEE inter-\\nnational conference on computer vision , pp. 1026–1034,\\n2015.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition ,\\npp. 770–778, 2016a.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition ,\\npp. 770–778, 2016b.\\nHe, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo-\\nmentum contrast for unsupervised visual representation\\nlearning. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pp. 9729–\\n9738, 2020.\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classiﬁcation with convolutional\\nneural networks. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition , pp. 558–\\n567, 2019.\\nHe, X. and Peng, Y . Fine-grained image classiﬁcation via\\ncombining vision and language. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition , pp. 5994–6002, 2017.\\nHelber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\\nA novel dataset and deep learning benchmark for land\\nuse and land cover classiﬁcation. IEEE Journal of Se-\\nlected Topics in Applied Earth Observations and Remote\\nSensing , 12(7):2217–2226, 2019.\\nHenaff, O. Data-efﬁcient image recognition with contrastive\\npredictive coding. In International Conference on Ma-\\nchine Learning , pp. 4182–4192. PMLR, 2020.\\nHendrycks, D. and Dietterich, T. Benchmarking neural\\nnetwork robustness to common corruptions and perturba-\\ntions. arXiv preprint arXiv:1903.12261 , 2019.Hendrycks, D. and Gimpel, K. Gaussian error linear units\\n(gelus). arXiv preprint arXiv:1606.08415 , 2016.\\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\\nSong, D. Natural adversarial examples. arXiv preprint\\narXiv:1907.07174 , 2019.\\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\\nDorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\\net al. The many faces of robustness: A critical analy-\\nsis of out-of-distribution generalization. arXiv preprint\\narXiv:2006.16241 , 2020a.\\nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\\nR., and Song, D. Pretrained transformers improve out-of-\\ndistribution robustness. arXiv preprint arXiv:2004.06100 ,\\n2020b.\\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\\nKianinejad, H., Patwary, M., Ali, M., Yang, Y ., and Zhou,\\nY . Deep learning scaling is predictable, empirically. arXiv\\npreprint arXiv:1712.00409 , 2017.\\nHill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\\nM., McClelland, J. L., and Santoro, A. Environmental\\ndrivers of systematicity and generalization in a situated\\nagent. In International Conference on Learning Repre-\\nsentations , 2019.\\nHodosh, M., Young, P., and Hockenmaier, J. Framing image\\ndescription as a ranking task: Data, models and evaluation\\nmetrics. Journal of Artiﬁcial Intelligence Research , 47:\\n853–899, 2013.\\nHongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\\nEnhancing image geolocalization by combinatorial parti-\\ntioning of maps. In Proceedings of the European Confer-\\nence on Computer Vision (ECCV) , pp. 536–551, 2018.\\nHoward, J. and Ruder, S. Universal language model\\nﬁne-tuning for text classiﬁcation. arXiv preprint\\narXiv:1801.06146 , 2018.\\nIlyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\\nB., and Madry, A. Adversarial examples are not bugs,\\nthey are features. In Advances in Neural Information\\nProcessing Systems , pp. 125–136, 2019.\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\narXiv preprint arXiv:1502.03167 , 2015.\\nJaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\\nA. Deep structured output learning for unconstrained text\\nrecognition. arXiv preprint arXiv:1412.5903 , 2014.\\nJaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial\\ntransformer networks. Advances in neural information\\nprocessing systems , 28:2017–2025, 2015.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cc4045a5-d702-4f6a-8575-767c165c7f2f', embedding=None, metadata={'page_label': '31', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7bd75ef3ad18527950e0c138e815e0f18dd3d7e83771078cdaa8baa7cb75d3c0', text='Learning Transferable Visual Models From Natural Language Supervision 31\\nJohnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\\nnostic dataset for compositional language and elementary\\nvisual reasoning. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition , pp.\\n2901–2910, 2017.\\nJoulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\\nLearning visual features from large weakly supervised\\ndata. In European Conference on Computer Vision , pp.\\n67–84. Springer, 2016.\\nKalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\\nmodeling in 3d cnn architectures with bert for action\\nrecognition. arXiv preprint arXiv:2008.01232 , 2020.\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models.\\narXiv preprint arXiv:2001.08361 , 2020.\\nKarpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment\\nembeddings for bidirectional image sentence mapping.\\nInAdvances in neural information processing systems ,\\npp. 1889–1897, 2014.\\nKeyes, O. The misgendering machines: Trans/hci implica-\\ntions of automatic gender recognition. Proceedings of the\\nACM on Human-Computer Interaction , 2(CSCW):1–22,\\n2018.\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\narXiv preprint arXiv:2005.04790 , 2020.\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. arXiv preprint arXiv:1412.6980 , 2014.\\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying\\nvisual-semantic embeddings with multimodal neural lan-\\nguage models. arXiv preprint arXiv:1411.2539 , 2014.\\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun,\\nR., Torralba, A., and Fidler, S. Skip-thought vectors.\\nAdvances in neural information processing systems , 28:\\n3294–3302, 2015.\\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\\nJ., Gelly, S., and Houlsby, N. Large scale learning of\\ngeneral visual representations for transfer. arXiv preprint\\narXiv:1912.11370 , 2019.\\nKornblith, S., Shlens, J., and Le, Q. V . Do better imagenet\\nmodels transfer better? In Proceedings of the IEEE\\nconference on computer vision and pattern recognition ,\\npp. 2661–2671, 2019.Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K.,\\nKravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma,\\nD. A., et al. Visual genome: Connecting language and\\nvision using crowdsourced dense image annotations. In-\\nternational journal of computer vision , 123(1):32–73,\\n2017.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\\nclassiﬁcation with deep convolutional neural networks.\\nInAdvances in neural information processing systems ,\\npp. 1097–1105, 2012.\\nKuhnle, A. and Copestake, A. Shapeworld-a new test\\nmethodology for multimodal language understanding.\\narXiv preprint arXiv:1704.04517 , 2017.\\nK¨arkk¨ainen, K. and Joo, J. Fairface: Face attribute dataset\\nfor balanced race, gender, and age, 2019.\\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\\nman, S. J. Building machines that learn and think like\\npeople, 2016.\\nLampert, C. H., Nickisch, H., and Harmeling, S. Learning\\nto detect unseen object classes by between-class attribute\\ntransfer. In 2009 IEEE Conference on Computer Vision\\nand Pattern Recognition , pp. 951–958. IEEE, 2009.\\nLarochelle, H., Erhan, D., and Bengio, Y . Zero-data learning\\nof new tasks. 2008.\\nLe, Q. and Mikolov, T. Distributed representations of sen-\\ntences and documents. In International conference on\\nmachine learning , pp. 1188–1196, 2014.\\nLeCun, Y . The mnist database of handwritten digits.\\nhttp://yann. lecun. com/exdb/mnist/ .\\nLee, D.-H. Pseudo-label: The simple and efﬁcient semi-\\nsupervised learning method for deep neural networks.\\nLei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep\\nzero-shot convolutional neural networks using textual\\ndescriptions. In Proceedings of the IEEE International\\nConference on Computer Vision , pp. 4247–4255, 2015.\\nLi, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\\nvisual n-grams from web data. In Proceedings of the\\nIEEE International Conference on Computer Vision , pp.\\n4183–4192, 2017.\\nLi, G., Duan, N., Fang, Y ., Gong, M., and Jiang, D.\\nUnicoder-vl: A universal encoder for vision and language\\nby cross-modal pre-training. 2020a.\\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J.\\nLearning through dialogue interactions by asking ques-\\ntions. arXiv preprint arXiv:1612.04936 , 2016.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='74c3318a-8f6a-476a-bffb-2dc3d9b880c6', embedding=None, metadata={'page_label': '32', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='279e0084c9e20521ebcbb838b7173e0f1c68c25ba654d5a87de78c7d0b75ce0b', text='Learning Transferable Visual Models From Natural Language Supervision 32\\nLi, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\\nL., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-\\nsemantics aligned pre-training for vision-language tasks.\\narXiv preprint arXiv:2004.06165 , 2020b.\\nLiang, W., Zou, J., and Yu, Z. Alice: Active learning with\\ncontrastive natural language explanations. arXiv preprint\\narXiv:2009.10259 , 2020.\\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\\nmanan, D., Doll ´ar, P., and Zitnick, C. L. Microsoft coco:\\nCommon objects in context. In European conference on\\ncomputer vision , pp. 740–755. Springer, 2014.\\nLinzen, T. How can we accelerate progress towards\\nhuman-like linguistic generalization? arXiv preprint\\narXiv:2005.00955 , 2020.\\nLippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\\ntoniou, G., Shutova, E., and Yannakoudakis, H. A mul-\\ntimodal framework for the detection of hateful memes.\\narXiv preprint arXiv:2012.12871 , 2020.\\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\\nssi, R., Kaiser, L., and Shazeer, N. Generating\\nwikipedia by summarizing long sequences. arXiv preprint\\narXiv:1801.10198 , 2018.\\nLocatello, F., Bauer, S., Lucic, M., R ¨atsch, G., Gelly, S.,\\nSch¨olkopf, B., and Bachem, O. A sober look at the\\nunsupervised learning of disentangled representations\\nand their evaluation. arXiv preprint arXiv:2010.14766 ,\\n2020.\\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gra-\\ndient descent with warm restarts. arXiv preprint\\narXiv:1608.03983 , 2016.\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\\nlarization. arXiv preprint arXiv:1711.05101 , 2017.\\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\\ntask-agnostic visiolinguistic representations for vision-\\nand-language tasks. In Advances in Neural Information\\nProcessing Systems , pp. 13–23, 2019.\\nLu, Z., Xiong, X., Li, Y ., Stroud, J., and Ross, D. Leveraging\\nweakly supervised data and pose representation for action\\nrecognition, 2020. URL https://www.youtube.\\ncom/watch?v=KOQFxbPPLOE&t=1390s .\\nLucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\\nquet, O. Are gans created equal? a large-scale study.\\nAdvances in neural information processing systems , 31:\\n700–709, 2018.\\nMahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri,\\nM., Li, Y ., Bharambe, A., and van der Maaten, L. Ex-\\nploring the limits of weakly supervised pretraining. InProceedings of the European Conference on Computer\\nVision (ECCV) , pp. 181–196, 2018.\\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\\nLearned in translation: Contextualized word vectors. In\\nAdvances in neural information processing systems , pp.\\n6294–6305, 2017.\\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\\nnatural language decathlon: Multitask learning as ques-\\ntion answering. arXiv preprint arXiv:1806.08730 , 2018.\\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\\nVenkatesh, G., et al. Mixed precision training. arXiv\\npreprint arXiv:1710.03740 , 2017.\\nMiech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\\nI., and Sivic, J. Howto100m: Learning a text-video em-\\nbedding by watching hundred million narrated video clips.\\nInProceedings of the IEEE international conference on\\ncomputer vision , pp. 2630–2640, 2019.\\nMiech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\\nman, A. Rareact: A video dataset of unusual interactions.\\narXiv preprint arXiv:2008.01018 , 2020a.\\nMiech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\\nand Zisserman, A. End-to-end learning of visual represen-\\ntations from uncurated instructional videos. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 9879–9889, 2020b.\\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\\nDean, J. Distributed representations of words and phrases\\nand their compositionality. Advances in neural informa-\\ntion processing systems , 26:3111–3119, 2013.\\nMiller, J., Krauth, K., Recht, B., and Schmidt, L. The effect\\nof natural distribution shift on question answering models.\\narXiv preprint arXiv:2004.14444 , 2020.\\nMishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\\ntion using higher order language priors. 2012.\\nMithun, N. C., Panda, R., Papalexakis, E. E., and Roy-\\nChowdhury, A. K. Webly supervised joint embedding for\\ncross-modal image-text retrieval. In Proceedings of the\\n26th ACM international conference on Multimedia , pp.\\n1856–1864, 2018.\\nMori, Y ., Takahashi, H., and Oka, R. Image-to-word trans-\\nformation based on dividing and vector quantizing images\\nwith words. Citeseer, 1999.\\nMu, J., Liang, P., and Goodman, N. Shaping visual represen-\\ntations with language for few-shot classiﬁcation. arXiv\\npreprint arXiv:1911.02683 , 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='54404b02-0a1d-4de9-8d5a-3ae91d9575b8', embedding=None, metadata={'page_label': '33', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2fd5dc5783328dfff9eb80cc0110a3c65e2e4bd5b72c1bc1503b3292d9188f0f', text='Learning Transferable Visual Models From Natural Language Supervision 33\\nMuller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\\ncation estimation of photos using a hierarchical model\\nand scene classiﬁcation. In Proceedings of the European\\nConference on Computer Vision (ECCV) , pp. 563–579,\\n2018.\\nMurty, S., Koh, P. W., and Liang, P. Expbert: Representation\\nengineering with natural language explanations. arXiv\\npreprint arXiv:2005.01932 , 2020.\\nNarasimhan, K., Kulkarni, T., and Barzilay, R. Language\\nunderstanding for text-based games using deep reinforce-\\nment learning. arXiv preprint arXiv:1506.08941 , 2015.\\nNetzer, Y ., Wang, T., Coates, A., Bissacco, A., Wu, B.,\\nand Ng, A. Y . Reading digits in natural images with\\nunsupervised feature learning. 2011.\\nNoble, S. U. Algorithms of oppression: How search engines\\nreinforce racism. 2018.\\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\\ning implicit group attitudes and beliefs from a demonstra-\\ntion web site. Group Dynamics: Theory, Research, and\\nPractice , 6(1):101, 2002.\\nOh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\\nJ. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\\nA large-scale benchmark dataset for event recognition in\\nsurveillance video. In CVPR 2011 , pp. 3153–3160. IEEE,\\n2011.\\nOliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\\nfellow, I. Realistic evaluation of deep semi-supervised\\nlearning algorithms. Advances in neural information pro-\\ncessing systems , 31:3235–3246, 2018.\\nOord, A. v. d., Li, Y ., and Vinyals, O. Representation learn-\\ning with contrastive predictive coding. arXiv preprint\\narXiv:1807.03748 , 2018.\\nOrdonez, V ., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs. Advances\\nin neural information processing systems , 24:1143–1151,\\n2011.\\npandas development team, T. pandas-dev/pandas: Pan-\\ndas, February 2020. URL https://doi.org/10.\\n5281/zenodo.3509134 .\\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\\nC. V . Cats and dogs. In IEEE Conference on Computer\\nVision and Pattern Recognition , 2012.\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,Bai, J., and Chintala, S. Pytorch: An imperative style,\\nhigh-performance deep learning library. In Advances\\nin Neural Information Processing Systems 32 , pp. 8024–\\n8035, 2019.\\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V .,\\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\\nWeiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cour-\\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\\nScikit-learn: Machine learning in Python. Journal of\\nMachine Learning Research , 12:2825–2830, 2011.\\nPennington, J., Socher, R., and Manning, C. D. Glove:\\nGlobal vectors for word representation. In Proceedings\\nof the 2014 conference on empirical methods in natural\\nlanguage processing (EMNLP) , pp. 1532–1543, 2014.\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\\nword representations. arXiv preprint arXiv:1802.05365 ,\\n2018.\\nQi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\\nA. Imagebert: Cross-modal pre-training with large-\\nscale weak-supervised image-text data. arXiv preprint\\narXiv:2001.07966 , 2020.\\nQuattoni, A., Collins, M., and Darrell, T. Learning visual\\nrepresentations using images with captions. In 2007 IEEE\\nConference on Computer Vision and Pattern Recognition ,\\npp. 1–8. IEEE, 2007.\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\\nI. Improving language understanding by generative pre-\\ntraining, 2018.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. 2019.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683 , 2019.\\nRaji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\\nJ., and Denton, E. Saving face: Investigating the ethical\\nconcerns of facial recognition auditing, 2020.\\nRamanathan, V ., Liang, P., and Fei-Fei, L. Video event\\nunderstanding using natural language descriptions. In\\nProceedings of the IEEE International Conference on\\nComputer Vision , pp. 905–912, 2013.\\nRashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J.\\nCollecting image annotations using amazon’s mechanical\\nturk. In Proceedings of the NAACL HLT 2010 Workshop\\non Creating Speech and Language Data with Amazon’s\\nMechanical Turk , pp. 139–147, 2010.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f2a7517f-b3d6-4463-a4e9-2e9367938e8f', embedding=None, metadata={'page_label': '34', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='64b233fac205eb8dca1201155edab397c9203358a8bb7d38759d262d4644f23c', text='Learning Transferable Visual Models From Natural Language Supervision 34\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do im-\\nagenet classiﬁers generalize to imagenet? arXiv preprint\\narXiv:1902.10811 , 2019.\\nSalimans, T. and Kingma, D. P. Weight normalization: A\\nsimple reparameterization to accelerate training of deep\\nneural networks. In Advances in neural information pro-\\ncessing systems , pp. 901–909, 2016.\\nScheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\\ncomputers see gender: An evaluation of gender classiﬁca-\\ntion in commercial facial analysis services. Proceedings\\nof the ACM on Human-Computer Interaction , 3(CSCW):\\n1–33, 2019.\\nSchwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\\nS., Schoonvelde, M., and Lockhart, J. W. Diagnosing\\ngender bias in image recognition systems. Socius , 6:\\n2378023120967171, 2020.\\nSennrich, R., Haddow, B., and Birch, A. Neural machine\\ntranslation of rare words with subword units. arXiv\\npreprint arXiv:1508.07909 , 2015.\\nShankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B.,\\nand Schmidt, L. Do image classiﬁers generalize across\\ntime? arXiv preprint arXiv:1906.02168 , 2019.\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Proceedings\\nof the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers) , pp. 2556–\\n2565, 2018.\\nSingh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X.,\\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\\nmodels that can read. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition , pp.\\n8317–8326, 2019.\\nSocher, R. and Fei-Fei, L. Connecting modalities: Semi-\\nsupervised segmentation and annotation of images using\\nunaligned text corpora. In 2010 IEEE Computer Society\\nConference on Computer Vision and Pattern Recognition ,\\npp. 966–973. IEEE, 2010.\\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\\nC. D., Ng, A. Y ., and Potts, C. Recursive deep models for\\nsemantic compositionality over a sentiment treebank. In\\nProceedings of the 2013 conference on empirical methods\\nin natural language processing , pp. 1631–1642, 2013.\\nSocher, R., Karpathy, A., Le, Q. V ., Manning, C. D., and Ng,\\nA. Y . Grounded compositional semantics for ﬁnding and\\ndescribing images with sentences. Transactions of the\\nAssociation for Computational Linguistics , 2:207–218,\\n2014.Sohn, K. Improved deep metric learning with multi-class\\nn-pair loss objective. In Advances in neural information\\nprocessing systems , pp. 1857–1865, 2016.\\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\\nV oss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\\nKreps, S., McCain, M., Newhouse, A., Blazakis, J.,\\nMcGufﬁe, K., and Wang, J. Release strategies and the\\nsocial impacts of language models, 2019.\\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\\nof 101 human actions classes from videos in the wild.\\narXiv preprint arXiv:1212.0402 , 2012.\\nSpeer, R. ftfy. Zenodo, 2019. URL https://doi.org/\\n10.5281/zenodo.2591652 . Version 5.5.\\nSrivastava, N. and Salakhutdinov, R. Multimodal learning\\nwith deep boltzmann machines. In NIPS , 2012.\\nSrivastava, S., Labutov, I., and Mitchell, T. Joint concept\\nlearning and semantic parsing from natural language ex-\\nplanations. In Proceedings of the 2017 conference on\\nempirical methods in natural language processing , pp.\\n1527–1536, 2017.\\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\\nGerman Trafﬁc Sign Recognition Benchmark: A multi-\\nclass classiﬁcation competition. In IEEE International\\nJoint Conference on Neural Networks , pp. 1453–1460,\\n2011.\\nStroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\\nand Schmid, C. Learning video representations from tex-\\ntual web supervision. arXiv preprint arXiv:2007.14937 ,\\n2020.\\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi,\\nA. Inception-v4, inception-resnet and the impact\\nof residual connections on learning. arXiv preprint\\narXiv:1602.07261 , 2016.\\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\\nencoder representations from transformers. arXiv preprint\\narXiv:1908.07490 , 2019.\\nTan, M. and Le, Q. V . Efﬁcientnet: Rethinking model\\nscaling for convolutional neural networks. arXiv preprint\\narXiv:1905.11946 , 2019.\\nTaori, R., Dave, A., Shankar, V ., Carlini, N., Recht, B.,\\nand Schmidt, L. Measuring robustness to natural dis-\\ntribution shifts in image classiﬁcation. arXiv preprint\\narXiv:2007.00644 , 2020.\\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\\nnew data in multimedia research. Communications of the\\nACM , 59(2):64–73, 2016.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d0653ac5-baa2-4363-89a9-f035fefb62e2', embedding=None, metadata={'page_label': '35', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='c8412e66e0370586e98ef4d9f80d3c3121de2a139ee0f9a421d2f45a8343a419', text='Learning Transferable Visual Models From Natural Language Supervision 35\\nTian, Y ., Krishnan, D., and Isola, P. Contrastive multiview\\ncoding. arXiv preprint arXiv:1906.05849 , 2019.\\nTian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and\\nIsola, P. Rethinking few-shot image classiﬁcation: a\\ngood embedding is all you need? arXiv preprint\\narXiv:2003.11539 , 2020.\\nTorralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\\nimages: A large data set for nonparametric object and\\nscene recognition. IEEE transactions on pattern analysis\\nand machine intelligence , 30(11):1958–1970, 2008.\\nTouvron, H., Vedaldi, A., Douze, M., and J ´egou, H. Fix-\\ning the train-test resolution discrepancy. In Advances in\\nneural information processing systems , pp. 8252–8262,\\n2019.\\nVaradarajan, J. and Odobez, J.-M. Topic models for scene\\nanalysis and abnormality detection. In 2009 IEEE 12th\\nInternational Conference on Computer Vision Workshops,\\nICCV Workshops , pp. 1338–1345. IEEE, 2009.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\\ntion is all you need. In Advances in neural information\\nprocessing systems , pp. 5998–6008, 2017.\\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\\nWelling, M. Rotation equivariant CNNs for digital pathol-\\nogy. June 2018.\\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,\\nFeng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,\\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\\n1.0: Fundamental Algorithms for Scientiﬁc Computing\\nin Python. Nature Methods , 17:261–272, 2020. doi:\\n10.1038/s41592-019-0686-2.\\nV o, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\\ndeep learning era. In Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision , pp. 2621–2630,\\n2017.\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\\nBowman, S. R. Glue: A multi-task benchmark and anal-\\nysis platform for natural language understanding. arXiv\\npreprint arXiv:1804.07461 , 2018.\\nWang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro-\\nbust global representations by penalizing local predictive\\npower. In Advances in Neural Information Processing\\nSystems , pp. 10506–10518, 2019.Wang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y ., He,\\nM., Wang, Y ., and Liu, W. All you need is boundary: To-\\nward arbitrary-shaped text spotting. In Proceedings of the\\nAAAI Conference on Artiﬁcial Intelligence , volume 34,\\npp. 12160–12167, 2020.\\nWang, J., Markert, K., and Everingham, M. Learning mod-\\nels for object recognition from natural language descrip-\\ntions. In BMVC , volume 1, pp. 2, 2009.\\nWeston, J., Bengio, S., and Usunier, N. Large scale im-\\nage annotation: learning to rank with joint word-image\\nembeddings. Machine learning , 81(1):21–35, 2010.\\nWeston, J. E. Dialog-based language learning. In Advances\\nin Neural Information Processing Systems , pp. 829–837,\\n2016.\\nWeyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\\ncation with convolutional neural networks. In European\\nConference on Computer Vision , pp. 37–55. Springer,\\n2016.\\nWu, Y ., Kirillov, A., Massa, F., Lo, W.-Y ., and Gir-\\nshick, R. Detectron2. https://github.com/\\nfacebookresearch/detectron2 , 2019.\\nWu, Z., Xiong, Y ., Yu, S., and Lin, D. Unsupervised feature\\nlearning via non-parametric instance-level discrimination.\\narXiv preprint arXiv:1805.01978 , 2018.\\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V . Self-training\\nwith noisy student improves imagenet classiﬁcation. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pp. 10687–10698, 2020.\\ny Arcas, B. A., Mitchell, M., and Todorov,\\nA. Physiognomy’s new clothes. 2017.\\nURL https://medium.com/@blaisea/\\nphysiognomys-new-clothes-f2d4b59fdd6a .\\nYang, Z., Lu, Y ., Wang, J., Yin, X., Florencio, D., Wang,\\nL., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\\npre-training for text-vqa and text-caption. arXiv preprint\\narXiv:2012.04638 , 2020.\\nYogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\\nYu, L., Dyer, C., et al. Learning and evaluating general\\nlinguistic intelligence. arXiv preprint arXiv:1901.11373 ,\\n2019.\\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\\nimage descriptions to visual denotations: New similarity\\nmetrics for semantic inference over event descriptions.\\nTransactions of the Association for Computational Lin-\\nguistics , 2:67–78, 2014.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='75d0c69c-a406-4890-8fbd-c27827162e0a', embedding=None, metadata={'page_label': '36', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5d300416b0be2e6e3f3e4de1fba02a9838add38d7a0d6a8a7ff1007546f84b3d', text='Learning Transferable Visual Models From Natural Language Supervision 36\\nYu, F., Tang, J., Yin, W., Sun, Y ., Tian, H., Wu, H.,\\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\\nlanguage representations through scene graph. arXiv\\npreprint arXiv:2006.16934 , 2020.\\nZeiler, M. D. and Fergus, R. Visualizing and understand-\\ning convolutional networks. In European conference on\\ncomputer vision , pp. 818–833. Springer, 2014.\\nZhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\\nmann, M., Dosovitskiy, A., et al. A large-scale study of\\nrepresentation learning with the visual task adaptation\\nbenchmark. arXiv preprint arXiv:1910.04867 , 2019.\\nZhang, R. Making convolutional networks shift-invariant\\nagain. arXiv preprint arXiv:1904.11486 , 2019.\\nZhang, Y ., Jiang, H., Miura, Y ., Manning, C. D., and Lan-\\nglotz, C. P. Contrastive learning of medical visual repre-\\nsentations from paired images and text. arXiv preprint\\narXiv:2010.00747 , 2020.\\nZuboff, S. Big other: surveillance capitalism and the\\nprospects of an information civilization. Journal of Infor-\\nmation Technology , 30(1):75–89, 2015.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7950a042-85a3-45f3-b89c-3b8da6b5965c', embedding=None, metadata={'page_label': '37', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8fa343e1f07e0ac9bd9fd6197a7348151585cb6b372845d660107093ceaa5a23', text='Learning Transferable Visual Models From Natural Language Supervision 37\\nA. Linear-probe evaluation\\nWe provide additional details for linear probe experiments\\npresented in this paper, including the list of the datasets and\\nmodels used for evaluation.\\nA.1. Datasets\\nWe use the 12 datasets from the well-studied evaluation\\nsuite introduced by (Kornblith et al., 2019) and add 15\\nadditional datasets in order to assess the performance of\\nmodels on a wider variety of distributions and tasks. These\\ndatasets include MNIST, the Facial Expression Recognition\\n2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\\net al., 2011), EuroSAT (Helber et al., 2019), the NWPU-\\nRESISC45 dataset (Cheng et al., 2017), the German Traf-\\nﬁc Sign Recognition Benchmark (GTSRB) dataset (Stal-\\nlkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\\nPatchCamelyon (Veeling et al., 2018), the UCF101 action\\nrecognition dataset (Soomro et al., 2012), Kinetics 700 (Car-\\nreira et al., 2019), 2,500 random samples of the CLEVR\\ndataset (Johnson et al., 2017), the Hateful Memes dataset\\n(Kiela et al., 2020), and the ImageNet-1k dataset (Deng\\net al., 2012). For the two video datasets (UCF101 and Ki-\\nnetics700), we use the middle frame of each video clip as\\nthe input image. STL-10 and UCF101 have multiple pre-\\ndeﬁned train/validation/test splits, 10 and 3 respectively, and\\nwe report the average over all splits. Details on each dataset\\nand the corresponding evaluation metrics are provided in\\nTable 9.\\nAdditionally, we created two datasets that we call Coun-\\ntry211 and Rendered SST2. The Country211 dataset is\\ndesigned to assess the geolocation capability of visual rep-\\nresentations. We ﬁltered the YFCC100m dataset (Thomee\\net al., 2016) to ﬁnd 211 countries (deﬁned as having an\\nISO-3166 country code) that have at least 300 photos with\\nGPS coordinates, and we built a balanced dataset with 211\\ncategories, by sampling 200 photos for training and 100\\nphotos for testing, for each country.\\nThe Rendered SST2 dataset is designed to measure the opti-\\ncal character recognition capability of visual representations.\\nTo do so, we used the sentences from the Stanford Sentiment\\nTreebank dataset (Socher et al., 2013) and rendered them\\ninto images, with black texts on a white background, in a\\n448×448 resolution. Two example images from this dataset\\nare shown in Figure 19.\\nA.2. Models\\nIn combination with the datasets listed above, we evaluate\\nthe following series of models using linear probes.\\nLM RN50 This is a multimodal model that uses an au-\\ntoregressive loss instead of a contrastive loss, while usingthe ResNet-50 architecture as in the smallest contrastive\\nmodel. To do so, the output from the CNN is projected into\\nfour tokens, which are then fed as a preﬁx to a language\\nmodel autoregressively predicting the text tokens. Apart\\nfrom the training objective, the model was trained on the\\nsame dataset for the same number of epochs as other CLIP\\nmodels.\\nCLIP-RN Five ResNet-based contrastive CLIP models\\nare included. As discussed in the paper, the ﬁrst two models\\nfollow ResNet-50 and ResNet-101, and we use EfﬁcientNet-\\nstyle (Tan & Le, 2019) scaling for the next three models\\nwhich simultaneously scale the model width, the number\\nof layers, and the input resolution to obtain models with\\nroughly 4x, 16x, and 64x computation.\\nCLIP-ViT We include four CLIP models that use the Vi-\\nsion Transformer (Dosovitskiy et al., 2020) architecture as\\nthe image encoder. We include three models trained on 224-\\nby-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and\\nthe ViT-L/14 model ﬁne-tuned on 336-by-336 pixel input\\nimages.\\nEfﬁcietNet We use the nine models (B0-B8) from the\\noriginal EfﬁcientNet paper (Tan & Le, 2019), as well as\\nthe noisy-student variants (B0-B7, L2-475, and L2-800)\\n(Tan & Le, 2019). The largest models (L2-475 and L2-800)\\ntake the input resolutions of 475x475 and 800x800 pixels,\\nrespectively.\\nInstagram-pretrained ResNeXt We use the four models\\n(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan\\net al., 2018), as well as their two FixRes variants which use\\nhigher input resolutions (Touvron et al., 2019).\\nBig Transfer (BiT) We use BiT-S and BiT-M models\\n(Kolesnikov et al., 2019), trained on the ImageNet-1k and\\nImageNet-21k datasets. The model weights for BiT-L is not\\npublicly available.\\nVision Transformer (ViT) We also include four ViT\\n(Dosovitskiy et al., 2020) checkpoints pretrained on the\\nImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-\\nL/16, and ViT-H/14. We note that their best-performing\\nmodels, trained on the JFT-300M dataset, are not available\\npublicly.\\nSimCLRv2 The SimCLRv2 (Chen et al., 2020c) project\\nreleased pre-trained and ﬁne-tuned models in various set-\\ntings. We use the seven pretrain-only checkpoints with\\nselective kernels.\\nBYOL We use the recently released model weights of\\nBYOL (Grill et al., 2020), speciﬁcally their 50x1 and 200x2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='56055d47-6cda-4632-a261-76f604015898', embedding=None, metadata={'page_label': '38', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='18774c0ba722850a9f36d95ebdb1a14daf63728beffbbe4c55fb6cd1d29e5c51', text='Learning Transferable Visual Models From Natural Language Supervision 38\\nFigure 19. Two example images from the Rendered SST2 dataset\\ncheckpoints.\\nMomentum Contrast (MoCo) We include the MoCo-v1\\n(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)\\ncheckpoints.\\nVirTex We use the pretrained model of VirTex (Desai &\\nJohnson, 2020). We note that VirTex has a similar model\\ndesign to CLIP-AR but is trained on a 1000x smaller dataset\\nof high-quality captions from MSCOCO.\\nResNet We add the original ResNet checkpoints released\\nby (He et al., 2016b), namely ResNet-50, ResNet-101, and\\nResNet152.\\nA.3. Evaluation\\nWe use image features taken from the penultimate layer of\\neach model, ignoring any classiﬁcation layer provided. For\\nCLIP-ViT models, we used the features before the linear\\nprojection to the embedding space, which corresponds to\\nIfin Figure 3. We train a logistic regression classiﬁer\\nusing scikit-learn’s L-BFGS implementation, with maxi-\\nmum 1,000 iterations, and report the corresponding met-\\nric for each dataset. We determine the L2 regularization\\nstrengthλusing a hyperparameter sweep on the validation\\nsets over the range between 10−6and106, with 96 log-\\narithmically spaced steps. To save compute required for\\nthe sweeps, we perform a parametric binary search that\\nstarts withλ= [10−6,10−4,10−2,1,102,104,106]and it-\\neratively halves the interval around the peak until it reaches\\na resolution of 8 steps per decade. The hyperparameter\\nsweeps are performed on a validation split of each dataset.\\nFor the datasets that contain a validation split in addition toa test split, we use the provided validation set to perform\\nthe hyperparameter search, and for the datasets that do not\\nprovide a validation split or have not published labels for\\nthe test data, we split the training dataset to perform the\\nhyperparameter search. For the ﬁnal result, we combine the\\nvalidation split back with the training split and report the\\nperformance on the unused split.\\nA.4. Results\\nThe individual linear probe scores are provided in Table 10\\nand plotted in Figure 20. The best-performing CLIP model,\\nusing ViT-L/14 archiecture and 336-by-336 pixel images,\\nachieved the state of the art in 21 of the 27 datasets, i.e.\\nincluded in the Clopper-Pearson 99.5% conﬁdence interval\\naround each dataset’s top score. For many datasets, CLIP\\nperforms signiﬁcantly better than other models, demonstrat-\\ning the advantage of natural language supervision over tradi-\\ntional pre-training approaches based on image classiﬁcation.\\nSee Section 3.2 for more discussions on the linear probe\\nresults.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f0244b67-c68e-4a0c-b209-d33b28ee6387', embedding=None, metadata={'page_label': '39', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7dc961b03c9833fae7b9a1ed618d47fa9be7df6c34eeab9765a09c9da308502d', text='Learning Transferable Visual Models From Natural Language Supervision 39\\nDataset Classes Train size Test size Evaluation metric\\nFood-101 102 75,750 25,250 accuracy\\nCIFAR-10 10 50,000 10,000 accuracy\\nCIFAR-100 100 50,000 10,000 accuracy\\nBirdsnap 500 42,283 2,149 accuracy\\nSUN397 397 19,850 19,850 accuracy\\nStanford Cars 196 8,144 8,041 accuracy\\nFGVC Aircraft 100 6,667 3,333 mean per class\\nPascal VOC 2007 Classiﬁcation 20 5,011 4,952 11-point mAP\\nDescribable Textures 47 3,760 1,880 accuracy\\nOxford-IIIT Pets 37 3,680 3,669 mean per class\\nCaltech-101 102 3,060 6,085 mean-per-class\\nOxford Flowers 102 102 2,040 6,149 mean per class\\nMNIST 10 60,000 10,000 accuracy\\nFacial Emotion Recognition 2013 8 32,140 3,574 accuracy\\nSTL-10 10 1000 8000 accuracy\\nEuroSAT 10 10,000 5,000 accuracy\\nRESISC45 45 3,150 25,200 accuracy\\nGTSRB 43 26,640 12,630 accuracy\\nKITTI 4 6,770 711 accuracy\\nCountry211 211 43,200 21,100 accuracy\\nPatchCamelyon 2 294,912 32,768 accuracy\\nUCF101 101 9,537 1,794 accuracy\\nKinetics700 700 494,801 31,669 mean(top1, top5)\\nCLEVR Counts 8 2,000 500 accuracy\\nHateful Memes 2 8,500 500 ROC AUC\\nRendered SST2 2 7,792 1,821 accuracy\\nImageNet 1000 1,281,167 50,000 accuracy\\nTable 9. Datasets examined for linear probes. We note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are\\navailable online at the time of this writing.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8c6b2052-cd06-4106-a48a-36d2c866d1b2', embedding=None, metadata={'page_label': '40', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='963b5bf9de1617e01302024fc65091ef72e4ac1106cb03a92a6ffd67f2a1c1df', text='Learning Transferable Visual Models From Natural Language Supervision 40Food101\\nCIFAR10\\nCIFAR100\\nBirdsnap\\nSUN397\\nCars\\nAircraft\\nVOC2007\\nDTD\\nPets\\nCaltech101\\nFlowers\\nMNIST\\nFER2013\\nSTL10⋆\\nEuroSAT\\nRESISC45\\nGTSRB\\nKITTI\\nCountry211\\nPCAM\\nUCF101\\nKinetics700\\nCLEVR\\nHatefulMemes\\nSST\\nImageNet\\nLM RN50 81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2CLIP-RN50 86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3\\n101 88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7\\n50x4 91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2\\n50x16 93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0 66.3 53.8 71.1 80.0 81.5\\n50x64 94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6CLIP-ViTB/32 88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1\\nB/16 92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2\\nL/14 95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9\\nL/14-336px 95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4EfﬁcientNetB0 74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9\\nB1 74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6\\nB2 75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7\\nB3 77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0\\nB4 79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9\\nB5 81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7\\nB6 82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0\\nB7 84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8\\nB8 84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3EfﬁcientNet Noisy StudentB0 78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5\\nB1 80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1\\nB2 80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2\\nB3 82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8\\nB4 85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4\\nB5 87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8\\nB6 87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4\\nB7 88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0\\nL2-475 91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3\\nL2-800 92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4Instagram32x8d 84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3\\n32x16d 85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4\\n32x32d 86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0\\n32x48d 86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2\\nFixRes-v1 88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0\\nFixRes-v2 88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0BiT-SR50x1 72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2\\nR50x3 75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2\\nR101x1 73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0\\nR101x3 74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4\\nR152x2 74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1\\nR152x4 74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5BiT-MR50x1 83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7\\nR50x3 86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7\\nR101x1 85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4\\nR101x3 87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2\\nR152x2 88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9\\nR152x4 87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5ViTB/32 81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6\\nB/16 86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9\\nL/16 87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9\\nH/14 83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4 52.1 41.1 55.9 54.1 75.9SimCLRv2R50x1 76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8\\nR50x3 81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3\\nR101x1 77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1\\nR101x3 82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8\\nR152x1 78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9\\nR152x2 82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2\\nR152x3 83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0BYOL50x1 74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3\\n200x2 78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2MoCov1 65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2\\nv2 72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1\\nVirTex 57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7ResNet50 71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3\\n101 72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8\\n152 73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1\\nTable 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson conﬁdence\\ninterval of each dataset’s top score are shown in bold.\\n⋆We updated the STL10 scores from the previous version of this paper after ﬁxing a CUDA-related bug.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='67a9f712-cafc-470f-96ad-3d7c6fc92ff1', embedding=None, metadata={'page_label': '41', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3e3e2401e14ec634068ab0414f78f478bea74cbd8d74f7fb28dc6bff091e0507', text='Learning Transferable Visual Models From Natural Language Supervision 41\\n1001011027580859095accuracy\\nFood101\\n1001011029092949698accuracy\\nCIFAR10\\n1001011027075808590accuracy\\nCIFAR100\\n100101102404550556065707580accuracy\\nBirdsnap\\n1001011026065707580accuracy\\nSUN397\\n1001011025060708090accuracy\\nStanfordCars\\n100101102455055606570mean per class\\nFGVCAircraft\\n100101102838485868788899011-point mAP over 20 classes\\nPascalVOC2007\\n100101102727476788082accuracy\\nDescribableTextures\\n10010110284868890929496mean per class\\nOxfordPets\\n10010110290919293949596mean-per-class\\nCaltech101\\n1001011029092949698100mean per class\\nFlowers102\\n10010110297.2597.5097.7598.0098.2598.5098.7599.00accuracy\\nMNIST\\n10010110255.057.560.062.565.067.570.072.5accuracy\\nFacialEmotionRecognition2013\\n10010110296.096.597.097.598.098.599.099.5accuracy\\nSTL10\\n10010110295.596.096.597.097.598.0accuracy\\nEuroSAT\\n10010110282848688909294accuracy\\nRESISC45\\n100101102657075808590accuracy\\nGTSRB\\n10010110257.560.062.565.067.570.072.575.0accuracy\\nKITTI\\n10010110281828384858687accuracy\\nPatchCamelyon\\n10010110275808590accuracy\\nUCF101\\n1001011025055606570mean(top1, top5)\\nKinetics700\\n1001011024045505560accuracy\\nCLEVRCounts\\n100101102\\nGFLOPs/image1015202530354045accuracy\\nCountry211\\n100101102\\nGFLOPs/image5560657075ROCAUC\\nHatefulMemes\\n100101102\\nGFLOPs/image556065707580accuracy\\nSST2\\n100101102\\nGFLOPs/image70.072.575.077.580.082.585.087.5accuracy\\nImageNet\\nCLIP-ViT\\nCLIP-ResNet\\nEfficientNet-NoisyStudent\\nEfficientNet\\nInstagram-pretrained\\nSimCLRv2\\nBYOL\\nMoCo\\nViT (ImageNet-21k)\\nBiT-M\\nBiT-S\\nResNet\\nFigure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='487fa3ae-2bef-4202-b22b-c662040fce7b', embedding=None, metadata={'page_label': '42', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='38c7b243d7eda8d3311bcdce77f8e221527d0ae0c923a5aee26be7e066dc3656', text='Learning Transferable Visual Models From Natural Language Supervision 42\\ncorrect label: red and white triangle with exclamation mark warning\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign.\\na zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign.\\na zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign.\\na zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign.\\na zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign.correct rank: 1/43    correct probability: 45.75%German Traffic Sign Recognition Benchmark (GTSRB)\\ncorrect label: positive\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na positive review of a movie.\\na negative review of a movie.correct rank: 1/2    correct probability: 78.21%Stanford Sentiment Treebank\\ncorrect label: meme\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na meme.\\na hatespeech meme.correct rank: 1/2    correct probability: 99.20%Hateful Memes\\ncorrect label: barn\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a barn.\\na photo of a church.\\na photo of a threshing machine.\\na photo of a sawmill.\\na photo of a prison.correct rank: 1/1000    correct probability: 79.56%ImageNet Sketch\\ncorrect label(s): antelope\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a antelope.\\na photo of a zebra.\\na photo of a car.\\na photo of a cattle.\\na photo of a elephant.correct rank: 1/30    correct probability: 99.77%ImageNet Vid\\ncorrect label: 158\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na street sign of the number: \"1157\".\\na street sign of the number: \"1165\".\\na street sign of the number: \"1164\".\\na street sign of the number: \"1155\".\\na street sign of the number: \"1364\".correct rank: 83/2000    correct probability: 0.27%Street View House Numbers (SVHN)\\ncorrect label: 7\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of the number: \"7\".\\na photo of the number: \"2\".\\na photo of the number: \"1\".\\na photo of the number: \"6\".\\na photo of the number: \"4\".correct rank: 1/10    correct probability: 85.32%MNIST\\ncorrect label(s): motorcycle\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a motorcycle.\\na photo of a bicycle.\\na photo of a car.\\na photo of a horse.\\na photo of a dining table.correct rank: 1/20    correct probability: 99.69%PASCAL VOC 2007\\ncorrect label: perforated\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a polka-dotted texture.\\na photo of a perforated texture.\\na photo of a dotted texture.\\na photo of a studded texture.\\na photo of a freckled texture.correct rank: 2/47    correct probability: 20.50%Describable Textures Dataset (DTD)\\ncorrect label: marimba\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a marimba.\\na photo of a abacus.\\na photo of a steel drum.\\na photo of a computer keyboard.\\na photo of a pool table.correct rank: 1/1000    correct probability: 79.54%ImageNet Blurry\\ncorrect label: Pill bottle\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a pill bottle.\\na photo of a bottle cap.\\na photo of a beer bottle.\\na photo of a pillow.\\na photo of a wine bottle.correct rank: 1/113    correct probability: 98.34%ObjectNet ImageNet Overlap\\ncorrect label: building\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a building.\\na photo of a carriage.\\na photo of a statue.\\na photo of a bag.\\na photo of a mug.correct rank: 1/12    correct probability: 97.69%aYahoo\\ncorrect label: Black chinned Hummingbird\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a broad tailed hummingbird, a type of bird.\\na photo of a calliope hummingbird, a type of bird.\\na photo of a costas hummingbird, a type of bird.\\na photo of a black chinned hummingbird, a type of bird.\\na photo of a annas hummingbird, a type of bird.correct rank: 4/500    correct probability: 12.00%Birdsnap\\ncorrect label: King Charles Spaniel\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a king charles spaniel.\\na photo of a brittany dog.\\na photo of a cocker spaniel.\\na photo of a papillon.\\na photo of a sussex spaniel.correct rank: 1/1000    correct probability: 91.61%ImageNet\\ncorrect label: great masterwort\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a great masterwort, a type of flower.\\na photo of a bishop of llandaff, a type of flower.\\na photo of a pincushion flower, a type of flower.\\na photo of a globe flower, a type of flower.\\na photo of a prince of wales feathers, a type of flower.correct rank: 1/102    correct probability: 74.25%Flowers-102\\ncorrect label: country line dancing\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of country line dancing.\\na photo of square dancing.\\na photo of swing dancing.\\na photo of dancing charleston.\\na photo of salsa dancing.correct rank: 1/700    correct probability: 98.98%Kinetics-700\\ncorrect label: kennel indoor\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a kennel indoor.\\na photo of a kennel outdoor.\\na photo of a jail cell.\\na photo of a jail indoor.\\na photo of a veterinarians office.correct rank: 1/723    correct probability: 98.63%SUN\\ncorrect label: 2012 Honda Accord Coupe\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a 2012 honda accord coupe.\\na photo of a 2012 honda accord sedan.\\na photo of a 2012 acura tl sedan.\\na photo of a 2012 acura tsx sedan.\\na photo of a 2008 acura tl type-s.correct rank: 1/196    correct probability: 63.30%Stanford Cars\\ncorrect label: roundabout\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\nsatellite imagery of roundabout.\\nsatellite imagery of intersection.\\nsatellite imagery of church.\\nsatellite imagery of medium residential.\\nsatellite imagery of chaparral.correct rank: 1/45    correct probability: 96.39%RESISC45\\ncorrect label: Belize\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo i took in french guiana.\\na photo i took in gabon.\\na photo i took in cambodia.\\na photo i took in guyana.\\na photo i took in belize.correct rank: 5/211    correct probability: 3.92%Country211\\ncorrect label: Boeing 717\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a mcdonnell douglas md-90, a type of aircraft.\\na photo of a boeing 717, a type of aircraft.\\na photo of a fokker 100, a type of aircraft.\\na photo of a mcdonnell douglas dc-9-30, a type of aircraft.\\na photo of a boeing 727-200, a type of aircraft.correct rank: 2/100    correct probability: 9.91%FGVC Aircraft\\ncorrect label: beer bottle\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a beer bottle.\\na photo of a pirate ship.\\na photo of a chocolate syrup.\\na photo of a product packet / packaging.\\na photo of a wine bottle.correct rank: 1/1000    correct probability: 88.27%ImageNetV2 Matched Frequency\\ncorrect label: snake\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a snake.\\na photo of a sweet pepper.\\na photo of a flatfish.\\na photo of a turtle.\\na photo of a lizard.correct rank: 1/100    correct probability: 38.02%CIFAR-100\\ncorrect label: Maine Coon\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a maine coon, a type of pet.\\na photo of a persian, a type of pet.\\na photo of a ragdoll, a type of pet.\\na photo of a birman, a type of pet.\\na photo of a siamese, a type of pet.correct rank: 1/37    correct probability: 99.99%Oxford-IIIT Pets\\ncorrect label: Siberian Husky\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a siberian husky.\\na photo of a german shepherd dog.\\na photo of a collie.\\na photo of a border collie.\\na photo of a rottweiler.correct rank: 1/200    correct probability: 76.02%ImageNet-R (Rendition)\\ncorrect label: kangaroo\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a kangaroo.\\na photo of a gerenuk.\\na photo of a emu.\\na photo of a wild cat.\\na photo of a scorpion.correct rank: 1/102    correct probability: 99.81%Caltech-101\\ncorrect label: Volleyball Spiking\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a person volleyball spiking.\\na photo of a person jump rope.\\na photo of a person long jump.\\na photo of a person soccer penalty.\\na photo of a person table tennis shot.correct rank: 1/101    correct probability: 99.30%UCF101\\ncorrect label: angry\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a happy looking face.\\na photo of a neutral looking face.\\na photo of a surprised looking face.\\na photo of a fearful looking face.\\na photo of a angry looking face.correct rank: 5/7    correct probability: 8.16%Facial Emotion Recognition 2013 (FER2013)\\ncorrect label: 4\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of 3 objects.\\na photo of 4 objects.\\na photo of 5 objects.\\na photo of 6 objects.\\na photo of 10 objects.correct rank: 2/8    correct probability: 17.11%CLEVR Count\\ncorrect label: bird\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a bird.\\na photo of a cat.\\na photo of a deer.\\na photo of a frog.\\na photo of a dog.correct rank: 1/10    correct probability: 40.86%CIFAR-10\\ncorrect label: lynx\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a fox squirrel.\\na photo of a mongoose.\\na photo of a skunk.\\na photo of a red fox.\\na photo of a lynx.correct rank: 5/200    correct probability: 4.18%ImageNet-A (Adversarial)\\ncorrect label: healthy lymph node tissue\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\nthis is a photo of lymph node tumor tissue\\nthis is a photo of healthy lymph node tissuecorrect rank: 2/2    correct probability: 22.81%PatchCamelyon (PCam)\\ncorrect label: annual crop land\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na centered satellite photo of permanent crop land.\\na centered satellite photo of pasture land.\\na centered satellite photo of highway or road.\\na centered satellite photo of annual crop land.\\na centered satellite photo of brushland or shrubland.correct rank: 4/10    correct probability: 12.90%EuroSAT\\ncorrect label(s): airplane,person\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a airplane.\\na photo of a bird.\\na photo of a bear.\\na photo of a giraffe.\\na photo of a car.correct rank: 1/23    correct probability: 88.98%Youtube-BB\\ncorrect label: television studio\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of a television studio.\\na photo of a podium indoor.\\na photo of a conference room.\\na photo of a lecture room.\\na photo of a control room.correct rank: 1/397    correct probability: 90.22%SUN397\\ncorrect label: guacamole\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\na photo of guacamole, a type of food.\\na photo of ceviche, a type of food.\\na photo of edamame, a type of food.\\na photo of tuna tartare, a type of food.\\na photo of hummus, a type of food.correct rank: 1/101    correct probability: 90.15%Food101\\nFigure 21. Visualization of predictions from 36 CLIP zero-shot classiﬁers. All examples are random with the exception of reselecting\\nHateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent\\nthe class. When more than one template is used, the ﬁrst template is shown. The ground truth label is colored green while an incorrect\\nprediction is colored orange.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9dc5f7dc-2468-4fc5-8daf-0ad32592fcbd', embedding=None, metadata={'page_label': '43', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='888045998c0d763c63060ebf2ca64f23d314e6cae9ed1a404dda9758986bc79b', text='Learning Transferable Visual Models From Natural Language Supervision 43Food101\\nCIFAR10\\nCIFAR100\\nBirdsnap\\nSUN397\\nStanford Cars\\nFGVC Aircraft\\nVOC2007\\nDTD\\nOxford Pets\\nCaltech101\\nFlowers102\\nMNIST\\nFER2013\\nSTL10\\nEuroSAT\\nRESISC45\\nGTSRB\\nKITTI\\nCountry211\\nPCam\\nUCF101\\nKinetics700\\nCLEVR\\nHatefulMemes\\nRendered SST2\\nImageNetCLIP-ResNetRN50 81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6\\nRN101 83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2\\nRN50x4 86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8\\nRN50x16 90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5\\nRN50x64 91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6CLIP-ViTB/32 84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2\\nB/16 89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4 56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6\\nL/14 92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3\\nL/14-336px 93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2\\nTable 11. Zero-shot performance of CLIP models over 27 datasets.\\n10110275808590accuracy\\nFood101\\n1011027580859095accuracy\\nCIFAR10\\n10110240506070accuracy\\nCIFAR100\\n1011023540455055accuracy\\nBirdsnap\\n1011026062646668accuracy\\nSUN397\\n10110250607080accuracy\\nStanfordCars\\n101102203040mean per class\\nFGVCAircraft\\n10110282.082.583.083.584.084.511-point mAP over 20 classes\\nPascalVOC2007\\n101102506070accuracy\\nDescribableTextures\\n10110286889092mean per class\\nOxfordPets\\n101102828486889092mean-per-class\\nCaltech101\\n101102657075808590mean per class\\nFlowers102\\n1011025060708090100accuracy\\nMNIST\\n10110245505560accuracy\\nFacialEmotionRecognition2013\\n1011029596979899accuracy\\nSTL10\\n101102406080100accuracy\\nEuroSAT\\n101102607080accuracy\\nRESISC45\\n10110240506070accuracy\\nGTSRB\\n101102203040506070accuracy\\nKITTI\\n10110250607080accuracy\\nPatchCamelyon\\n101102657075accuracy\\nUCF101\\n10110245505560mean(top1, top5)\\nKinetics700\\n1011022025303540accuracy\\nCLEVRCounts\\n1011021520253035accuracy\\nCountry211\\n101102\\nGFLOPs/image5456586062ROCAUC\\nHatefulMemes\\n101102\\nGFLOPs/image55606570accuracy\\nSST2\\n101102\\nGFLOPs/image60657075accuracy\\nImageNet\\nCLIP-ViT\\nCLIP-ResNet\\nResNet\\nFigure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='23325b2a-f543-44e4-9fdd-c27a137c144a', embedding=None, metadata={'page_label': '44', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='20a15013c6e3cb4a999fbf86f70b5c845b4a908565fb3f8cd93c76ba71239c92', text='Learning Transferable Visual Models From Natural Language Supervision 44\\nB. Zero-Shot Prediction\\nTo provide a qualitative summary / overview of CLIP’s zero-\\nshot performance we visualize a randomly selected predic-\\ntion for 36 different zero-shot CLIP classiﬁers in Figure\\n21. In addition, Table 11 and Figure 22 show the individual\\nzero-shot performance scores for each dataset.\\nC. Duplicate Detector\\nOur early attempts at duplicate detection and analysis used\\nnearest neighbors in the model’s learned embedding space.\\nWhile it is intuitive to use a model’s own notion of similar-\\nity, we encountered issues. We found the model’s feature\\nspace is weighted very heavily towards semantic similar-\\nity. Many false positives occurred due to distinct objects\\nthat would be described similarly (soccer balls, ﬂowers of\\nthe same species, etc...) having almost perfect similarity.\\nWe also observed the model was quite poor at assigning\\ncertain kinds of near-duplicates high similarity scores. We\\nnoticed repeatedly that images with high-frequency textures\\n(such as fur or stripe patterns) pre-processed by different\\nresizing algorithms (nearest neighbor vs bi-linear) could\\nhave surprisingly low similarity. This resulted in many false\\nnegatives.\\nWe built our own near-duplicate detector to ﬁx this issue.\\nWe created a synthetic data augmentation pipeline that com-\\nbined a variety of common image manipulations. The aug-\\nmentation pipeline combines random cropping and zooming,\\naspect ratio distortion, downsizing and upscaling to different\\nresolutions, minor rotations, jpeg compression, and HSV\\ncolor jitter. The pipeline also randomly selects from differ-\\nent interpolation algorithms for all relevant steps. We then\\ntrained a model to maximize the similarity of an image and\\nits transformed variant while minimizing similarity to all\\nother images in a training batch. We used the same n-pair /\\nInfoNCE loss as CLIP but with a ﬁxed temperature of 0.07.\\nWe selected a ResNet-50 as the model architecture. We\\nmodiﬁed the base ResNet-50 with the anti-alias improve-\\nments from (Zhang, 2019) and used weight norm (Sali-\\nmans & Kingma, 2016) instead of batch norm (Ioffe &\\nSzegedy, 2015) to avoid leaking information about dupli-\\ncates via batch statistics - a problem previously noted in\\n(Henaff, 2020). We also found the GELU activation func-\\ntion (Hendrycks & Gimpel, 2016) to perform better for this\\ntask. We trained the model with a total batch size of 1,712\\nfor approximately 30 million images sampled from our pre-\\ntraining dataset. At the end of training it achieves nearly\\n100% accuracy on its proxy training task.Linear Classiﬁer Zero Shot\\nDataset YFCC WIT ∆ YFCC WIT ∆\\nBirdsnap 47.4 35.3 +12.1 19.9 4.5 +15.4\\nCountry211 23.1 17.3 +5.8 5.2 5.3 +0.1\\nFlowers102 94.4 89.8 +4.6 48.6 21.7 +26.9\\nGTSRB 66.8 72.5 −5.7 6.9 7.0 −0.1\\nUCF101 69.2 74.9 −5.7 22.9 32.0 −9.1\\nStanford Cars 31.4 50.3 −18.9 3.8 10.9 −7.1\\nImageNet 62.0 60.8 +1.2 31.3 27.6 +3.7\\nDataset Average 65.5 66.6 −1.1 29.6 30.0 −0.4\\nDataset “Wins” 10 15 −5 19 18 +1\\nTable 12. CLIP performs similarly when trained on only\\nYFCC100M. Comparing a ResNet-50 trained on only\\nYFCC100M with a same sized subset of WIT shows simi-\\nlar average performance and number of wins on zero shot and\\nlinear classiﬁer evals. However, large differences in dataset\\nspeciﬁc performance occur. We include performance on the 3\\ndatasets where YFCC does best and worst compared to WIT\\naccording to a linear probe in order to highlight this as well as\\naggregate performance across all linear and zero-shot evals and\\nthe canonical ImageNet dataset.\\nD. Dataset Ablation on YFCC100M\\nTo study whether our custom dataset is critical to the perfor-\\nmance of CLIP, we trained a model on a ﬁltered subset of\\nthe YFCC100M dataset (details described in Section 2.2)\\nand compared its performance to the same model trained\\non an equally sized subset of WIT. We train each model for\\n32 epochs at which point transfer performance begins to\\nplateau due to overﬁtting. Results are shown in Table 12.\\nAcross our whole eval suite, YFCC and WIT perform simi-\\nlarly on average for both zero-shot and linear probe settings.\\nHowever, performance on speciﬁc ﬁne-grained classiﬁca-\\ntion datasets can vary widely - sometimes by over 10%.\\nOur speculation is that these differences in performance re-\\nﬂect the relative density of relevant data in each pre-training\\ndataset. For instance, pre-training on YFCC100M, which\\nmight contain many photos of birds and ﬂowers (common\\nsubjects for photographers), results in better performance on\\nBirdsnap and Flowers102, while pre-training on WIT results\\nin better car and pet classiﬁers (which appear common in\\nour dataset).\\nOverall, these results are encouraging as they suggest our\\napproach can use any reasonably ﬁltered collection of paired\\n(text, image) data. This mirrors recent work which reported\\npositive results using the same contrastive pre-training ob-\\njective on the relatively different domain of medical imaging\\n(Zhang et al., 2020). It also is similar to the ﬁndings of noisy\\nstudent self-training which reported only slight improve-\\nments when using their JFT300M dataset over YFCC100M\\n(Xie et al., 2020). We suspect the major advantage of our\\ndataset over the already existing YFCC100M is its much\\nlarger size.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='52cdefab-0fbc-477b-97f1-787b3713d4b2', embedding=None, metadata={'page_label': '45', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='55105aa71d23dd4e2194a45d339a73e273a7b22977b18230504ba5a2f87b6056', text='Learning Transferable Visual Models From Natural Language Supervision 45\\nFinally, we caution that WIT includes this ﬁltered subset\\nof YFCC100M. This could result in our ablation under-\\nestimating the size of performance differences between\\nYFCC100M and the rest of WIT. We do not think this is\\nlikely as YFCC100M is only 3.7% of the overall WIT data\\nblend and it did not noticeably change the performance of\\nmodels when it was added to the existing data blend during\\nthe creation of WIT.\\nE. Selected Task and Dataset Results\\nDue to the large variety of datasets and experiments consid-\\nered in this work, the main body focuses on summarizing\\nand analyzing overall results. In the following subsections\\nwe report details of performance for speciﬁc groups of tasks,\\ndatasets, and evaluation settings.\\nE.1. Image and Text Retrieval\\nCLIP pre-trains for the task of image-text retrieval on our\\nnoisy web-scale dataset. Although the focus of this paper\\nis on representation learning and task learning for the pur-\\npose of transfer to a wide variety of downstream datasets,\\nvalidating that CLIP is able to achieve high transfer perfor-\\nmance transfer on exactly what it is pre-trained for is an\\nimportant sanity check / proof of concept. In Table 13 we\\ncheck the zero-shot transfer performance of CLIP for both\\ntext and image retrieval on the Flickr30k and MSCOCO\\ndatsets. Zero-shot CLIP matches or outperforms all prior\\nzero-shot results on these two datasets. Zero-shot CLIP is\\nalso competitive with the current overall SOTA for the task\\nof text retrieval on Flickr30k. On image retrieval, CLIP’s\\nperformance relative to the overall state of the art is notice-\\nably lower. However, zero-shot CLIP is still competitive\\nwith a ﬁne-tuned Unicoder-VL. On the larger MS-COCO\\ndataset ﬁne-tuning improves performance signiﬁcantly and\\nzero-shot CLIP is not competitive with the most recent work.\\nFor both these datasets we prepend the prompt “ a photo\\nof” to the description of each image which we found boosts\\nCLIP’s zero-shot R@1 performance between 1 and 2 points.\\nE.2. Optical Character Recognition\\nAlthough visualizations have shown that ImageNet models\\ncontain features that respond to the presence of text in an\\nimage (Zeiler & Fergus, 2014), these representations are\\nnot sufﬁciently ﬁne-grained to use for the task of optical\\ncharacter recognition (OCR). To compensate, models are\\naugmented with the outputs of custom OCR engines and\\nfeatures to boost performance on tasks where this capability\\nis required (Singh et al., 2019; Yang et al., 2020). Early dur-\\ning the development of CLIP, we noticed that CLIP began to\\nlearn primitive OCR capabilities which appeared to steadily\\nimprove over the course of the project. To evaluate this\\nqualitatively noticed behavior, we measured performanceon 5 datasets requiring the direct and indirect use of OCR.\\nThree of these datasets MNIST (LeCun), SVHN (Netzer\\net al., 2011), and IIIT5K (Mishra et al., 2012) directly check\\nthe ability of a model to perform low-level character and\\nword recognition, while Hateful Memes (Kiela et al., 2020)\\nand SST-2 (Socher et al., 2013) check the ability of a model\\nto use OCR to perform a semantic task. Results are reported\\nin Table 14.\\nCLIP’s performance is still highly variable and appears to\\nbe sensitive to some combination of the domain (rendered or\\nnatural images) and the type of text to be recognized (num-\\nbers or words). CLIP’s OCR performance is strongest Hate-\\nful Memes and SST-2 - datasets where the text is digitally\\nrendered and consists mostly of words. On IIIT5K, which\\nis natural images of individually cropped words, zero-shot\\nCLIP performs a bit more respectively and its performance\\nis similar to Jaderberg et al. (2014) early work combining\\ndeep learning and structured prediction to perform open-\\nvocabulary OCR. However, performance is noticeably lower\\non two datasets involving recognition of hand written and\\nstreet view numbers. CLIP’s 51% accuracy on full number\\nSVHN is well below any published results. Inspection sug-\\ngests CLIP struggles with repeated characters as well as the\\nlow resolution and blurry images of SVHN. CLIP’s zero-\\nshot MNIST performance is also poor and is outperformed\\nby supervised logistic regression on raw pixels, one of the\\nsimplest possible machine learning baselines.\\nSST-2 is a sentence level NLP dataset which we render into\\nimages. We include SST-2 in order to check whether CLIP\\nis able to convert low level OCR capability into a higher\\nlevel representation. Fitting a linear classiﬁer on CLIP’s rep-\\nresentation of rendered sentences achives 80.5% accuracy.\\nThis is on par with the 80% accuracy of a continuous bag\\nof words baseline using GloVe word vectors pre-trained on\\n840 billion tokens (Pennington et al., 2014). While this is a\\nsimple NLP baseline by today’s standard, and well below\\nthe 97.5% of the current SOTA, it is encouraging to see\\nthat CLIP is able to turn an image of rendered text into a\\nnon-trivial sentence level representation. Fully supervised\\nCLIP is also surprisingly strong on Hateful Meme detec-\\ntion, where CLIP is only 0.7 points behind the current single\\nmodel SOTA and several points above the best baseline from\\nthe original paper. Similar to SST-2, these other results on\\nHateful Memes use the ground truth text which CLIP does\\nnot have access to. Finally, we note that zero-shot CLIP\\noutperforms the best results using fully supervised linear\\nprobes across all other 56 models included in our evaluation\\nsuite. This suggests CLIP’s OCR capability is at least some-\\nwhat unique compared to existing work on self-supervised\\nand supervised representation learning.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ca10d43c-e3b6-4d5a-84ae-f36af35ecbfe', embedding=None, metadata={'page_label': '46', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='9de08489ef450878ac6be9e84d84b0c7607b7134f09049aee6b76165ce9f1b70', text='Learning Transferable Visual Models From Natural Language Supervision 46\\nText Retrieval Image Retrieval\\nFlickr30k MSCOCO Flickr30k MSCOCO\\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10FinetuneUnicoder-VLa86.2 96.3 99.0 62.3 87.1 92.8 71.5 90.9 94.9 46.7 76.0 85.3\\nUniterb87.3 98.0 99.2 65.7 88.6 93.8 75.6 94.1 96.8 52.9 79.9 88.0\\nVILLAc87.9 97.5 98.8 - - - 76.3 94.2 96.8 - - -\\nOscard- - - 73.5 92.2 96.0 - - - 57.5 82.8 89.8\\nERNIE-ViLe88.7 98.0 99.2 - - - 76.7 93.6 96.4 - - -Zero-ShotVisual N-Gramsf15.4 35.7 45.1 8.7 23.1 33.3 8.8 21.2 29.9 5.0 14.5 21.9\\nImageBERTg- - - 44.0 71.2 80.4 - - - 32.3 59.0 70.2\\nUnicoder-VLa64.3 86.8 92.3 - - - 48.4 76.0 85.2 - - -\\nUniterb83.6 95.7 97.7 - - - 68.7 89.2 93.9 - - -\\nCLIP 88.0 98.7 99.4 58.4 81.5 88.1 68.7 90.6 95.2 37.8 62.4 72.2\\nTable 13. CLIP improves zero-shot retrieval and is competitive with the best ﬁne-tuned result on Flickr30k text retrieval. Bold\\nindicates best overall performance while an underline indicates best in category performance (zero-shot or ﬁne-tuned). For all other\\nmodels, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set.\\na(Li et al., 2020a)b(Chen et al., 2019)c(Gan et al., 2020)d(Li et al., 2020b)e(Yu et al., 2020)f(Li et al., 2017)g(Qi et al., 2020)\\nIIIT5K Hateful\\nMNIST SVHN 1k Memes SST-2FinetuneSOTA 99.8a96.4b98.9c78.0d97.5e\\nJOINTf- - 89.6 - -\\nCBoWg- - - - 80.0LinearRaw Pixels 92.5 - - - -\\nES Best 98.9h- - 58.6h59.0i\\nCLIP 99.2 - - 77.3 80.5ZSCLIP 88.4 51.0 90.0 63.3 67.9\\nTable 14. OCR performance on 5 datasets. All metrics are accuracy\\non the test set except for Hateful Memes which reports ROC AUC\\non the dev set. Single model SOTA reported to best of knowledge.\\nES Best reports the best performance across the 56 non-CLIP\\nmodels in our evaluation suite.a(Assiri, 2020)b(Jaderberg et al.,\\n2015)c(Wang et al., 2020)d(Lippe et al., 2020)f(Jaderberg et al.,\\n2014)g(Wang et al., 2018)h(Xie et al., 2020)i(Mahajan et al.,\\n2018)\\nE.3. Action Recognition in Videos\\nFor the purpose of learning, a potentially important aspect\\nof natural language is its ability to express, and therefore su-\\npervise, an extremely wide set of concepts. A CLIP model,\\nsince it is trained to pair semi-arbitrary text with images, is\\nlikely to receive supervision for a wide range of visual con-\\ncepts involving both common and proper nouns, verbs, and\\nadjectives. ImageNet-1K, by contrast, only labels common\\nnouns. Does the lack of broader supervision in ImageNet\\nresult in weaker transfer of ImageNet models to tasks involv-\\ning the recognition of visual concepts that are not nouns?\\nTo investigate this, we measure and compare the perfor-\\nmance of CLIP and ImageNet models on several videoUCF101 K700 RareAct\\nTop-1 A VG mWAP mWSAPFinetuneR(2+1)D-BERTa98.7 - - -\\nNS ENet-L2b- 84.8 - -\\nHT100M S3Dd91.3 - - -\\nBaseline I3De- 70.2 - -LinearMMV FACf91.8 - - -\\nNS ENet-L2c89.4c68.2c- -\\nCLIP 92.0 73.0 - -ZSHT100M S3Dd- - 30.5 34.8\\nCLIP 80.3 69.6 40.7 44.8\\nTable 15. Action recognition performance on 3 video datasets. Sin-\\ngle model SOTA reported to best of knowledge. Note that linear\\nCLIP andlinear NS ENet-L2 are trained and evaluated on a single\\nframe subsampled version of each dataset and not directly compa-\\nrable to prior work. On Kinetics-700, we report the ActivityNet\\ncompetition metric which is the average of top-1 and top-5 per-\\nformance.a(Kalfaoglu et al., 2020)b(Lu et al., 2020)c(Xie et al.,\\n2020)d(Miech et al., 2020b)e(Carreira et al., 2019)f(Alayrac\\net al., 2020)\\naction classiﬁcation datasets which measure the ability of a\\nmodel to recognize verbs. In Table 15 we report results on\\nUCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira\\net al., 2019), two common datasets for the task. Unfortu-\\nnately, our CPU based linear classiﬁer takes a prohibitively\\nlong time to evaluate on a video dataset due to the very large\\nnumber of training frames. To deal with this, we aggres-\\nsively sub-sample each video to only a single center frame,\\neffectively turning it into an image classiﬁcation dataset.\\nAs a result, our reported performance in a linear evaluation\\nsetting likely under estimates performance by a moderate\\namount.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8fec2688-dc0c-4d17-9031-f603b525b17d', embedding=None, metadata={'page_label': '47', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5fed0cbcf72f7ab21137eef35d2effa6044ea010a3849d2848bf47a67dcaf0cc', text='Learning Transferable Visual Models From Natural Language Supervision 47\\nIN IN-V2 IN-A IN-R ObjectNet IN-Sketch IN-Vid YTBB\\nTop-1 Top-1 Top-1 Top-1 Top-1 Top-1 PM0 PM10 PM0 PM10\\nNS EfﬁcientNet-L2a88.3 80.2 84.9 74.7 68.5 47.6 88.0 82.1 67.7 63.5\\nFixResNeXt101-32x48d V2b86.4 78.0 68.4 80.0 57.8 59.1 85.8 72.2 68.9 57.7\\nLinear Probe CLIP 85.4 75.9 75.3 84.2 66.2 57.4 89.1 77.2 68.7 63.1\\nZero-Shot CLIP 76.2 70.1 77.2 88.9 72.3 60.2 95.3 89.2 95.2 88.5\\nTable 16. Detailed ImageNet robustness performance. IN is used to abbreviate for ImageNet.a(Xie et al., 2020)b(Touvron et al., 2019)\\nDespite this handicap, CLIP features transfer surprisingly\\nwell to this task. CLIP matches the best prior result on UCF-\\n101 in a linear probe evaluation setting and also outperforms\\nall other models in our evaluation suite. On Kinetics-700,\\nCLIP also outperforms the ﬁne-tuned I3D baseline from the\\noriginal paper. Since it does not require a training stage,\\nwe report CLIP’s zero-shot performance when averaging\\npredictions across all frames. CLIP also performs well in\\nthis setting and on Kinetics-700 its performance is within\\n1% of the fully supervised I3D baseline which is trained\\non 545000 labeled videos. Encouraged by these results, we\\nalso measure CLIP’s performance on the recently introduced\\nRareAct dataset (Miech et al., 2020a) which was designed\\nto measure zero-shot recognition of unusual actions like\\n“hammering a phone” and “drilling an egg”. CLIP improves\\nover the prior state of the art, a S3D model trained on auto-\\nmatically extracted captions from 100 million instructional\\nvideos, by 10 points.\\nWhile CLIP has encouragingly strong performance on the\\ntask of action recognition, we note that there are many differ-\\nences between the models being compared beyond just their\\nform of supervision such as model architecture, training\\ndata distribution, dataset size, and compute used. Further\\nwork is needed to more precisely determine what speciﬁc\\ndesign decisions contribute to achieving high performance\\non this task.\\n1km 25km 200km 750km 2500km\\nISNsa16.9 43.0 51.9 66.7 80.2\\nCPlaNetb16.5 37.1 46.4 62.0 78.5\\nCLIP 13.9 32.9 43.0 62.0 79.3\\nDeep-Ret+c14.4 33.3 47.7 61.6 73.4\\nPlaNetd8.4 24.5 37.6 53.6 71.3\\nTable 17. Geolocalization performance on the IM2GPS test set.\\nMetric is percent of images localized within a given radius. Models\\nare ordered by average performance.a(Muller-Budack et al., 2018)\\nb(Hongsuck Seo et al., 2018)c(V o et al., 2017)c(Weyand et al.,\\n2016)E.4. Geolocalization\\nAnother behavior we noticed during the development of\\nCLIP was its ability to recognize many places and locations.\\nTo quantify this we created the Country211 dataset as de-\\nscribed in Appendix A and report results on it throughout\\nthe paper. However it is a new benchmark so to compare\\nwith prior work on geolocalization we also report results\\non the IM2GPS test set from Hays & Efros (2008) in Table\\n17. Since IM2GPS is a regression benchmark, we guess the\\nGPS coordinates of the nearest image in a set of reference\\nimages using CLIP’s embedding space. This is not a zero-\\nshot result since it uses nearest-neighbor regression. Despite\\nquerying only 1 million images, which is much less than\\nprior work, CLIP performs similarly to several task speciﬁc\\nmodels. It is not, however, competitive with the current state\\nof the art.\\nE.5. Robustness to Distribution Shift\\nSection 3.3 provides a high level summary and analysis of\\nImageNet-related robustness results. We brieﬂy provide\\nsome additional numerical details in this appendix. Per-\\nformance results per dataset are provided in Table 16 and\\ncompared with the current state of the art results reported\\nin Taori et al. (2020)’s evaluation suite. Zero-shot CLIP im-\\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\\nBB. CLIP’s improvements are largest on ImageNet-Vid and\\nYoutube-BB due to its ﬂexible zero-shot capability and on\\nImageNet-R, which likely reﬂects CLIP’s pre-training dis-\\ntribution including signiﬁcant amounts of creative content.\\nA similar behavior has been documented for the Instagram\\npre-trained ResNeXt models as discussed in Taori et al.\\n(2020).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='699c6f91-6561-4b87-b8ae-ff3b57a4f591', embedding=None, metadata={'page_label': '48', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2cae170b8744b23a02c4292643bd642a702de2b16bee9e8b2ddd07a0d9ec9793', text='Learning Transferable Visual Models From Natural Language Supervision 48\\nF. Model Hyperparameters\\nHyperparameter Value\\nBatch size 32768\\nV ocabulary size 49408\\nTraining epochs 32\\nMaximum temperature 100.0\\nWeight decay 0.2\\nWarm-up iterations 2000\\nAdamβ1 0.9\\nAdamβ2 0.999 (ResNet), 0.98 (ViT)\\nAdamϵ 10−8(ResNet), 10−6(ViT)\\nTable 18. Common CLIP hyperparameters\\nLearning Embedding Input ResNet Text Transformer\\nModel rate dimension resolution blocks width layers width heads\\nRN50 5×10−41024 224 (3, 4, 6, 3) 2048 12 512 8\\nRN101 5×10−4512 224 (3, 4, 23, 3) 2048 12 512 8\\nRN50x4 5×10−4640 288 (4, 6, 10, 6) 2560 12 640 10\\nRN50x16 4×10−4768 384 (6, 8, 18, 8) 3072 12 768 12\\nRN50x64 3.6×10−41024 448 (3, 15, 36, 10) 4096 12 1024 16\\nTable 19. CLIP-ResNet hyperparameters\\nLearning Embedding Input Vision Transformer Text Transformer\\nModel rate dimension resolution layers width heads layers width heads\\nViT-B/32 5×10−4512 224 12 768 12 12 512 8\\nViT-B/16 5×10−4512 224 12 768 12 12 512 8\\nViT-L/14 4×10−4768 224 24 1024 16 12 768 12\\nViT-L/14-336px 2×10−5768 336 24 1024 16 12 768 12\\nTable 20. CLIP-ViT hyperparameters', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "required_exts = [\".pdf\"]\n",
    "reader = SimpleDirectoryReader(input_dir=document_path, required_exts=required_exts, recursive=True)\n",
    "docs = reader.load_data()\n",
    "print(f\"documents: {docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Index の構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.NODE_PARSING ->  1.051726 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.17051 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.053864 seconds\n",
      "Trace: index_construction\n",
      "    |_CBEventType.NODE_PARSING ->  1.051726 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.17051 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.053864 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.053925 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.037909 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.024215 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.048476 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.027224 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.016171 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.012628 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.038998 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.026826 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00611 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.002654 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000404 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000396 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000428 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000169 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.019642 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.015214 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.010744 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.010947 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.011914 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.015299 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.007601 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00812 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.008759 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00762 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.006497 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001778 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.007537 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.003114 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00901 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.006283 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.008434 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.008238 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00537 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.010173 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.007869 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.006466 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.011516 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.010039 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.005599 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.01095 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.007152 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.010271 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.008691 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00935 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.009796 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.007307 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.007349 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.007736 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.011124 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.003101 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.003376 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001433 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001237 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.035895 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000778 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.017402 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.003525 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.005775 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00837 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.003421 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.006024 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000485 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.534668 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.056794 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.058965 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.058114 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.048965 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.060202 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.052928 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.046937 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.045984 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.041928 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.044044 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.037315 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 構築したIndexの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存用のディレクトリを作成\n",
    "import os\n",
    "vector_index_dir = f\"{base_path}/vector_store\"\n",
    "os.makedirs(vector_index_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_context を保存\n",
    "storage_context = vector_index.storage_context\n",
    "storage_context.persist(vector_index_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 保存したIndexの読み出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore = SimpleDocumentStore.from_persist_dir(persist_dir=vector_index_dir),\n",
    "    vector_store = SimpleVectorStore.from_persist_dir(persist_dir=vector_index_dir),\n",
    "    index_store = SimpleIndexStore.from_persist_dir(persist_dir=vector_index_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "Trace: index_construction\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "vector_store_index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  173.090907 seconds\n",
      "      |_CBEventType.RETRIEVE ->  0.274669 seconds\n",
      "        |_CBEventType.EMBEDDING ->  0.055135 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  172.816051 seconds\n",
      "        |_CBEventType.TEMPLATING ->  3.8e-05 seconds\n",
      "        |_CBEventType.LLM ->  172.55109 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:        load time = 16011.75 ms\n",
      "llama_print_timings:      sample time =   214.04 ms /   256 runs   (    0.84 ms per token,  1196.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 116082.92 ms /  2425 tokens (   47.87 ms per token,    20.89 tokens per second)\n",
      "llama_print_timings:        eval time = 54544.01 ms /   255 runs   (  213.90 ms per token,     4.68 tokens per second)\n",
      "llama_print_timings:       total time = 172077.94 ms\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_store_index.as_query_engine(service_context=service_context)\n",
    "response = query_engine.query(\"自然言語処理の最近の動向について\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "近年、自然言語処理の研究は深層学習の手法を用いて、文章の意味や関係性などより高次な情報を抽出する手法の開発に注目が集まっています。また、画像とテキストの両方の情報から一つの結果を出力するバイタルの研究も盛んです。\n",
      "\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: 自然言語処理の最近の動向について\n",
      "Answer: 近年、自然言語処理の研究は深層学習の手法を用いて、文章の意味や関係性などより高次な情報を抽出する手法の開発に注目が集まっています。また、画像とテキストの両方の情報から一つの結果を出力するバイタルの研究も盛んです。\n",
      "\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: 自然言語処理の最近の動向について\n",
      "Answer: 近年、自然言語処理の研究は深層学習の手法を用いて、文章の意味や関係性などより高次な情報を抽出する手法の開発に注目が集ま\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. プロンプトテンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = \"prompt.txt\"\n",
    "promot_dir_path = f\"{base_path}/prompt_temp\"\n",
    "f_path = f\"{promot_dir_path}/{f_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f_path, 'r', encoding='utf-8') as file:\n",
    "    text_qa_template_str = file.read()\n",
    "\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "query_engine = vector_store_index.as_query_engine(\n",
    "    #response_mode=\"refine\",\n",
    "    response_mode=\"compact\",\n",
    "    #response_mode=\"tree_summarize\", \n",
    "    text_qa_template=text_qa_template, \n",
    "    service_context=service_context, \n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  204.172424 seconds\n",
      "      |_CBEventType.RETRIEVE ->  0.073542 seconds\n",
      "        |_CBEventType.EMBEDDING ->  0.067013 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  204.094899 seconds\n",
      "        |_CBEventType.TEMPLATING ->  4.4e-05 seconds\n",
      "        |_CBEventType.LLM ->  203.644593 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 249080.89 ms\n",
      "llama_print_timings:      sample time =   166.36 ms /   221 runs   (    0.75 ms per token,  1328.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 45636.30 ms /  2412 tokens (   18.92 ms per token,    52.85 tokens per second)\n",
      "llama_print_timings:        eval time = 155638.97 ms /   220 runs   (  707.45 ms per token,     1.41 tokens per second)\n",
      "llama_print_timings:       total time = 203544.82 ms\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"自然言語処理の最近の動向について\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然言語処理の近年の動きは、深層学習による進化が著しいです。\n",
      "- 文章を理解するためには、単語や句から構成要素とその関係性を学ぶ必要があります。\n",
      "- この構成要素とその関係性は、文書の構造と表現に反映されています。\n",
      "- 従って、文書の構造と表現を学習することで、文章を理解する能力を身につけることが可能です。\n",
      "- 深層学習の手法は、このような課題を解決するために適しているため、近年急速に普及してきました。\n",
      "- 特に、自然言語処理では、大量の文書からデータを集めることができるため、大規模なモデルを訓練し、性能を向上させることが可能です。\n",
      "- また、構成要素とその関係性を表す情報が、単なる文字列ではなく、数値的な表現で表されることも深層学習の手法を自然言語処理に適している理由の一つです。\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. プロンプトの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.XMLUtils import get_sections, make_xml_file\n",
    "from src.OpenAIUtils import get_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/home/paper_translator/data/documents/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    "pdf_name = (\n",
    "    \"Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training methods which learn directly from raw text have revolutionized NLP over the last few years (Dai & Le, 2015;Peters et al., 2018;Howard & Ruder, 2018;Radford et al., 2018;Devlin et al., 2018;Raffel et al., 2019).Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities. The development of \"text-to-text\" as a standardized input-output interface (McCann et al., 2018;Radford et al., 2019;Raffel et al., 2019) has enabled taskagnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization. Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data.These results suggest that the aggregate supervision accessible to modern pre-training methods within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets. However, in other fields such as computer vision it is still standard practice to pre-train models on crowd-labeled datasets such as ImageNet (Deng et al., 2009). Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision? Prior work is encouraging.Over 20 years ago Mori et al. (1999) explored improving content based image retrieval by training a model to predict the nouns and adjectives in text documents paired with images. Quattoni et al. (2007) demonstrated it was possible to learn more data efficient image representations via manifold learning in the weight space of classifiers trained to predict words in captions associated with images. Srivastava & Salakhutdinov (2012) explored deep representation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features. Joulin et al. (2016) modernized this line of work and demonstrated that CNNs trained to predict words in image captions learn useful image representations. They converted the title, description, and hashtag metadata of images in the YFCC100M dataset (Thomee et al., 2016) into a bag-ofwords multi-label classification task and showed that pretraining AlexNet (Krizhevsky et al., 2012) to predict these labels learned representations which preformed similarly to ImageNet-based pre-training on transfer tasks. Li et al. (2017) then extended this approach to predicting phrase ngrams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image classification datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score. Adopting more recent architectures and pre-training approaches, VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-VIRT (Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text.While exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on common benchmarks is much lower than alternative approaches. For example, Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accuracy of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012). Instead, more narrowly scoped but well-targeted uses of weak supervision have improved performance. Mahajan et al. (2018) showed that predicting ImageNet-related hashtags on Instagram images is an effective pre-training task. When fine-tuned to ImageNet these pre-trained models increased accuracy by over 5% and improved the overall state of the art at the time. Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset.This line of work represents the current pragmatic middle ground between learning from a limited amount of supervised \"gold-labels\" and learning from practically unlimited amounts of raw text. However, it is not without compro-mises. Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their \"zero-shot\" capabilities.A crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale. While Mahajan et al. (2018) and Kolesnikov et al. (2019) trained their models for accelerator years on millions to billions of images, VirTex, ICMLM, and ConVIRT trained for accelerator days on one to two hundred thousand images. In this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision. We study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute and observe that transfer performance is a smoothly predictable function of compute (Hestness et al., 2017;Kaplan et al., 2020). We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find  . CLIP is much more efficient at zero-shot transfer than our image caption baseline. Although highly expressive, we found that transformer-based language models are relatively weak at zero-shot ImageNet classification. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words (BoW) encoding of the text (Joulin et al., 2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x.it can be competitive with prior task-specific supervised models. We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model's capability. These results have significant policy and ethical implications, which we consider in Section 7.\n"
     ]
    }
   ],
   "source": [
    "root = make_xml_file(dir_path=dir_path, pdf_name=pdf_name, is_debug=True)\n",
    "section = get_sections(root=root)[0]\n",
    "print(section.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"\n",
    "### 指示 ###\n",
    "文章の内容の中で、重要なポイントを3つ箇条書きしてください。\n",
    "箇条書きは、以下の制約に従ってください。\n",
    "\n",
    "### 箇条書きの制約 ###\n",
    "- 箇条書きの数は3個\n",
    "- 箇条書きは、文章の内容を簡潔にまとめたものである必要があります。\n",
    "- 箇条書き1個を50文字以内\n",
    "\n",
    "### 対象とする論文の内容 ###\n",
    "{text}\n",
    "\n",
    "### 出力形式 ###\n",
    "- 箇条書き1\n",
    "- 箇条書き2\n",
    "- 箇条書き3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pre-training methods have revolutionized NLP by learning directly from raw text, scaling across compute, model capacity, and data, allowing for task-agnostic architectures to zero-shot transfer to downstream datasets.\n",
      "- Prior work suggests that using natural language supervision for image representation learning is promising, although demonstrated performance on common benchmarks is much lower than alternative approaches.\n",
      "- CLIP, a simplified version of ConVIRT trained from scratch for Contrastive Language-Image Pre-training, is an efficient method for learning from natural language supervision. CLIP learns to perform a wide set of tasks during pre-training and is competitive with prior task-specific supervised models.\n"
     ]
    }
   ],
   "source": [
    "prompt_text = get_message(text=section.body, system=SYSTEM)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = query_engine.query(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLIP (Contrastive Language-Image Pre-training) is a method that enables efficient learning from natural language supervision and competitive performance with task-specific supervised models in various computer vision tasks. It uses a contrastive learning approach to learn representations from large amounts of textual data, which can be used for image classification, object detection, segmentation, among others.\n",
      "The original answer is as follows: - Pre-training methods directly from raw text have revolutionized NLP, and may hold potential for computer vision as well.\n",
      "- Natural language supervision for image representation learning is still rare due to lower performance on common benchmarks compared to alternative approaches.\n",
      "- CLIP, a contrastive language-image pre-training method, enables efficient learning from natural language supervision and is competitive with prior task-specific supervised models.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
