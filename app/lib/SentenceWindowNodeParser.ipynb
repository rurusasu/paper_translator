{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/paper_translator/lib',\n",
      " '/home/paper_translator',\n",
      " '/usr/lib/python311.zip',\n",
      " '/usr/lib/python3.11',\n",
      " '/usr/lib/python3.11/lib-dynload',\n",
      " '',\n",
      " '/home/paper_translator/.venv/lib/python3.11/site-packages',\n",
      " '/home/paper_translator/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pprint import pprint\n",
    "sys.path.append(\"/home/paper_translator/\")\n",
    "pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# ログレベルの設定\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "llmama_debug_handler = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llmama_debug_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ドキュメントの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/home/paper_translator/.venv/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_verify_locations cafile='/home/paper_translator/.venv/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:slack_bolt.App:Sending a request - url: https://www.slack.com/api/auth.test, query_params: {}, body_params: {}, files: {}, json_body: None, headers: {'Content-Type': 'application/x-www-form-urlencoded', 'Authorization': '(redacted)', 'User-Agent': 'Bolt/1.18.0 Python/3.11.5 slackclient/3.23.0 Linux/5.15.133.1-microsoft-standard-WSL2'}\n",
      "DEBUG:slack_bolt.App:Received the following response - status: 200, headers: {'date': 'Tue, 24 Oct 2023 22:14:44 GMT', 'server': 'Apache', 'vary': 'Accept-Encoding', 'x-slack-req-id': 'a20c069657fea01a72887dcea7109d12', 'x-content-type-options': 'nosniff', 'x-xss-protection': '0', 'pragma': 'no-cache', 'cache-control': 'private, no-cache, no-store, must-revalidate', 'expires': 'Sat, 26 Jul 1997 05:00:00 GMT', 'content-type': 'application/json; charset=utf-8', 'x-oauth-scopes': 'chat:write,app_mentions:read,channels:history,chat:write.customize,chat:write.public,commands,groups:history,mpim:history,im:history', 'access-control-expose-headers': 'x-slack-req-id, retry-after', 'access-control-allow-headers': 'slack-route, x-slack-version-ts, x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, x-b3-flags', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'referrer-policy': 'no-referrer', 'x-slack-unique-id': 'ZThB1F91BssuYBVBN7fD6gAAwBo', 'x-slack-backend': 'r', 'access-control-allow-origin': '*', 'content-length': '194', 'via': '1.1 slack-prod.tinyspeck.com, envoy-www-iad-xwmcjqfi, envoy-edge-nrt-slwwcqoo', 'x-envoy-attempt-count': '1', 'x-envoy-upstream-service-time': '188', 'x-backend': 'main_normal main_canary_with_overflow main_control_with_overflow', 'x-server': 'slack-www-hhvm-main-iad-acsi', 'x-slack-shared-secret-outcome': 'no-match', 'x-edge-backend': 'envoy-www', 'x-slack-edge-shared-secret-outcome': 'no-match', 'connection': 'close'}, body: {\"ok\":true,\"url\":\"https:\\/\\/work-dlf7572.slack.com\\/\",\"team\":\"Work\",\"user\":\"papertranslator\",\"team_id\":\"T04HAF9RVQD\",\"user_id\":\"U05Q1615Y1X\",\"bot_id\":\"B05PNDXBT7D\",\"is_enterprise_install\":false}\n"
     ]
    }
   ],
   "source": [
    "from src.XMLUtils import DocumentCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/paper_translator/data\"\n",
    "document_name = (\n",
    "    \"Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    "    #\"ChatKBQA_A_Generate-then-Retrieve_Framework_for_Knowledge_Base_Question_Answering_with_Fine-tuned_Large_Language_Models\"\n",
    ")\n",
    "document_path = f\"{base_path}/documents/{document_name}\"\n",
    "xml_path = f\"{document_path}/{document_name}.tei.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents_2 metadata: \n",
      "{'Section No.': '1.', 'Section Title': 'Introduction and Motivating Work', 'Title': 'Learning Transferable Visual Models From Natural Language Supervision', 'Authors': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever', 'Idno': 'arXiv:2103.00020v1[cs.CV]', 'Published': '26 Feb 2021', 'Language': 'en'}\n",
      "documents_2 text: \n",
      "Pre-training methods which learn directly from raw text have revolutionized NLP over the last few years (Dai & Le, 2015;Peters et al., 2018;Howard & Ruder, 2018;Radford et al., 2018;Devlin et al., 2018;Raffel et al., 2019).Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities. The development of \"text-to-text\" as a standardized input-output interface (McCann et al., 2018;Radford et al., 2019;Raffel et al., 2019) has enabled taskagnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization. Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data.These results suggest that the aggregate supervision accessible to modern pre-training methods within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets. However, in other fields such as computer vision it is still standard practice to pre-train models on crowd-labeled datasets such as ImageNet (Deng et al., 2009). Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision? Prior work is encouraging.Over 20 years ago Mori et al. (1999) explored improving content based image retrieval by training a model to predict the nouns and adjectives in text documents paired with images. Quattoni et al. (2007) demonstrated it was possible to learn more data efficient image representations via manifold learning in the weight space of classifiers trained to predict words in captions associated with images. Srivastava & Salakhutdinov (2012) explored deep representation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features. Joulin et al. (2016) modernized this line of work and demonstrated that CNNs trained to predict words in image captions learn useful image representations. They converted the title, description, and hashtag metadata of images in the YFCC100M dataset (Thomee et al., 2016) into a bag-ofwords multi-label classification task and showed that pretraining AlexNet (Krizhevsky et al., 2012) to predict these labels learned representations which preformed similarly to ImageNet-based pre-training on transfer tasks. Li et al. (2017) then extended this approach to predicting phrase ngrams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image classification datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score. Adopting more recent architectures and pre-training approaches, VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-VIRT (Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text.I 1 •T 2 I 1 •T 3 … I 2 •T 1 I 2 •T 3 … I 3 •T 1 I 3 •T 2 … ⋮ ⋮ ⋮ I 1 •T 1 I 2 •T 2 I 3 •T 3(While exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on common benchmarks is much lower than alternative approaches. For example, Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accuracy of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012). Instead, more narrowly scoped but well-targeted uses of weak supervision have improved performance. Mahajan et al. (2018) showed that predicting ImageNet-related hashtags on Instagram images is an effective pre-training task. When fine-tuned to ImageNet these pre-trained models increased accuracy by over 5% and improved the overall state of the art at the time. Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset.This line of work represents the current pragmatic middle ground between learning from a limited amount of supervised \"gold-labels\" and learning from practically unlimited amounts of raw text. However, it is not without compro-mises. Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their \"zero-shot\" capabilities.A crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale. While Mahajan et al. (2018) and Kolesnikov et al. (2019) trained their models for accelerator years on millions to billions of images, VirTex, ICMLM, and ConVIRT trained for accelerator days on one to two hundred thousand images. In this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision. We study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute and observe that transfer performance is a smoothly predictable function of compute (Hestness et al., 2017;Kaplan et al., 2020). We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find  . CLIP is much more efficient at zero-shot transfer than our image caption baseline. Although highly expressive, we found that transformer-based language models are relatively weak at zero-shot ImageNet classification. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words (BoW) encoding of the text (Joulin et al., 2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x.it can be competitive with prior task-specific supervised models. We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model's capability. These results have significant policy and ethical implications, which we consider in Section 7.\n"
     ]
    }
   ],
   "source": [
    "# 自作の DirectoryReader を使用して、\n",
    "# ディレクトリ内の xml ファイルをDocumentオブジェクトとして読み込む\n",
    "# run_grobid(dir_path, pdf_name)\n",
    "creator = DocumentCreator()\n",
    "creator.load_xml(xml_path, contain_abst=False)\n",
    "docs = creator.create_docs()\n",
    "print(f\"documents_2 metadata: \\n{docs[0].metadata}\")\n",
    "print(f\"documents_2 text: \\n{docs[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1660 Ti, compute capability 7.5\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 45043,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:            blk.3.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:            blk.4.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:            blk.7.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:            blk.9.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:           blk.10.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:           blk.11.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:           blk.14.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:           blk.15.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.17.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:           blk.21.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:           blk.22.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 45043,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:       general.source.hugginface.repository str     \n",
      "llama_model_loader: - kv   3:                   llama.tensor_data_layout str     \n",
      "llama_model_loader: - kv   4:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   5:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   6:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   8:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:               general.quantization_version u32     \n",
      "llama_model_loader: - kv  20:                          general.file_type u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 45043\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.85 B\n",
      "llm_load_print_meta: model size       = 3.87 GiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name   = ELYZA-japanese-Llama-2-7b-fast-instruct\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  = 3961.79 MB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/35 layers to GPU\n",
      "llm_load_tensors: VRAM used: 0.00 MB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  = 1950.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 281.25 MB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 275.37 MB\n",
      "llama_new_context_with_model: total VRAM used: 275.37 MB (model: 0.00 MB, context: 275.37 MB)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from src.translator.llama_cpp import create_llama_cpp_model\n",
    "\n",
    "model_path = \"/home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf\"\n",
    "llm = create_llama_cpp_model(package_name=\"llama_index\", model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1. SentenceWindowNodeParser の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"sentence_window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Embedding の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-l6-v2/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-l6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "chunk_size = 3072\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=model_name, max_length=chunk_size, device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. ServiceContext の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "ctx = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    node_parser=node_parser,\n",
    "    callback_manager=callback_manager,\n",
    "    chunk_size=chunk_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. StorageContext の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "\n",
    "# Storage Context の作成\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore = SimpleDocumentStore(),\n",
    "    vector_store = SimpleVectorStore(),\n",
    "    index_store = SimpleIndexStore()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Index の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.base import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) QAプロンプトの定義。\n",
    "# QAシステムプロンプト\n",
    "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n",
    "    content=(\n",
    "            \"#依頼\\n\"\n",
    "            \"あなたは高度な理解能力を持ち、複雑なテキストも簡潔に要約することができるAIです。\\n\"\n",
    "            \"事前知識ではなく、提供されたコンテキストに基づいて精確な回答を行ってください。\\n\"\n",
    "            \"#従うべきルール\\n\"\n",
    "            \"1. 略語や初出の用語には解説を加え、AI分野やコンピュータの初心者も理解できるように工夫してください。\\n\"\n",
    "            \"2. 回答内で指定されたコンテキストを直接参照しないでください。\\n\"\n",
    "            \"3. 「コンテキストに基づいて、...」や「コンテキスト情報は...」、またはそれに類するような記述は避けてください。\\n\"\n",
    "            \"#手順\\n\"\n",
    "            \"1. 与えられたコンテキストに含まれる主要なポイントやコンセプトを細かく分解してください。\\n\"\n",
    "            \"2. それぞれのポイントやコンセプトに対して詳細な説明を加えてください。\\n\"\n",
    "            \"3. まずは指示に従って、文書の初版を作成してください。\\n\"\n",
    "            \"4. 作成した初版をルールに従っているか自己分析してください。\\n\"\n",
    "            \"5. 自己分析の結果を踏まえて、文書を改善してください。\\n\"\n",
    "    ),\n",
    "    role=MessageRole.SYSTEM,\n",
    ")\n",
    "\n",
    "# QAプロンプトテンプレートメッセージ\n",
    "TEXT_QA_PROMPT_TMPL_MSGS = [\n",
    "    TEXT_QA_SYSTEM_PROMPT,\n",
    "    ChatMessage(\n",
    "        content=(\n",
    "            \"複数のソースからのコンテキスト情報を以下に示します。\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"予備知識ではなく、複数のソースからの情報を考慮して質問に答えてください。\\n\"\n",
    "            \"疑問がある場合は、「情報無し」と答えてください。\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        ),\n",
    "        role=MessageRole.USER,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# チャットQAプロンプト\n",
    "CHAT_TEXT_QA_PROMPT = ChatPromptTemplate(message_templates=TEXT_QA_PROMPT_TMPL_MSGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) TreeSummarizeプロンプトの定義。\n",
    "# QAシステムプロンプト\n",
    "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n",
    "    content=(\n",
    "            \"#依頼\\n\"\n",
    "            \"あなたは高度な理解能力を持ち、複雑なテキストも簡潔に要約することができるAIです。\\n\"\n",
    "            \"事前知識ではなく、提供されたコンテキストに基づいて精確な回答を行ってください。\\n\"\n",
    "            \"#従うべきルール\\n\"\n",
    "            \"1. 略語や初出の用語には解説を加え、AI分野やコンピュータの初心者も理解できるように工夫してください。\\n\"\n",
    "            \"2. 回答内で指定されたコンテキストを直接参照しないでください。\\n\"\n",
    "            \"3. 「コンテキストに基づいて、...」や「コンテキスト情報は...」、またはそれに類するような記述は避けてください。\\n\"\n",
    "            \"#手順\\n\"\n",
    "            \"1. 与えられたコンテキストに含まれる主要なポイントやコンセプトを細かく分解してください。\\n\"\n",
    "            \"2. それぞれのポイントやコンセプトに対して詳細な説明を加えてください。\\n\"\n",
    "            \"3. まずは指示に従って、文書の初版を作成してください。\\n\"\n",
    "            \"4. 作成した初版をルールに従っているか自己分析してください。\\n\"\n",
    "            \"5. 自己分析の結果を踏まえて、文書を改善してください。\\n\"\n",
    "    ),\n",
    "    role=MessageRole.SYSTEM,\n",
    ")\n",
    "\n",
    "# ツリー要約プロンプトメッセージ\n",
    "TREE_SUMMARIZE_PROMPT_TMPL_MSGS = [\n",
    "    TEXT_QA_SYSTEM_PROMPT,\n",
    "    ChatMessage(\n",
    "        content=(\n",
    "            \"複数のソースからのコンテキスト情報を以下に示します。\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"予備知識ではなく、複数のソースからの情報を考慮して質問に答えてください。\\n\"\n",
    "            \"疑問がある場合は、「情報無し」と答えてください。\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        ),\n",
    "        role=MessageRole.USER,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ツリー要約プロンプト\n",
    "CHAT_TREE_SUMMARIZE_PROMPT = ChatPromptTemplate(\n",
    "    message_templates=TREE_SUMMARIZE_PROMPT_TMPL_MSGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaryクエリ\n",
    "SUMMARY_QUERY = \"提供されたテキストの内容を要約してください。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from llama_index import get_response_synthesizer\n",
    "\n",
    "# 非同期処理の有効化\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# レスポンスシンセサイザーの準備\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    "    text_qa_template=CHAT_TEXT_QA_PROMPT,  # QAプロンプト\n",
    "    summary_template=CHAT_TREE_SUMMARIZE_PROMPT,  # TreeSummarizeプロンプト\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. DocumentSummaryIndex の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Pre-training methods which learn directly from ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The development of \"text-to-text\" as a standard...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Flagship systems like GPT-3 (Brown et al., 2020...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, in other fields such as computer visio...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Could scalable pre-training methods which learn...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Prior work is encouraging.Over 20 years ago Mor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (1999) explored improving content based image r...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Quattoni et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2007) demonstrated it was possible to learn mo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Srivastava & Salakhutdinov (2012) explored deep...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Joulin et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2016) modernized this line of work and demonst...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: They converted the title, description, and hash...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Li et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2017) then extended this approach to predictin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Adopting more recent architectures and pre-trai...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is likely because demonstrated performance...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For example, Li et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2017) reach only 11.5% accuracy on ImageNet in...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is well below the 88.4% accuracy of the cu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: It is even below the 50% accuracy of classic co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Instead, more narrowly scoped but well-targeted...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Mahajan et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018) showed that predicting ImageNet-related ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: When fine-tuned to ImageNet these pre-trained m...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Kolesnikov et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019) and Dosovitskiy et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) have also demonstrated large gains on a ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, it is not without compro-mises. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Both works carefully design, and in the process...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Natural language is able to express, and theref...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Both approaches also use static softmax classif...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This severely curtails their flexibility and li...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While Mahajan et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018) and Kolesnikov et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019) trained their models for accelerator yea...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In this work, we close this gap and study the b...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Enabled by the large amounts of publicly availa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We study the scalability of CLIP by training a ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We find that CLIP, similar to the GPT family, l...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We measure this by benchmarking the zero-shot t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP is much more efficient at zero-shot transf...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Although highly expressive, we found that trans...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Here, we see that it learns 3x slower than a ba...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Swapping the prediction objective for the contr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also confirm these findings with linear-prob...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We additionally find that zero-shot CLIP models...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These results have significant policy and ethic...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: At the core of our approach is the idea of lear...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As discussed in the introduction, this is not a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Zhang et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020), Gomez et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2017), Joulin et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2016), and Desai & Johnson (2020) all introduc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: All these approaches are learning from natural ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Although early work wrestled with the complexit...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: It's much easier to scale natural language supe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Instead, methods which work on natural language...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Learning from natural language also has an impo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In the following subsections, we detail the spe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Existing work has mainly used three datasets, M...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While MS-COCO and Visual Genome are high qualit...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: By comparison, other computer vision systems ar...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: YFCC100M, at 100 million photos, is a possible ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Many images use automatically generated filenam...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: After filtering to keep only images with natura...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is approximately the same size as ImageNet...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Since existing datasets do not adequately refle...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To address this, we constructed a new dataset o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To attempt to cover as broad a set of visual co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 1 We approximately class 1 The base query list ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is augmented with bi-grams balance the res...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The resulting dataset has a similar total word ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We refer to this dataset as WIT for WebImageText.\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: State-of-the-art computer vision systems use ve...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Mahajan et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018) required 19 GPU years to train their Res...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) required 33 TPUv3 core-years to train th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: When considering that both these systems were t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In the course of our efforts, we found training...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, we encountered difficulties efficientl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 2 we show that a 63 million parameter...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: They try to predict the exact words of the text...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is a difficult task due to the wide variet...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Recent work in contrastive representation learn...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Other work has found that although generative m...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Noting these findings, we explored training a s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Starting with the same bag-of-words encoding ba...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To do this, CLIP learns a with high pointwise m...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finally all WordNet synsets not already in the ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: multi-modal embedding space by jointly training...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We optimize a symmetric cross entropy loss over...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 3 we include pseudocode of the core o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To our knowledge this batch construction techni...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018) as the InfoNCE loss, and was recently ad...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020).Due to the large size of our pre-trainin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We train CLIP from scratch without initializing...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We do not use the non-linear projection between...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ( 2019) and popularized by Chen et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020b). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We instead use only a linear projection to map ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We did not notice a difference in training effi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also remove the text transformation function...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) which samples a single sentence at unifo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also simplify the image transformation funct...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A random square crop from resized images is the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finally, the temperature parameter which contro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We consider two different architectures for the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For the first, we use ResNet-50 (He et al., 201...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We make several modifications to the original v...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019) and the antialiased rect-2 blur pooling ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also replace the global average pooling laye...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The attention pooling is implemented as a singl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For the second architecture, we experiment with...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We closely follow their implementation with onl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As a base size we use a 63M-parameter 12layer 5...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The transformer operates on a lower-cased byte ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For computational efficiency, the max sequence ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The text sequence is bracketed with [SOS] and [...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Masked self-attention was used in the text enco...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While Tan & Le (2019) tune the ratio of compute...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For the text encoder, we only scale the width o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We train a series of 5 ResNets and 3 Vision Tra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: They are denoted as RN50x4, RN50x16, and RN50x6...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For the Vision Transformers we train a ViT-B/32...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We train all models for 32 epochs. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We use the Adam optimizer (Kingma & Ba, 2014) w...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Initial hyperparameters were set using a combin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Hyper-parameters were then adapted heuristicall...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The learnable temperature parameter τ was initi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We use a very large minibatch size of 32,768. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Mixed-precision (Micikevicius et al., 2017) was...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To save additional memory, gradient checkpointi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The calculation of embedding similarities was a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The largest ResNet model, RN50x64, took 18 days...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For the ViT-L/14 we also pre-train at a higher ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We denote this model as ViT-L/14@336px. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Unless otherwise specified, all results reporte...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 3.1. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Zero-Shot Transfer\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In computer vision, zero-shot learning usually ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We instead use the term in a broader sense and ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We motivate this as a proxy for performing unse...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2008). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While much research in the field of unsupervise...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In this view, a dataset evaluates performance o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, many popular computer vision datasets ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While it is reasonable to say that the SVHN dat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On these kinds of datasets, zero-shot transfer ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Please see Section 3.3 for analysis focused on ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: It is also the only other work we are aware of ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Their approach learns the parameters of a dicti...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To our knowledge Liu et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018) first identified task learning as an \"un...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While GPT-1 (Radford et al., 2018) focused on p...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This analysis served as the basis for GPT-2 (Ra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP is pre-trained to predict if an image and ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To perform zero-shot classification, we reuse t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For each dataset, we use the names of all the c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In a bit more detail, we first compute the feat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Note that this prediction layer is a multinomia...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: When interpreted this way, the image encoder is...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Lei Ba et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2015) first introduced a zero-shot image class...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2013). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Continuing with this interpretation, every step...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For zero-shot evaluation, we cache the zero-sho...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This allows the cost of generating it to be amo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In . \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP improves performance on all three datasets...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This improvement reflects many differences in t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As mentioned above, the comparison to Visual N-...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For instance, we train on a dataset that is 10x...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As a closer comparison, we trained a CLIP ResNe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This baseline was also trained from scratch ins...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On aYahoo, CLIP achieves a 95% reduction in the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To conduct a more comprehensive analysis and st...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Most standard image classification datasets tre...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The vast majority of datasets annotate images w...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Some datasets, such as Flowers102 and GTSRB, do...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 2017) Figure 4. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Prompt engineering and ensembling improve zeros...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Compared to the baseline of using contextless c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This improvement is similar to the gain from us...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: chosen somewhat haphazardly and do not anticipa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: When the name of a class is the only informatio...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In some cases multiple meanings of the same wor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This happens in ImageNet which contains both co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Another example is found in classes of the Oxfo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usually the text is a full sentence describing ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To help bridge this distribution gap, we found ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: to be a good default that helps specify the tex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This often improves performance over the baseli...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For instance, just using this prompt improves a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Similar to the \"prompt engineering\" discussion ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A few, non exhaustive, examples follow. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We found on several fine-grained image classifi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For example on Oxford-IIIT Pets, using \"A photo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: to help provide context worked well. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Likewise, on Food101 specifying a type of food ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For OCR datasets, we found that putting quotes ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finally, we found that on satellite image class...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: \".We also experimented with ensembling over mul...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These classifiers are computed by using differe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We construct the ensemble over the embedding sp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This allows us to cache a single set of average...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We've observed ensembling across many generated...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On ImageNet, we ensemble 80 different context p...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: When considered together, prompt engineering an...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 4 we visualize how prompt engineering...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2017).\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Since task-agnostic zero-shot classifiers for c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In this section, we conduct a study of various ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As a first question, we look simply at how well...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To contextualize this, we compare to the perfor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 5  ten than not and wins on 16 of the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Looking at individual datasets reveals some int...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On fine-grained classification tasks, we observ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On two of these datasets, Stanford Cars and Foo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On OxfordPets and Birdsnap, performance is much...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We suspect these difference are primarily due t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On \"general\" object classification datasets suc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On STL10, CLIP achieves 99.3% overall which app...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Zeroshot CLIP significantly outperforms a ResNe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On Kinet-ics700, CLIP outperforms a ResNet-50 b...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Zeroshot CLIP also outperforms a ResNet-50's fe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We speculate this is due to natural language pr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These results highlight the poor capability of ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: By contrast, non-expert humans can robustly per...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, we caution that it is unclear whether ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 6, we visualize how zero-shot CLIP co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While it is intuitive to expect zero-shot to un...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is likely due to an important difference b...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: First, CLIP's zero-shot classifier is generated...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: By contrast, \"normal\" supervised learning must ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Context-less example-based learning has the dra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Although a capable learner is able to exploit v...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Research into better methods of combining the s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: That a BiT-M ResNet-152x2 performs best in a 16...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 7, we show estimates for the number o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Since zero-shot CLIP is also a linear classifie...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We find that zero-shot transfer can If we assum...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 8 we compare CLIP's zeroshot performa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The dashed, y = x line represents an \"optimal\" ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For most datasets, the performance of zero-shot...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Zero-shot performance is correlated with linear...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Comparing zero-shot and linear probe performanc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On only 5 datasets does zero-shot performance a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: mance, suggesting that CLIP is relatively consi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, zero-shot CLIP only approaches fully s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On all 5 datasets, both zero-shot accuracy and ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This suggests that CLIP may be more effective a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The slope of a linear regression model predicti...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, the 95th-percentile confidence interva...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The GPT family of models has so far demonstrate...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 9, we check whether the zero-shot per...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We plot the average error rate of the 5 ResNet ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While the overall trend is smooth, we found tha...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We are unsure whether this is caused by high va...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ( 2020)) masking a steadily improving trend or ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While we have extensively analyzed the task-lea...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: There exist many ways to evaluate the quality o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Fitting a linear classifier on a representation...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: An alternative is measuring the performance of ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This increases flexibility, and prior work has ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While the high performance of fine-tuning motiv...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Our work is focused on developing a high-perfor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Fine-tuning, because it adapts representations ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Linear classifiers, because of their limited fl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For CLIP, training supervised linear classifier...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finally, we aim to compare CLIP to a comprehens...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Studying 66 different models on 27 different da...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Fine-tuning opens up a much larger design and h...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: By comparison, linear classifiers require minim...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Please see Appendix A for further details on ev...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To minimize selection effects that could raise ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While small CLIP models such as a ResNet-50 and...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These small CLIP models also underperform model...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, models trained with CLIP scale very we...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also find that CLIP vision transformers are ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These results qualitatively replicate the findi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) which reported that vision transformers ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Our best overall model is a ViT-L/14 that is fi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This model outperforms the best existing model ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These tasks include geo-localization, optical c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: None of these tasks are measured in the evaluat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This could be argued to be a form of selection ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019)'s study towards tasks that overlap with ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To address this, we also measure performance on...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This evaluation suite, detailed in Appendix A i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Linear probe average over all 27 datasetsCLIP-V...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Linear probe performance of CLIP models in comp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (Left) Scores are averaged over 12 datasets stu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (Right) Scores are averaged over 27 datasets th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Dotted lines indicate models fine-tuned or eval...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: See Table 10 for individual scores and Figure 2...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: All CLIP models, regardless of scale, outperfor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The improvement in average score of the best mo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also find that self-supervised systems do no...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For instance, while SimCLRv2 still underperform...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019), SimCLRv2 outperforms BiT-M on our 27 da...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These findings suggest continuing to expand tas...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We suspect additional evaluation efforts along ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP outperforms the Noisy Student EfficientNet...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP improves the most on tasks which require O...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In addition CLIP also does much better on fine-...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This may reflect a problem with overly narrow s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A result such as the 14.7% improvement on GTSRB...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This could encourage a supervised representatio...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As mentioned, CLIP still underperforms the Effi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Unsurprisingly, the dataset that the Effi-cient...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The EffcientNet also slightly outperforms CLIP ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We suspect this is at least partly due to the l...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The Effi-cientNet also does slightly better on ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In 2015, it was announced that a deep learning ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, research in the subsequent years has r...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: What explains this discrepancy? \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Various ideas have been suggested and studied (...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A common theme of proposed explanations is that...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However many of these correlations and patterns...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Recalling the topic of discussion, it may be a ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To what degree are these failures attributable ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP models, which are trained via natural lang...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Taori et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) is a recent comprehensive study moving t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Taori et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) study how the performance of ImageNet mo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: They measure performance on a set of 7 distribu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: They distinguish these datasets, which all cons...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: They propose this distinction because in part b...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 3Across these collected datasets, the accuracy ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For the following summary discussion we report ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Additionally, for Youtube-BB and ImageNet-Vid, ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Encouragingly however, Taori et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) find that accuracy under distribution sh...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Taori et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020)  Linear probe average over 26 datasetsCL...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For both dataset splits, the transfer scores of...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This suggests that the representations of model...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Returning to the discussion in the introduction...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Intuitively, a zero-shot model should not be ab...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 4 Thus it is reasonable to expect zero-shot mod...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 13, we compare the performance of zer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: All zero-shot CLIP models improve effective rob...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Other details of CLIP, such as its large and di...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As an initial experiment to potentially begin n...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We visualize how performance changes from the z...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Although adapting CLIP to the ImageNet distribu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018), average accuracy under distribution shi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also break down the differences between zero...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ImageNetV2 closely followed the creation proces...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Performance decreases by 4.7% on   ImageNet-R, ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The change in accuracy on the two other dataset...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Is the gain primarily from \"exploiting spurious...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Is this behavior unique to some combination of ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Does it hold for end-to-end finetuning as well ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We do not have confident answers to these quest...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Prior work has also pre-trained models on distr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As a step towards understanding whether pre-tra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018), Kolesnikov et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019), and Dosovitskiy et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) to, if possible, study these questions o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The target classes across the 7 transfer datase...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Two datasets, Youtube-BB and ImageNet-Vid, cons...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This presents a problem when trying to use the ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Taori et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) handle this by max-pooling predictions a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Sometimes this mapping is much less than perfect. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For the person class in Youtube-BB, predictions...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: With CLIP we can instead generate a custom zero...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 14 we see that this improves average ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Curiously, accuracy on ObjectNet also increases...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Although the dataset was designed to closely ov...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To better understand this difference, we invest...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In Figure 15 we visualize the performance of 0-...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We see that while few-shot models also show hig...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Additionally, zero-shot CLIP is notably more ro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Across our experiments, high effective robustne...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019) and Linzen (2020)) promotes the developm...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We are curious to see if the same results hold ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While Hendrycks et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020b) has reported that pre-training improves...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020)'s study of the robustness of question an...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020), little evidence of effective robustness...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: How does CLIP compare to human performance and ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To get a better understanding of how well human...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We wanted to get a sense of how strong human ze...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This can help us to compare task difficulty for...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In the zero-shot case the humans were given no ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In the one-shot experiment the humans were give...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 5One possible concern was that the human worker...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: High human accuracy of 94% on the STL-10 datase...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The gain in accuracy going from zero to one sho...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This suggests that humans \"know what they don't...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Given this, it seems that while CLIP is a promi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2016) and others. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Because these few-shot evaluations of CLIP don'...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To our knowledge, using a linear classifier on ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2012), the metric is average per-class classif...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Most of the gain in performance when going from...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: \"Guesses\" refers to restricting the dataset to ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To the extent that errors are consistent, our h...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A concern with pre-training on a very large int...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is important to investigate since, in a wo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: One option to prevent this is to identify and r...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While this guarantees reporting true hold-out p...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This has the downside of limiting the scope of ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Adding a new evaluation would require an expens...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: To do this, we use the following procedure: con...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We denote the unaltered full dataset All for re...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: From this we first record the degree of data co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is the difference in accuracy due to conta...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: When positive it is our estimate of how much th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also calculate 99.5% Clopper-Pearson confide...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Out of 35 datasets studied, 9 datasets have no ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Most of these datasets are synthetic or special...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This demonstrates our detector has a low-false ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: There is a median overlap of 2.2% and an averag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Due to this small amount of overlap, overall ac...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Of these, only 2 are statistically significant ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The max detected improvement is only 0.6% on Bi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The largest overlap is for Country211 at 21.5%. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is due to it being constructed out of YFCC...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Despite this large overlap there is only a 0.2%...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This may be because the training text accompany...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: First our detector is not perfect. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While it achieves near 100% accuracy on its pro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For example, on Kinetics-700 many \"overlaps\" ar...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This explains why Kinetics-700 has an apparent ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We suspect more subtle distribution shifts like...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: One possibility we noticed on CIFAR-100 is that...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Changes in accuracy could instead be due to cha...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Unfortunately, these distribution and difficult...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Mahajan et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018) and Kolesnikov et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019) detected similar overlap rates and found...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Importantly, Kolesnikov et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ( 2019) also compared the alternative de-duplic...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: There are still many limitations to CLIP. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While several of these are discussed as part of...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Similarly, for only 6 datasets are the accuracy...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: On most of these datasets, the performance of t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Significant work is still needed to improve the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While scaling has so far steadily improved perf...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is infeasible to train with current hardwa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Further research into improving upon the comput...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: When compared to task-specific models, the perf...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP also struggles with more abstract and syst...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finally for novel tasks which are unlikely to b...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We are confident that there are still many, man...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: An illustrative example occurs for the task of ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, CLIP only achieves 88% accuracy on the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: An embarrassingly simple baseline of logistic r...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Both semantic and near-duplicate nearest-neighb...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This suggests CLIP does little to address the u...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Instead CLIP tries to circumvent the problem an...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is a naive assumption that, as MNIST demon...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is a significant restriction compared to a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Unfortunately, as described in Section 2.3 we f...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A simple idea worth trying is joint training of...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As another alternative, search could be perform...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2017).CLIP also does not address the poor data...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Instead CLIP compensates by using a source of s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: If every image seen during training of a CLIP m...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Combining CLIP with self-supervision (Henaff, 2...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Despite our focus on zero-shot transfer, we rep...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These validation sets often have thousands of e...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Similar concerns have been raised in the field ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Another potential issue is our selection of eva...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While we have reported results on Kornblith et ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019)'s 12 dataset evaluation suite as a stand...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Creating a new benchmark of tasks designed expl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These image-text pairs are unfiltered and uncur...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This has been previously demonstrated for image...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We refer readers to Section 7 for detailed anal...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Many complex tasks and visual concepts can be d...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Actual training examples are undeniably useful ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In our work, we fall back to fitting linear cla...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This results in a counter-intuitive drop in per...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As discussed in Section 4, this is notably diff...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Future work is needed to develop methods that c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP has a wide range of capabilities due to it...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: One can give it images of cats and dogs and ask...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Like any image classification system, CLIP's pe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP also introduces a capability that will mag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This capability introduces challenges similar t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For example, it can find relevant images in a d...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Further, the relative ease of steering CLIP tow...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We then characterize the model's performance in...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Many of CLIP's capabilities are omni-use in nat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: OCR can be used to make scanned documents searc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Several of the capabilities measured, from acti...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Given its social implications, we address this ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Our bias tests represent our initial efforts to...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP and models like it will need to be analyze...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Further community exploration will be required ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Race \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Algorithmic decisions, training data, and choic...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Class design is particularly relevant to models...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also conduct exploratory bias research inten...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2019).We start by analyzing the performance of...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: It categorizes gender into 2 groups: female and...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: There are inherent problems with race and gende...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Bowker & Star (2000) as an initial bias probe, ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We find that LR CLIP gets higher accuracy on th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ZS CLIP's performance varies by category and is...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (See Table 3 andTable  4). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: and Keyes (2018) have shown. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While FairFace's dataset reduces the proportion...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We use the 2 gender categories and 7 race categ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 7 One challenge with this comparison is that th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Additionally, we test the performance of the LR...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We find that model performance on gender classi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Table 5 summarizes these results.While LR CLIP ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) have shown, and often fails as a meaning...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Even if a model has both higher accuracy and lo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For example, higher performance on underreprese...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Our use of facial classification benchmarks to ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We carried out an experiment in which the ZS CL...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The goal of this experiment was to check if har...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Out of these, 'Black' images had the highest mi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: People aged 0-20 years had the highest proporti...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Interestingly, we found that people aged 0-20 y...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We found significant disparities in classificat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Our goal here was to see if this category would...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We found that this drastically reduced the numb...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This points to how class design has the potenti...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Poor class design can lead to poor real world p...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) to test how CLIP treated images of men a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As part of these experiments, we studied how ce...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For our first label set, we used a label set of...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We found that the model got 100% accuracy on th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is slightly better performance than the mo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We hypothesize that one of the reasons for this...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We found that the lower threshold led to lower ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, even the differing distributions of la...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For example, we find that under the 0.5% thresh...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This points to gendered associations similar to...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, the presence of these biases amongst l...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) found in GCV systems, we found our syste...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For example, labels such as 'brown hair', 'blon...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Additionally, CLIP attached some labels that de...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Out of the only four occupations that it attach...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is again similar to the biases found in GC...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Many occupation oriented words such as 'militar...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The reverse was not true. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Descriptive words used to describe women were s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We next sought to characterize model performanc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Our analysis aims to better embody the characte...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP performance on Member of Congress images w...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The 20 most gendered labels for men and women w...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Labels are sorted by absolute frequencies. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Bars denote the percentage of images for a cert...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: around such systems. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Our inclusion of surveillance is not intended t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We first tested model performance on low-resolu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CCTV cameras). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We used the VIRAT dataset (Oh et al., 2011) and...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Given CLIP's flexible class construction, we te...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Coarse classification required the model to cor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: determine if the image was a picture of an empt...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: For fine-grained classification, the model had ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Additionally, we carried out a 'stress test' wh...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 'parking lot with red car'). \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We found that the model had a top-1 accuracy of...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The accuracy dropped significantly to 51.1% for...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Note that this experiment was targeted only tow...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We did this to evaluate the model's performance...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While we tested this on a dataset of celebritie...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, this performance dropped to 43.3% when...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This performance is not competitive when compar...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, what makes these results noteworthy is...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, large datasets and high performing sup...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As a result, CLIP's comparative appeal for such...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Additionally, CLIP is not designed for common s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This means it has limited use for certain surve...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Thus, CLIP and similar models could enable besp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: As our experiments show, ZS CLIP displays nontr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This preliminary analysis is intended to illust...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This process of characterization can help resea...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This is an admittedly extremely broad area and ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Work in NLP intentionally leveraging natural la...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Dialog based learning (Weston, 2016;Li et al., ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Several papers have leveraged semantic parsing ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: More recently, ExpBERT (Murty et al., 2020) use...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In this context, the earliest use of the term n...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2013) which showed that natural language descr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, as mentioned in the introduction and a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Other early work leveraged tags (but not natura...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: More recently, He & Peng (2017) and Liang et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) demonstrated using natural language desc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Others have investigated how grounded language ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finally, techniques which combine natural langu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP's pre-training task optimizes for text-ima...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This areas of research dates back to the mid-90...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (1999) as representative of early work. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While initial efforts focused primarily on pred...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Over time work explored many combinations of tr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Stroud et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) explores large scale representation lear...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Several works have explored using dense spoken ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: When considered together with CLIP, these works...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Alayrac et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2020) extended this line of work to an additio...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modern work on image-text retrieval has relied ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, these datasets are still relatively sm...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Several methods have been proposed to create la...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ( 2011) as a notable early example. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In the deep learning era, Mithun et al. \n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (2018) demonstrated an additional set of (image...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However, these datasets still use significantly...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This line of work queries image search engines ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Classifiers trained on these large but noisily ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These image-query pairs are also often used to ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP also uses search queries as part of its da...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: However CLIP only uses full text sequences co-o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We also restrict this step in CLIP to text only...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Of this line of work, Learning Everything about...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This line of work focuses on richly connecting ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These approaches leverage impressively engineer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: These systems are then jointly fine-tuned via v...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP is instead focused on learning visual mode...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The only interaction in a CLIP model between th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We are excited to see CLIP hybridized with this...\n",
      "current doc id: 0\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nPre-training methods which learn directly from raw text have revolutionized NLP over the last few years (Dai & Le, 2015;Peters et al., 2018;Howard & Ruder, 2018;Radford et al., 2018;Devlin et al., 2018;Raffel et al., 2019).Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe development of \\\\\"text-to-text\\\\\" as a standardized input-output interface (McCann et al., 2018;Radford et al., 2019;Raffel et al., 2019) has enabled taskagnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFlagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data.These results suggest that the aggregate supervision accessible to modern pre-training methods within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nHowever, in other fields such as computer vision it is still standard practice to pre-train models on crowd-labeled datasets such as ImageNet (Deng et al., 2009).\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nCould scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision?\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nPrior work is encouraging.Over 20 years ago Mori et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(1999) explored improving content based image retrieval by training a model to predict the nouns and adjectives in text documents paired with images.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nQuattoni et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2007) demonstrated it was possible to learn more data efficient image representations via manifold learning in the weight space of classifiers trained to predict words in captions associated with images.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nSrivastava & Salakhutdinov (2012) explored deep representation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nJoulin et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2016) modernized this line of work and demonstrated that CNNs trained to predict words in image captions learn useful image representations.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThey converted the title, description, and hashtag metadata of images in the YFCC100M dataset (Thomee et al., 2016) into a bag-ofwords multi-label classification task and showed that pretraining AlexNet (Krizhevsky et al., 2012) to predict these labels learned representations which preformed similarly to ImageNet-based pre-training on transfer tasks.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nLi et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2017) then extended this approach to predicting phrase ngrams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image classification datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAdopting more recent architectures and pre-training approaches, VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-VIRT (Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text.I 1 \\\\u2022T 2 I 1 \\\\u2022T 3 \\\\u2026 I 2 \\\\u2022T 1 I 2 \\\\u2022T 3 \\\\u2026 I 3 \\\\u2022T 1 I 3 \\\\u2022T 2 \\\\u2026 \\\\u22ee \\\\u22ee \\\\u22ee I 1 \\\\u2022T 1 I 2 \\\\u2022T 2 I 3 \\\\u2022T 3(While exciting as proofs of concept, using natural language supervision for image representation learning is still rare.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis is likely because demonstrated performance on common benchmarks is much lower than alternative approaches.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nrecently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text.I 1 \\\\u2022T 2 I 1 \\\\u2022T 3 \\\\u2026 I 2 \\\\u2022T 1 I 2 \\\\u2022T 3 \\\\u2026 I 3 \\\\u2022T 1 I 3 \\\\u2022T 2 \\\\u2026 \\\\u22ee \\\\u22ee \\\\u22ee I 1 \\\\u2022T 1 I 2 \\\\u2022T 2 I 3 \\\\u2022T 3(While exciting as proofs of concept, using natural language supervision for image representation learning is still rare.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis is likely because demonstrated performance on common benchmarks is much lower than alternative approaches.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor example, Li et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis is well below the 88.4% accuracy of the current state of the art (Xie et al., 2020).\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIt is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012).\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nInstead, more narrowly scoped but well-targeted uses of weak supervision have improved performance.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nMahajan et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2018) showed that predicting ImageNet-related hashtags on Instagram images is an effective pre-training task.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhen fine-tuned to ImageNet these pre-trained models increased accuracy by over 5% and improved the overall state of the art at the time.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nKolesnikov et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2019) and Dosovitskiy et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2020) have also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset.This line of work represents the current pragmatic middle ground between learning from a limited amount of supervised \\\\\"gold-labels\\\\\" and learning from practically unlimited amounts of raw text.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nHowever, it is not without compro-mises.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nBoth works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nNatural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nBoth approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis severely curtails their flexibility and limits their \\\\\"zero-shot\\\\\" capabilities.A crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhile Mahajan et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2018) and Kolesnikov et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nKim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhile Mahajan et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2018) and Kolesnikov et al.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2019) trained their models for accelerator years on millions to billions of images, VirTex, ICMLM, and ConVIRT trained for accelerator days on one to two hundred thousand images.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nEnabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute and observe that transfer performance is a smoothly predictable function of compute (Hestness et al., 2017;Kaplan et al., 2020).\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find  .\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nCLIP is much more efficient at zero-shot transfer than our image caption baseline.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAlthough highly expressive, we found that transformer-based language models are relatively weak at zero-shot ImageNet classification.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nHere, we see that it learns 3x slower than a baseline which predicts a bag-of-words (BoW) encoding of the text (Joulin et al., 2016).\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nSwapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x.it can be competitive with prior task-specific supervised models.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model\\'s capability.\\\\n\\\\nSection No.: 1.\\\\nSection Title: Introduction and Motivating Work\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThese results have significant policy and ethical implications, which we consider in Section 7.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=9358 request_id=2400657555fd81323d928aa13ffdb6f6 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」という論文のコンテキスト情報です。この論文では、自然言語の監督学習を用いて学習された画像分類モデルの振る舞いについて研究されています。論文では、大規模なデータセットを用いて学習されたモデルの転移性能を評価し、その効率性や汎用性について議論されています。また、論文では、モデルの学習において対照的な言語-画像事前学習を行うことで、効率性が向上することが示されています。さらに、論文では、モデルの性能評価やロバスト性についても議論されています。この研究結果は、政策や倫理にも重要な影響を与える可能性があります。\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=17450 request_id=448e9ec2b7d7f096e3e151f54df391a3 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストには、自然言語監督を使用した画像表現学習の可能性についての情報が含まれています。これは、トランスフォーマーベースの言語モデリング、マスクされた言語モデリング、および対照的な目標を使用して、テキストから画像表現を学習するための証明として最近示されました。しかし、自然言語監督を使用した画像表現学習の性能は、他の手法と比較して低いことが示されています。一部の研究では、ImageNetのゼロショット設定での正確度がわずか11.5％であり、現在の最先端の正確度である88.4％や、古典的なコンピュータビジョン手法の50％を下回っています。しかし、弱い監督のより狭い範囲での使用は、性能の向上に貢献しています。さらに、Instagramの画像でImageNet関連のハッシュタグを予測することが効果的な事前学習タスクであることも示されています。これらの事前学習モデルをImageNetにファインチューニングすると、正確度が5％以上向上し、当時の最先端の性能を改善します。ただし、自然言語監督モデルは、1000クラスまたは18291クラスに限定された監督を使用しており、動的な出力のメカニズムを欠いています。これにより、柔軟性が制限され、\"ゼロショット\"の能力も制限されます。自然言語は、その一般性により、視覚的な概念のより広範なセットを表現し、監督することができます。\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=23301 request_id=035740809ac7d462897bf503aea2e53d response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:このテキストは、自然言語の監督から転移可能な視覚モデルを学習する方法についての研究に関するものです。過去数年間、生のテキストから直接学習する事前学習手法は、NLP（自然言語処理）の分野で革命を起こしてきました。自己回帰的およびマスクされた言語モデリングなどのタスク非依存の目的は、計算、モデル容量、データの多様性のオーダーを超えてスケーリングし、能力を着実に向上させています。\n",
      "\n",
      "この研究では、テキストからテキストへの変換が標準化された入出力インターフェースとして使用され、タスク非依存のアーキテクチャがダウンストリームのデータセットにゼロショット転送されることが可能になりました。これにより、特殊な出力ヘッドやデータセット固有のカスタマイズの必要性がなくなりました。\n",
      "\n",
      "GPT-3などのフラッグシップシステムは、ほとんどまたはまったくデータセット固有のトレーニングデータを必要とせずに、多くのタスクで特注のモデルと競争力を持つようになりました。これらの結果は、現代の事前学習手法がウェブスケールのテキストコレクション内で利用可能な集約的な監督が、高品質なクラウドラベル付きNLPデータセットを上回っていることを示唆しています。\n",
      "\n",
      "しかし、コンピュータビジョンなどの他の分野では、ImageNetなどのクラウドラベル付きデータセットでモデルを事前学習するのが標準的な手法です。\n",
      "\n",
      "この研究では、ウェブテキストから直接学習するスケーラブルな事前学習手法が、コンピュータビジョンにおいても同様のブレークスルーをもたらす可能性があるのかを検討しています。\n",
      "\n",
      "過去の研究は、テキストドキュメントと関連する画\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\n\\\\u3053\\\\u306e\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306f\\\\u3001\\\\u81ea\\\\u7136\\\\u8a00\\\\u8a9e\\\\u306e\\\\u76e3\\\\u7763\\\\u304b\\\\u3089\\\\u8ee2\\\\u79fb\\\\u53ef\\\\u80fd\\\\u306a\\\\u8996\\\\u899a\\\\u30e2\\\\u30c7\\\\u30eb\\\\u3092\\\\u5b66\\\\u7fd2\\\\u3059\\\\u308b\\\\u65b9\\\\u6cd5\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u306e\\\\u7814\\\\u7a76\\\\u306b\\\\u95a2\\\\u3059\\\\u308b\\\\u3082\\\\u306e\\\\u3067\\\\u3059\\\\u3002\\\\u904e\\\\u53bb\\\\u6570\\\\u5e74\\\\u9593\\\\u3001\\\\u751f\\\\u306e\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u304b\\\\u3089\\\\u76f4\\\\u63a5\\\\u5b66\\\\u7fd2\\\\u3059\\\\u308b\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u624b\\\\u6cd5\\\\u306f\\\\u3001NLP\\\\uff08\\\\u81ea\\\\u7136\\\\u8a00\\\\u8a9e\\\\u51e6\\\\u7406\\\\uff09\\\\u306e\\\\u5206\\\\u91ce\\\\u3067\\\\u9769\\\\u547d\\\\u3092\\\\u8d77\\\\u3053\\\\u3057\\\\u3066\\\\u304d\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\u81ea\\\\u5df1\\\\u56de\\\\u5e30\\\\u7684\\\\u304a\\\\u3088\\\\u3073\\\\u30de\\\\u30b9\\\\u30af\\\\u3055\\\\u308c\\\\u305f\\\\u8a00\\\\u8a9e\\\\u30e2\\\\u30c7\\\\u30ea\\\\u30f3\\\\u30b0\\\\u306a\\\\u3069\\\\u306e\\\\u30bf\\\\u30b9\\\\u30af\\\\u975e\\\\u4f9d\\\\u5b58\\\\u306e\\\\u76ee\\\\u7684\\\\u306f\\\\u3001\\\\u8a08\\\\u7b97\\\\u3001\\\\u30e2\\\\u30c7\\\\u30eb\\\\u5bb9\\\\u91cf\\\\u3001\\\\u30c7\\\\u30fc\\\\u30bf\\\\u306e\\\\u591a\\\\u69d8\\\\u6027\\\\u306e\\\\u30aa\\\\u30fc\\\\u30c0\\\\u30fc\\\\u3092\\\\u8d85\\\\u3048\\\\u3066\\\\u30b9\\\\u30b1\\\\u30fc\\\\u30ea\\\\u30f3\\\\u30b0\\\\u3057\\\\u3001\\\\u80fd\\\\u529b\\\\u3092\\\\u7740\\\\u5b9f\\\\u306b\\\\u5411\\\\u4e0a\\\\u3055\\\\u305b\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u3053\\\\u306e\\\\u7814\\\\u7a76\\\\u3067\\\\u306f\\\\u3001\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u304b\\\\u3089\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3078\\\\u306e\\\\u5909\\\\u63db\\\\u304c\\\\u6a19\\\\u6e96\\\\u5316\\\\u3055\\\\u308c\\\\u305f\\\\u5165\\\\u51fa\\\\u529b\\\\u30a4\\\\u30f3\\\\u30bf\\\\u30fc\\\\u30d5\\\\u30a7\\\\u30fc\\\\u30b9\\\\u3068\\\\u3057\\\\u3066\\\\u4f7f\\\\u7528\\\\u3055\\\\u308c\\\\u3001\\\\u30bf\\\\u30b9\\\\u30af\\\\u975e\\\\u4f9d\\\\u5b58\\\\u306e\\\\u30a2\\\\u30fc\\\\u30ad\\\\u30c6\\\\u30af\\\\u30c1\\\\u30e3\\\\u304c\\\\u30c0\\\\u30a6\\\\u30f3\\\\u30b9\\\\u30c8\\\\u30ea\\\\u30fc\\\\u30e0\\\\u306e\\\\u30c7\\\\u30fc\\\\u30bf\\\\u30bb\\\\u30c3\\\\u30c8\\\\u306b\\\\u30bc\\\\u30ed\\\\u30b7\\\\u30e7\\\\u30c3\\\\u30c8\\\\u8ee2\\\\u9001\\\\u3055\\\\u308c\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u53ef\\\\u80fd\\\\u306b\\\\u306a\\\\u308a\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\u3053\\\\u308c\\\\u306b\\\\u3088\\\\u308a\\\\u3001\\\\u7279\\\\u6b8a\\\\u306a\\\\u51fa\\\\u529b\\\\u30d8\\\\u30c3\\\\u30c9\\\\u3084\\\\u30c7\\\\u30fc\\\\u30bf\\\\u30bb\\\\u30c3\\\\u30c8\\\\u56fa\\\\u6709\\\\u306e\\\\u30ab\\\\u30b9\\\\u30bf\\\\u30de\\\\u30a4\\\\u30ba\\\\u306e\\\\u5fc5\\\\u8981\\\\u6027\\\\u304c\\\\u306a\\\\u304f\\\\u306a\\\\u308a\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\n\\\\nGPT-3\\\\u306a\\\\u3069\\\\u306e\\\\u30d5\\\\u30e9\\\\u30c3\\\\u30b0\\\\u30b7\\\\u30c3\\\\u30d7\\\\u30b7\\\\u30b9\\\\u30c6\\\\u30e0\\\\u306f\\\\u3001\\\\u307b\\\\u3068\\\\u3093\\\\u3069\\\\u307e\\\\u305f\\\\u306f\\\\u307e\\\\u3063\\\\u305f\\\\u304f\\\\u30c7\\\\u30fc\\\\u30bf\\\\u30bb\\\\u30c3\\\\u30c8\\\\u56fa\\\\u6709\\\\u306e\\\\u30c8\\\\u30ec\\\\u30fc\\\\u30cb\\\\u30f3\\\\u30b0\\\\u30c7\\\\u30fc\\\\u30bf\\\\u3092\\\\u5fc5\\\\u8981\\\\u3068\\\\u305b\\\\u305a\\\\u306b\\\\u3001\\\\u591a\\\\u304f\\\\u306e\\\\u30bf\\\\u30b9\\\\u30af\\\\u3067\\\\u7279\\\\u6ce8\\\\u306e\\\\u30e2\\\\u30c7\\\\u30eb\\\\u3068\\\\u7af6\\\\u4e89\\\\u529b\\\\u3092\\\\u6301\\\\u3064\\\\u3088\\\\u3046\\\\u306b\\\\u306a\\\\u308a\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\u3053\\\\u308c\\\\u3089\\\\u306e\\\\u7d50\\\\u679c\\\\u306f\\\\u3001\\\\u73fe\\\\u4ee3\\\\u306e\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u624b\\\\u6cd5\\\\u304c\\\\u30a6\\\\u30a7\\\\u30d6\\\\u30b9\\\\u30b1\\\\u30fc\\\\u30eb\\\\u306e\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u30b3\\\\u30ec\\\\u30af\\\\u30b7\\\\u30e7\\\\u30f3\\\\u5185\\\\u3067\\\\u5229\\\\u7528\\\\u53ef\\\\u80fd\\\\u306a\\\\u96c6\\\\u7d04\\\\u7684\\\\u306a\\\\u76e3\\\\u7763\\\\u304c\\\\u3001\\\\u9ad8\\\\u54c1\\\\u8cea\\\\u306a\\\\u30af\\\\u30e9\\\\u30a6\\\\u30c9\\\\u30e9\\\\u30d9\\\\u30eb\\\\u4ed8\\\\u304dNLP\\\\u30c7\\\\u30fc\\\\u30bf\\\\u30bb\\\\u30c3\\\\u30c8\\\\u3092\\\\u4e0a\\\\u56de\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u3053\\\\u3068\\\\u3092\\\\u793a\\\\u5506\\\\u3057\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u3057\\\\u304b\\\\u3057\\\\u3001\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u30d3\\\\u30b8\\\\u30e7\\\\u30f3\\\\u306a\\\\u3069\\\\u306e\\\\u4ed6\\\\u306e\\\\u5206\\\\u91ce\\\\u3067\\\\u306f\\\\u3001ImageNet\\\\u306a\\\\u3069\\\\u306e\\\\u30af\\\\u30e9\\\\u30a6\\\\u30c9\\\\u30e9\\\\u30d9\\\\u30eb\\\\u4ed8\\\\u304d\\\\u30c7\\\\u30fc\\\\u30bf\\\\u30bb\\\\u30c3\\\\u30c8\\\\u3067\\\\u30e2\\\\u30c7\\\\u30eb\\\\u3092\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u3059\\\\u308b\\\\u306e\\\\u304c\\\\u6a19\\\\u6e96\\\\u7684\\\\u306a\\\\u624b\\\\u6cd5\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\n\\\\u3053\\\\u306e\\\\u7814\\\\u7a76\\\\u3067\\\\u306f\\\\u3001\\\\u30a6\\\\u30a7\\\\u30d6\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u304b\\\\u3089\\\\u76f4\\\\u63a5\\\\u5b66\\\\u7fd2\\\\u3059\\\\u308b\\\\u30b9\\\\u30b1\\\\u30fc\\\\u30e9\\\\u30d6\\\\u30eb\\\\u306a\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u624b\\\\u6cd5\\\\u304c\\\\u3001\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u30d3\\\\u30b8\\\\u30e7\\\\u30f3\\\\u306b\\\\u304a\\\\u3044\\\\u3066\\\\u3082\\\\u540c\\\\u69d8\\\\u306e\\\\u30d6\\\\u30ec\\\\u30fc\\\\u30af\\\\u30b9\\\\u30eb\\\\u30fc\\\\u3092\\\\u3082\\\\u305f\\\\u3089\\\\u3059\\\\u53ef\\\\u80fd\\\\u6027\\\\u304c\\\\u3042\\\\u308b\\\\u306e\\\\u304b\\\\u3092\\\\u691c\\\\u8a0e\\\\u3057\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u904e\\\\u53bb\\\\u306e\\\\u7814\\\\u7a76\\\\u306f\\\\u3001\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u30c9\\\\u30ad\\\\u30e5\\\\u30e1\\\\u30f3\\\\u30c8\\\\u3068\\\\u95a2\\\\u9023\\\\u3059\\\\u308b\\\\u753b\\\\n\\\\n\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u306f\\\\u3001\\\\u81ea\\\\u7136\\\\u8a00\\\\u8a9e\\\\u76e3\\\\u7763\\\\u3092\\\\u4f7f\\\\u7528\\\\u3057\\\\u305f\\\\u753b\\\\u50cf\\\\u8868\\\\u73fe\\\\u5b66\\\\u7fd2\\\\u306e\\\\u53ef\\\\u80fd\\\\u6027\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u306e\\\\u60c5\\\\u5831\\\\u304c\\\\u542b\\\\u307e\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3053\\\\u308c\\\\u306f\\\\u3001\\\\u30c8\\\\u30e9\\\\u30f3\\\\u30b9\\\\u30d5\\\\u30a9\\\\u30fc\\\\u30de\\\\u30fc\\\\u30d9\\\\u30fc\\\\u30b9\\\\u306e\\\\u8a00\\\\u8a9e\\\\u30e2\\\\u30c7\\\\u30ea\\\\u30f3\\\\u30b0\\\\u3001\\\\u30de\\\\u30b9\\\\u30af\\\\u3055\\\\u308c\\\\u305f\\\\u8a00\\\\u8a9e\\\\u30e2\\\\u30c7\\\\u30ea\\\\u30f3\\\\u30b0\\\\u3001\\\\u304a\\\\u3088\\\\u3073\\\\u5bfe\\\\u7167\\\\u7684\\\\u306a\\\\u76ee\\\\u6a19\\\\u3092\\\\u4f7f\\\\u7528\\\\u3057\\\\u3066\\\\u3001\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u304b\\\\u3089\\\\u753b\\\\u50cf\\\\u8868\\\\u73fe\\\\u3092\\\\u5b66\\\\u7fd2\\\\u3059\\\\u308b\\\\u305f\\\\u3081\\\\u306e\\\\u8a3c\\\\u660e\\\\u3068\\\\u3057\\\\u3066\\\\u6700\\\\u8fd1\\\\u793a\\\\u3055\\\\u308c\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\u3057\\\\u304b\\\\u3057\\\\u3001\\\\u81ea\\\\u7136\\\\u8a00\\\\u8a9e\\\\u76e3\\\\u7763\\\\u3092\\\\u4f7f\\\\u7528\\\\u3057\\\\u305f\\\\u753b\\\\u50cf\\\\u8868\\\\u73fe\\\\u5b66\\\\u7fd2\\\\u306e\\\\u6027\\\\u80fd\\\\u306f\\\\u3001\\\\u4ed6\\\\u306e\\\\u624b\\\\u6cd5\\\\u3068\\\\u6bd4\\\\u8f03\\\\u3057\\\\u3066\\\\u4f4e\\\\u3044\\\\u3053\\\\u3068\\\\u304c\\\\u793a\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u4e00\\\\u90e8\\\\u306e\\\\u7814\\\\u7a76\\\\u3067\\\\u306f\\\\u3001ImageNet\\\\u306e\\\\u30bc\\\\u30ed\\\\u30b7\\\\u30e7\\\\u30c3\\\\u30c8\\\\u8a2d\\\\u5b9a\\\\u3067\\\\u306e\\\\u6b63\\\\u78ba\\\\u5ea6\\\\u304c\\\\u308f\\\\u305a\\\\u304b11.5\\\\uff05\\\\u3067\\\\u3042\\\\u308a\\\\u3001\\\\u73fe\\\\u5728\\\\u306e\\\\u6700\\\\u5148\\\\u7aef\\\\u306e\\\\u6b63\\\\u78ba\\\\u5ea6\\\\u3067\\\\u3042\\\\u308b88.4\\\\uff05\\\\u3084\\\\u3001\\\\u53e4\\\\u5178\\\\u7684\\\\u306a\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u30d3\\\\u30b8\\\\u30e7\\\\u30f3\\\\u624b\\\\u6cd5\\\\u306e50\\\\uff05\\\\u3092\\\\u4e0b\\\\u56de\\\\u3063\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3057\\\\u304b\\\\u3057\\\\u3001\\\\u5f31\\\\u3044\\\\u76e3\\\\u7763\\\\u306e\\\\u3088\\\\u308a\\\\u72ed\\\\u3044\\\\u7bc4\\\\u56f2\\\\u3067\\\\u306e\\\\u4f7f\\\\u7528\\\\u306f\\\\u3001\\\\u6027\\\\u80fd\\\\u306e\\\\u5411\\\\u4e0a\\\\u306b\\\\u8ca2\\\\u732e\\\\u3057\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3055\\\\u3089\\\\u306b\\\\u3001Instagram\\\\u306e\\\\u753b\\\\u50cf\\\\u3067ImageNet\\\\u95a2\\\\u9023\\\\u306e\\\\u30cf\\\\u30c3\\\\u30b7\\\\u30e5\\\\u30bf\\\\u30b0\\\\u3092\\\\u4e88\\\\u6e2c\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u52b9\\\\u679c\\\\u7684\\\\u306a\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u30bf\\\\u30b9\\\\u30af\\\\u3067\\\\u3042\\\\u308b\\\\u3053\\\\u3068\\\\u3082\\\\u793a\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3053\\\\u308c\\\\u3089\\\\u306e\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u30e2\\\\u30c7\\\\u30eb\\\\u3092ImageNet\\\\u306b\\\\u30d5\\\\u30a1\\\\u30a4\\\\u30f3\\\\u30c1\\\\u30e5\\\\u30fc\\\\u30cb\\\\u30f3\\\\u30b0\\\\u3059\\\\u308b\\\\u3068\\\\u3001\\\\u6b63\\\\u78ba\\\\u5ea6\\\\u304c5\\\\uff05\\\\u4ee5\\\\u4e0a\\\\u5411\\\\u4e0a\\\\u3057\\\\u3001\\\\u5f53\\\\u6642\\\\u306e\\\\u6700\\\\u5148\\\\u7aef\\\\u306e\\\\u6027\\\\u80fd\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\u305f\\\\u3060\\\\u3057\\\\u3001\\\\u81ea\\\\u7136\\\\u8a00\\\\u8a9e\\\\u76e3\\\\u7763\\\\u30e2\\\\u30c7\\\\u30eb\\\\u306f\\\\u30011000\\\\u30af\\\\u30e9\\\\u30b9\\\\u307e\\\\u305f\\\\u306f18291\\\\u30af\\\\u30e9\\\\u30b9\\\\u306b\\\\u9650\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u76e3\\\\u7763\\\\u3092\\\\u4f7f\\\\u7528\\\\u3057\\\\u3066\\\\u304a\\\\u308a\\\\u3001\\\\u52d5\\\\u7684\\\\u306a\\\\u51fa\\\\u529b\\\\u306e\\\\u30e1\\\\u30ab\\\\u30cb\\\\u30ba\\\\u30e0\\\\u3092\\\\u6b20\\\\u3044\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3053\\\\u308c\\\\u306b\\\\u3088\\\\u308a\\\\u3001\\\\u67d4\\\\u8edf\\\\u6027\\\\u304c\\\\u5236\\\\u9650\\\\u3055\\\\u308c\\\\u3001\\\\\"\\\\u30bc\\\\u30ed\\\\u30b7\\\\u30e7\\\\u30c3\\\\u30c8\\\\\"\\\\u306e\\\\u80fd\\\\u529b\\\\u3082\\\\u5236\\\\u9650\\\\u3055\\\\u308c\\\\u307e\\\\u3059\\\\u3002\\\\u81ea\\\\u7136\\\\u8a00\\\\u8a9e\\\\u306f\\\\u3001\\\\u305d\\\\u306e\\\\u4e00\\\\u822c\\\\u6027\\\\u306b\\\\u3088\\\\u308a\\\\u3001\\\\u8996\\\\u899a\\\\u7684\\\\u306a\\\\u6982\\\\u5ff5\\\\u306e\\\\u3088\\\\u308a\\\\u5e83\\\\u7bc4\\\\u306a\\\\u30bb\\\\u30c3\\\\u30c8\\\\u3092\\\\u8868\\\\u73fe\\\\u3057\\\\u3001\\\\u76e3\\\\u7763\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306f\\\\u3001Alec Radford\\\\u3089\\\\u306b\\\\u3088\\\\u308b\\\\u300cLearning Transferable Visual Models From Natural Language Supervision\\\\u300d\\\\u3068\\\\u3044\\\\u3046\\\\u8ad6\\\\u6587\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3067\\\\u3059\\\\u3002\\\\u3053\\\\u306e\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u81ea\\\\u7136\\\\u8a00\\\\u8a9e\\\\u306e\\\\u76e3\\\\u7763\\\\u5b66\\\\u7fd2\\\\u3092\\\\u7528\\\\u3044\\\\u3066\\\\u5b66\\\\u7fd2\\\\u3055\\\\u308c\\\\u305f\\\\u753b\\\\u50cf\\\\u5206\\\\u985e\\\\u30e2\\\\u30c7\\\\u30eb\\\\u306e\\\\u632f\\\\u308b\\\\u821e\\\\u3044\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u7814\\\\u7a76\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u5927\\\\u898f\\\\u6a21\\\\u306a\\\\u30c7\\\\u30fc\\\\u30bf\\\\u30bb\\\\u30c3\\\\u30c8\\\\u3092\\\\u7528\\\\u3044\\\\u3066\\\\u5b66\\\\u7fd2\\\\u3055\\\\u308c\\\\u305f\\\\u30e2\\\\u30c7\\\\u30eb\\\\u306e\\\\u8ee2\\\\u79fb\\\\u6027\\\\u80fd\\\\u3092\\\\u8a55\\\\u4fa1\\\\u3057\\\\u3001\\\\u305d\\\\u306e\\\\u52b9\\\\u7387\\\\u6027\\\\u3084\\\\u6c4e\\\\u7528\\\\u6027\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u8b70\\\\u8ad6\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u307e\\\\u305f\\\\u3001\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u30e2\\\\u30c7\\\\u30eb\\\\u306e\\\\u5b66\\\\u7fd2\\\\u306b\\\\u304a\\\\u3044\\\\u3066\\\\u5bfe\\\\u7167\\\\u7684\\\\u306a\\\\u8a00\\\\u8a9e-\\\\u753b\\\\u50cf\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u3092\\\\u884c\\\\u3046\\\\u3053\\\\u3068\\\\u3067\\\\u3001\\\\u52b9\\\\u7387\\\\u6027\\\\u304c\\\\u5411\\\\u4e0a\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u793a\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3055\\\\u3089\\\\u306b\\\\u3001\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u30e2\\\\u30c7\\\\u30eb\\\\u306e\\\\u6027\\\\u80fd\\\\u8a55\\\\u4fa1\\\\u3084\\\\u30ed\\\\u30d0\\\\u30b9\\\\u30c8\\\\u6027\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u3082\\\\u8b70\\\\u8ad6\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3053\\\\u306e\\\\u7814\\\\u7a76\\\\u7d50\\\\u679c\\\\u306f\\\\u3001\\\\u653f\\\\u7b56\\\\u3084\\\\u502b\\\\u7406\\\\u306b\\\\u3082\\\\u91cd\\\\u8981\\\\u306a\\\\u5f71\\\\u97ff\\\\u3092\\\\u4e0e\\\\u3048\\\\u308b\\\\u53ef\\\\u80fd\\\\u6027\\\\u304c\\\\u3042\\\\u308a\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=17573 request_id=3295aae2562d35a70ea48e60ad9fbef7 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:このテキストは、自然言語の監督から転移可能な視覚モデルを学習する方法に関する研究についての情報です。過去数年間、生のテキストから直接学習する事前学習手法は、NLP（自然言語処理）の分野で革命を起こしてきました。この研究では、テキストからテキストへの変換が標準化された入出力インターフェースとして使用され、タスク非依存のアーキテクチャがダウンストリームのデータセットにゼロショット転送されることが可能になりました。GPT-3などのフラッグシップシステムは、データセット固有のトレーニングデータを必要とせずに、多くのタスクで特注のモデルと競争力を持つようになりました。しかし、コンピュータビジョンなどの他の分野では、クラウドラベル付きデータセットでモデルを事前学習するのが標準的な手法です。この研究では、ウェブテキストから直接学習するスケーラブルな事前学習手法が、コンピュータビジョンにおいても同様のブレークスルーをもたらす可能性があるのかを検討しています。また、自然言語監督を使用した画像表現学習の性能は他の手法と比較して低いことが示されていますが、弱い監督のより狭い範囲での使用は性能の向上に貢献しています。この研究は、自然言語の監督学習を用いた画像分類モデルの振る舞いや効率性、汎用性について議論しています。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 0: このテキストは、自然言語の監督から転移可能な視覚モデルを学習する方法に関する研究についての情報です。過去数年間、生のテキストから直接学習する事前学習手法は、NLP（自然言語処理）の分野で革命を起こしてきました。この研究では、テキストからテキストへの変換が標準化された入出力インターフェースとして使用され、タスク非依存のアーキテクチャがダウンストリームのデータセットにゼロショット転送されることが可能になりました。GPT-3などのフラッグシップシステムは、データセット固有のトレーニングデータを必要とせずに、多くのタスクで特注のモデルと競争力を持つようになりました。しかし、コンピュータビジョンなどの他の分野では、クラウドラベル付きデータセットでモデルを事前学習するのが標準的な手法です。この研究では、ウェブテキストから直接学習するスケーラブルな事前学習手法が、コンピュータビジョンにおいても同様のブレークスルーをもたらす可能性があるのかを検討しています。また、自然言語監督を使用した画像表現学習の性能は他の手法と比較して低いことが示されていますが、弱い監督のより狭い範囲での使用は性能の向上に貢献しています。この研究は、自然言語の監督学習を用いた画像分類モデルの振る舞いや効率性、汎用性について議論しています。\n",
      "current doc id: 2\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAt the core of our approach is the idea of learning perception from supervision contained in natural language.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAs discussed in the introduction, this is not at all a new idea, however terminology used to describe work in this space is varied, even seemingly contradictory, and stated motivations are diverse.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nZhang et al.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2020), Gomez et al.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2017), Joulin et al.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2016), and Desai & Johnson (2020) all introduce methods which learn visual representations from text paired with images but describe their approaches as unsupervised, self-supervised, weakly supervised, and supervised respectively.We emphasize that what is common across this line of work is not any of the details of the particular methods used but the appreciation of natural language as a training signal.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAll these approaches are learning from natural language super-vision.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAlthough early work wrestled with the complexity of natural language when using topic model and n-gram representations, improvements in deep contextual representation learning suggest we now have the tools to effectively leverage this abundant source of supervision (McCann et al., 2017).Learning from natural language has several potential strengths over other training methods.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIt\\'s much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification since it does not require annotations to be in a classic \\\\\"machine learning compatible format\\\\\" such as the canonical 1-of-N majority vote \\\\\"gold label\\\\\".\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nInstead, methods which work on natural language can learn passively from the supervision contained in the vast amount of text on the internet.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nLearning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn\\'t \\\\\"just\\\\\" learn a representation but also connects that representation to language which enables flexible zero-shot transfer.\\\\n\\\\nSection No.: 2.1.\\\\nSection Title: Natural Language Supervision\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn the following subsections, we detail the specific approach we settled on.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=8600 request_id=928216d21298ed57652d5b8df3062d4b response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストは、\"Learning Transferable Visual Models From Natural Language Supervision\"というタイトルの論文の一部です。この論文では、自然言語に含まれる監督情報から知覚を学習するというアイデアが中心になっています。自然言語を訓練信号として活用することで、画像分類のための標準的な機械学習互換形式の注釈を必要とせずに、大量のテキストから学習することができます。このアプローチは、従来の教師あり、教師なし、自己教師ありの手法とは異なり、柔軟なゼロショット転送を可能にする言語との関連付けも行います。この論文では、具体的なアプローチについて詳細に説明されています。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 2: 提供されたテキストは、\"Learning Transferable Visual Models From Natural Language Supervision\"というタイトルの論文の一部です。この論文では、自然言語に含まれる監督情報から知覚を学習するというアイデアが中心になっています。自然言語を訓練信号として活用することで、画像分類のための標準的な機械学習互換形式の注釈を必要とせずに、大量のテキストから学習することができます。このアプローチは、従来の教師あり、教師なし、自己教師ありの手法とは異なり、柔軟なゼロショット転送を可能にする言語との関連付けも行います。この論文では、具体的なアプローチについて詳細に説明されています。\n",
      "current doc id: 3\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nExisting work has mainly used three datasets, MS-COCO (Lin et al., 2014), Visual Genome (Krishna et al., 2017), and YFCC100M (Thomee et al., 2016).\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhile MS-COCO and Visual Genome are high quality crowd-labeled datasets, they are small by modern standards with approximately 100,000 training photos each.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nBy comparison, other computer vision systems are trained on up to 3.5 billion Instagram photos (Mahajan et al., 2018).\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nYFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nMany images use automatically generated filenames like 20160716 113957.JPG as \\\\\"titles\\\\\" or contain \\\\\"descriptions\\\\\" of camera exposure settings.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAfter filtering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis is approximately the same size as ImageNet.A major motivation for natural language supervision is the large quantities of data of this form available publicly on the internet.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nSince existing datasets do not adequately reflect this possibility, considering results only on them would underestimate the potential of this line of research.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo address this, we constructed a new dataset of 400 million (image, text) pairs collected form a variety of publicly available sources on the Internet.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo attempt to cover as broad a set of visual concepts as possible, we search for (image, text) pairs as part of the construction process whose text includes one of a set of 500,000 queries.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n1 We approximately class 1 The base query list is all words occurring at least 100 times in the English version of Wikipedia.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis is augmented with bi-grams balance the results by including up to 20,000 (image, text) pairs per query.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe resulting dataset has a similar total word count as the WebText dataset used to train GPT-2.\\\\n\\\\nSection No.: 2.2.\\\\nSection Title: Creating a Sufficiently Large Dataset\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe refer to this dataset as WIT for WebImageText.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=28753 request_id=9dbad9d09dff838a0f168221c474ec9a response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:このテキストは、論文「Learning Transferable Visual Models From Natural Language Supervision」のセクション2.2からの抜粋です。このセクションでは、十分に大きなデータセットを作成する方法について説明されています。\n",
      "\n",
      "まず、既存のデータセットとして、MS-COCO、Visual Genome、およびYFCC100Mが紹介されています。これらのデータセットは、それぞれ約10万枚のトレーニング写真を持っていますが、現代の基準では小さくなっています。\n",
      "\n",
      "他のコンピュータビジョンシステムは、最大で35億枚のInstagramの写真で訓練されています。\n",
      "\n",
      "YFCC100Mは1億枚の写真を持っていますが、各画像のメタデータは希薄で品質も異なります。\n",
      "\n",
      "多くの画像は、自動生成されたファイル名（例：20160716 113957.JPG）を「タイトル」として使用したり、カメラの露出設定の「説明」を含んでいます。\n",
      "\n",
      "英語の自然言語のタイトルと/または説明を持つ画像のみを残すようにフィルタリングすると、データセットは6分の1に縮小し、1500万枚の写真になります。\n",
      "\n",
      "このデータセットは、ImageNetとほぼ同じサイズです。自然言語の監督の主な動機は、インターネット上で公開されているこの形式の大量のデータが利用可能であることです。\n",
      "\n",
      "既存のデータセットはこの可能性を十分に反映していないため、それらだけを考慮すると、この研究の潜在能力を過小評価することになります。\n",
      "\n",
      "このため、インターネット上のさまざまな公開ソースから収集された4億組の（画像、テキスト）ペアの新しいデータセットを作成しました。\n",
      "\n",
      "できるだけ広範なビジュアルコンセプトをカバーするために、構築プロセスの一環として、テキストに500,000のクエリのいずれかが含まれる（画像、テキスト）ペアを検索します。\n",
      "\n",
      "このために、英語版Wikipediaで少なくとも100回出現する単語を基本クエリリストとして使用し、バイグラムを追加して、クエリごとに最大で20,000の（画像、テキスト）ペアを含めるようにバランスを取りました。\n",
      "\n",
      "結果として得られたデータセットは、GPT-2のトレーニングに使用されたWebTextデータセットとほぼ同じ単語数を持っています。このデータセットはWebImageTextと呼ばれています。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 3: このテキストは、論文「Learning Transferable Visual Models From Natural Language Supervision」のセクション2.2からの抜粋です。このセクションでは、十分に大きなデータセットを作成する方法について説明されています。\n",
      "\n",
      "まず、既存のデータセットとして、MS-COCO、Visual Genome、およびYFCC100Mが紹介されています。これらのデータセットは、それぞれ約10万枚のトレーニング写真を持っていますが、現代の基準では小さくなっています。\n",
      "\n",
      "他のコンピュータビジョンシステムは、最大で35億枚のInstagramの写真で訓練されています。\n",
      "\n",
      "YFCC100Mは1億枚の写真を持っていますが、各画像のメタデータは希薄で品質も異なります。\n",
      "\n",
      "多くの画像は、自動生成されたファイル名（例：20160716 113957.JPG）を「タイトル」として使用したり、カメラの露出設定の「説明」を含んでいます。\n",
      "\n",
      "英語の自然言語のタイトルと/または説明を持つ画像のみを残すようにフィルタリングすると、データセットは6分の1に縮小し、1500万枚の写真になります。\n",
      "\n",
      "このデータセットは、ImageNetとほぼ同じサイズです。自然言語の監督の主な動機は、インターネット上で公開されているこの形式の大量のデータが利用可能であることです。\n",
      "\n",
      "既存のデータセットはこの可能性を十分に反映していないため、それらだけを考慮すると、この研究の潜在能力を過小評価することになります。\n",
      "\n",
      "このため、インターネット上のさまざまな公開ソースから収集された4億組の（画像、テキスト）ペアの新しいデータセットを作成しました。\n",
      "\n",
      "できるだけ広範なビジュアルコンセプトをカバーするために、構築プロセスの一環として、テキストに500,000のクエリのいずれかが含まれる（画像、テキスト）ペアを検索します。\n",
      "\n",
      "このために、英語版Wikipediaで少なくとも100回出現する単語を基本クエリリストとして使用し、バイグラムを追加して、クエリごとに最大で20,000の（画像、テキスト）ペアを含めるようにバランスを取りました。\n",
      "\n",
      "結果として得られたデータセットは、GPT-2のトレーニングに使用されたWebTextデータセットとほぼ同じ単語数を持っています。このデータセットはWebImageTextと呼ばれています。\n",
      "current doc id: 4\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nState-of-the-art computer vision systems use very large amounts of compute.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nMahajan et al.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2018) required 19 GPU years to train their ResNeXt101-32x48d and Xie et al.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2020) required 33 TPUv3 core-years to train their Noisy Student EfficientNet-L2.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhen considering that both these systems were trained to predict only 1000 ImageNet classes, the task of learning an open set of visual concepts from natural language seems daunting.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn the course of our efforts, we found training efficiency was key to successfully scaling natural language supervision and we selected our final pre-training method based on this metric.Our initial approach, similar to VirTex, jointly trained an image CNN and text transformer from scratch to predict the caption of an image.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nHowever, we encountered difficulties efficiently scaling this method.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn Figure 2 we show that a 63 million parameter transformer language model, which already uses twice the compute of its ResNet-50 image encoder, learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-ofwords encoding of the same text.Both these approaches share a key similarity.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThey try to predict the exact words of the text accompanying each image.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis is a difficult task due to the wide variety of descriptions, comments, and related text that co-occur with images.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nRecent work in contrastive representation learning for images has found that contrastive objectives can learn better representations than their equivalent predictive objective (Tian et al., 2019).\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nOther work has found that although generative models of images can learn high quality image representations, they require over an order of magnitude more compute than contrastive models with the same performance (Chen et al., 2020a).\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nNoting these findings, we explored training a system to solve the potentially easier proxy task of predicting only which text as a whole is paired with which image and not the exact words of that text.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nStarting with the same bag-of-words encoding baseline, we swapped the predictive objective for a contrastive objective in Figure 2 and observed a further 4x efficiency improvement in the rate of zero-shot transfer to ImageNet.Given a batch of N (image, text) pairs, CLIP is trained to predict which of the N \\\\u00d7 N possible (image, text) pairings across a batch actually occurred.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo do this, CLIP learns a with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFinally all WordNet synsets not already in the query list are added.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nmulti-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N 2 -N incorrect pairings.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nLearning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nmulti-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N 2 -N incorrect pairings.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe optimize a symmetric cross entropy loss over these similarity scores.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn Figure 3 we include pseudocode of the core of an implementation of CLIP.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo our knowledge this batch construction technique and objective was first introduced in the area of deep metric learning as the multi-class N-pair loss Sohn (2016), was popularized for contrastive representation learning by Oord et al.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2018) as the InfoNCE loss, and was recently adapted for contrastive (text, image) representation learning in the domain of medical imaging by Zhang et al.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2020).Due to the large size of our pre-training dataset, over-fitting is not a major concern and the details of training CLIP are simplified compared to the implementation of Zhang et al.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2020).\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe train CLIP from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe do not use the non-linear projection between the representation and the contrastive embedding space, a change which was introduced by Bachman et al.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n( 2019) and popularized by Chen et al.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2020b).\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe instead use only a linear projection to map from each encoder\\'s representation to the multi-modal embedding space.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe did not notice a difference in training efficiency between the two versions and speculate that non-linear projections may be co-adapted with details of current image only in self-supervised representation learning methods.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe also remove the text transformation function t u from Zhang et al.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2020) which samples a single sentence at uniform from the text since many of the (image, text) pairs in CLIP\\'s pretraining dataset are only a single sentence.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe also simplify the image transformation function t v .\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nA random square crop from resized images is the only data augmentation used during training.\\\\n\\\\nSection No.: 2.3.\\\\nSection Title: Selecting an Efficient Pre-Training Method\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFinally, the temperature parameter which controls the range of the logits in the softmax, \\\\u03c4 , is directly optimized during training as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=18913 request_id=c4e388c89ef3e7bc2cc80db4cadd14d7 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:このテキストは、論文「Learning Transferable Visual Models From Natural Language Supervision」に関する情報を提供しています。この論文では、画像とテキストの埋め込みを学習するための効率的な事前学習方法について説明されています。\n",
      "\n",
      "具体的には、画像エンコーダとテキストエンコーダを共同でトレーニングし、バッチ内のN個の正しいペアの画像とテキストの埋め込みのコサイン類似度を最大化し、N 2 -N個の誤ったペアの埋め込みのコサイン類似度を最小化することで、マルチモーダルな埋め込み空間を作成します。\n",
      "\n",
      "また、この論文では、対照的な表現学習のための効率的な事前学習方法の選択についても説明されています。具体的には、対照的な損失関数を最適化することによって、類似度スコアを最適化します。\n",
      "\n",
      "さらに、この論文では、CLIPの実装の中核部分の疑似コードも提供されています。\n",
      "\n",
      "この論文では、画像エンコーダをImageNetの重みで初期化せず、テキストエンコーダを事前学習済みの重みで初期化せずに、CLIPをゼロからトレーニングする方法も説明されています。\n",
      "\n",
      "また、この論文では、画像とテキストの変換関数やデータ拡張の詳細についても説明されています。\n",
      "\n",
      "最後に、この論文では、ソフトマックスのロジットの範囲を制御する温度パラメータτが、トレーニング中に直接最適化される方法についても説明されています。\n",
      "\n",
      "以上が、提供されたテキストの要約です。\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=21432 request_id=401ab53299dda0f5e8b0cd1d7c1e90ab response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:このテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文のセクション2.3からの抜粋です。このセクションでは、効率的な事前学習方法の選択について説明されています。\n",
      "\n",
      "まず、最新のコンピュータビジョンシステムは、非常に大量の計算を使用しています。これらのシステムは、画像のキャプションを予測するために画像CNNとテキストトランスフォーマを共同でトレーニングするVirTexという手法に似た初期アプローチを試しましたが、効率的なスケーリングには困難がありました。\n",
      "\n",
      "さらに、63百万パラメータのトランスフォーマ言語モデルは、ResNet-50画像エンコーダの2倍の計算を使用して、同じテキストのbag-of-wordsエンコーディングを予測するよりもImageNetクラスを3倍遅く認識することが示されました。\n",
      "\n",
      "この問題に対処するために、テキストと画像の組み合わせを予測するタスクにシステムをトレーニングすることを試みました。これにより、ImageNetへのゼロショット転送の効率がさらに4倍改善されました。\n",
      "\n",
      "最終的に、CLIPと呼ばれるシステムを開発しました。CLIPは、画像とテキストの組み合わせを予測するために、高い相互情報量を持つ単語や一定の検索ボリュームを持つWikipedia記事の名前などを学習します。\n",
      "\n",
      "このように、このセクションでは、効率的な事前学習方法の選択に関する研究と、画像とテキストの組み合わせを予測するためのCLIPシステムの開発について説明されています。\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\n\\\\u3053\\\\u306e\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306f\\\\u3001Alec Radford\\\\u3089\\\\u306b\\\\u3088\\\\u308b\\\\u300cLearning Transferable Visual Models From Natural Language Supervision\\\\u300d\\\\u3068\\\\u3044\\\\u3046\\\\u30bf\\\\u30a4\\\\u30c8\\\\u30eb\\\\u306e\\\\u8ad6\\\\u6587\\\\u306e\\\\u30bb\\\\u30af\\\\u30b7\\\\u30e7\\\\u30f32.3\\\\u304b\\\\u3089\\\\u306e\\\\u629c\\\\u7c8b\\\\u3067\\\\u3059\\\\u3002\\\\u3053\\\\u306e\\\\u30bb\\\\u30af\\\\u30b7\\\\u30e7\\\\u30f3\\\\u3067\\\\u306f\\\\u3001\\\\u52b9\\\\u7387\\\\u7684\\\\u306a\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u65b9\\\\u6cd5\\\\u306e\\\\u9078\\\\u629e\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u8aac\\\\u660e\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u307e\\\\u305a\\\\u3001\\\\u6700\\\\u65b0\\\\u306e\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u30d3\\\\u30b8\\\\u30e7\\\\u30f3\\\\u30b7\\\\u30b9\\\\u30c6\\\\u30e0\\\\u306f\\\\u3001\\\\u975e\\\\u5e38\\\\u306b\\\\u5927\\\\u91cf\\\\u306e\\\\u8a08\\\\u7b97\\\\u3092\\\\u4f7f\\\\u7528\\\\u3057\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3053\\\\u308c\\\\u3089\\\\u306e\\\\u30b7\\\\u30b9\\\\u30c6\\\\u30e0\\\\u306f\\\\u3001\\\\u753b\\\\u50cf\\\\u306e\\\\u30ad\\\\u30e3\\\\u30d7\\\\u30b7\\\\u30e7\\\\u30f3\\\\u3092\\\\u4e88\\\\u6e2c\\\\u3059\\\\u308b\\\\u305f\\\\u3081\\\\u306b\\\\u753b\\\\u50cfCNN\\\\u3068\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u30c8\\\\u30e9\\\\u30f3\\\\u30b9\\\\u30d5\\\\u30a9\\\\u30fc\\\\u30de\\\\u3092\\\\u5171\\\\u540c\\\\u3067\\\\u30c8\\\\u30ec\\\\u30fc\\\\u30cb\\\\u30f3\\\\u30b0\\\\u3059\\\\u308bVirTex\\\\u3068\\\\u3044\\\\u3046\\\\u624b\\\\u6cd5\\\\u306b\\\\u4f3c\\\\u305f\\\\u521d\\\\u671f\\\\u30a2\\\\u30d7\\\\u30ed\\\\u30fc\\\\u30c1\\\\u3092\\\\u8a66\\\\u3057\\\\u307e\\\\u3057\\\\u305f\\\\u304c\\\\u3001\\\\u52b9\\\\u7387\\\\u7684\\\\u306a\\\\u30b9\\\\u30b1\\\\u30fc\\\\u30ea\\\\u30f3\\\\u30b0\\\\u306b\\\\u306f\\\\u56f0\\\\u96e3\\\\u304c\\\\u3042\\\\u308a\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\n\\\\n\\\\u3055\\\\u3089\\\\u306b\\\\u300163\\\\u767e\\\\u4e07\\\\u30d1\\\\u30e9\\\\u30e1\\\\u30fc\\\\u30bf\\\\u306e\\\\u30c8\\\\u30e9\\\\u30f3\\\\u30b9\\\\u30d5\\\\u30a9\\\\u30fc\\\\u30de\\\\u8a00\\\\u8a9e\\\\u30e2\\\\u30c7\\\\u30eb\\\\u306f\\\\u3001ResNet-50\\\\u753b\\\\u50cf\\\\u30a8\\\\u30f3\\\\u30b3\\\\u30fc\\\\u30c0\\\\u306e2\\\\u500d\\\\u306e\\\\u8a08\\\\u7b97\\\\u3092\\\\u4f7f\\\\u7528\\\\u3057\\\\u3066\\\\u3001\\\\u540c\\\\u3058\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306ebag-of-words\\\\u30a8\\\\u30f3\\\\u30b3\\\\u30fc\\\\u30c7\\\\u30a3\\\\u30f3\\\\u30b0\\\\u3092\\\\u4e88\\\\u6e2c\\\\u3059\\\\u308b\\\\u3088\\\\u308a\\\\u3082ImageNet\\\\u30af\\\\u30e9\\\\u30b9\\\\u30923\\\\u500d\\\\u9045\\\\u304f\\\\u8a8d\\\\u8b58\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u793a\\\\u3055\\\\u308c\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\n\\\\n\\\\u3053\\\\u306e\\\\u554f\\\\u984c\\\\u306b\\\\u5bfe\\\\u51e6\\\\u3059\\\\u308b\\\\u305f\\\\u3081\\\\u306b\\\\u3001\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3068\\\\u753b\\\\u50cf\\\\u306e\\\\u7d44\\\\u307f\\\\u5408\\\\u308f\\\\u305b\\\\u3092\\\\u4e88\\\\u6e2c\\\\u3059\\\\u308b\\\\u30bf\\\\u30b9\\\\u30af\\\\u306b\\\\u30b7\\\\u30b9\\\\u30c6\\\\u30e0\\\\u3092\\\\u30c8\\\\u30ec\\\\u30fc\\\\u30cb\\\\u30f3\\\\u30b0\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u3092\\\\u8a66\\\\u307f\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\u3053\\\\u308c\\\\u306b\\\\u3088\\\\u308a\\\\u3001ImageNet\\\\u3078\\\\u306e\\\\u30bc\\\\u30ed\\\\u30b7\\\\u30e7\\\\u30c3\\\\u30c8\\\\u8ee2\\\\u9001\\\\u306e\\\\u52b9\\\\u7387\\\\u304c\\\\u3055\\\\u3089\\\\u306b4\\\\u500d\\\\u6539\\\\u5584\\\\u3055\\\\u308c\\\\u307e\\\\u3057\\\\u305f\\\\u3002\\\\n\\\\n\\\\u6700\\\\u7d42\\\\u7684\\\\u306b\\\\u3001CLIP\\\\u3068\\\\u547c\\\\u3070\\\\u308c\\\\u308b\\\\u30b7\\\\u30b9\\\\u30c6\\\\u30e0\\\\u3092\\\\u958b\\\\u767a\\\\u3057\\\\u307e\\\\u3057\\\\u305f\\\\u3002CLIP\\\\u306f\\\\u3001\\\\u753b\\\\u50cf\\\\u3068\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u7d44\\\\u307f\\\\u5408\\\\u308f\\\\u305b\\\\u3092\\\\u4e88\\\\u6e2c\\\\u3059\\\\u308b\\\\u305f\\\\u3081\\\\u306b\\\\u3001\\\\u9ad8\\\\u3044\\\\u76f8\\\\u4e92\\\\u60c5\\\\u5831\\\\u91cf\\\\u3092\\\\u6301\\\\u3064\\\\u5358\\\\u8a9e\\\\u3084\\\\u4e00\\\\u5b9a\\\\u306e\\\\u691c\\\\u7d22\\\\u30dc\\\\u30ea\\\\u30e5\\\\u30fc\\\\u30e0\\\\u3092\\\\u6301\\\\u3064Wikipedia\\\\u8a18\\\\u4e8b\\\\u306e\\\\u540d\\\\u524d\\\\u306a\\\\u3069\\\\u3092\\\\u5b66\\\\u7fd2\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u3053\\\\u306e\\\\u3088\\\\u3046\\\\u306b\\\\u3001\\\\u3053\\\\u306e\\\\u30bb\\\\u30af\\\\u30b7\\\\u30e7\\\\u30f3\\\\u3067\\\\u306f\\\\u3001\\\\u52b9\\\\u7387\\\\u7684\\\\u306a\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u65b9\\\\u6cd5\\\\u306e\\\\u9078\\\\u629e\\\\u306b\\\\u95a2\\\\u3059\\\\u308b\\\\u7814\\\\u7a76\\\\u3068\\\\u3001\\\\u753b\\\\u50cf\\\\u3068\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u7d44\\\\u307f\\\\u5408\\\\u308f\\\\u305b\\\\u3092\\\\u4e88\\\\u6e2c\\\\u3059\\\\u308b\\\\u305f\\\\u3081\\\\u306eCLIP\\\\u30b7\\\\u30b9\\\\u30c6\\\\u30e0\\\\u306e\\\\u958b\\\\u767a\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u8aac\\\\u660e\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u3053\\\\u306e\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306f\\\\u3001\\\\u8ad6\\\\u6587\\\\u300cLearning Transferable Visual Models From Natural Language Supervision\\\\u300d\\\\u306b\\\\u95a2\\\\u3059\\\\u308b\\\\u60c5\\\\u5831\\\\u3092\\\\u63d0\\\\u4f9b\\\\u3057\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3053\\\\u306e\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u753b\\\\u50cf\\\\u3068\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u57cb\\\\u3081\\\\u8fbc\\\\u307f\\\\u3092\\\\u5b66\\\\u7fd2\\\\u3059\\\\u308b\\\\u305f\\\\u3081\\\\u306e\\\\u52b9\\\\u7387\\\\u7684\\\\u306a\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u65b9\\\\u6cd5\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u8aac\\\\u660e\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u5177\\\\u4f53\\\\u7684\\\\u306b\\\\u306f\\\\u3001\\\\u753b\\\\u50cf\\\\u30a8\\\\u30f3\\\\u30b3\\\\u30fc\\\\u30c0\\\\u3068\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u30a8\\\\u30f3\\\\u30b3\\\\u30fc\\\\u30c0\\\\u3092\\\\u5171\\\\u540c\\\\u3067\\\\u30c8\\\\u30ec\\\\u30fc\\\\u30cb\\\\u30f3\\\\u30b0\\\\u3057\\\\u3001\\\\u30d0\\\\u30c3\\\\u30c1\\\\u5185\\\\u306eN\\\\u500b\\\\u306e\\\\u6b63\\\\u3057\\\\u3044\\\\u30da\\\\u30a2\\\\u306e\\\\u753b\\\\u50cf\\\\u3068\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u57cb\\\\u3081\\\\u8fbc\\\\u307f\\\\u306e\\\\u30b3\\\\u30b5\\\\u30a4\\\\u30f3\\\\u985e\\\\u4f3c\\\\u5ea6\\\\u3092\\\\u6700\\\\u5927\\\\u5316\\\\u3057\\\\u3001N 2 -N\\\\u500b\\\\u306e\\\\u8aa4\\\\u3063\\\\u305f\\\\u30da\\\\u30a2\\\\u306e\\\\u57cb\\\\u3081\\\\u8fbc\\\\u307f\\\\u306e\\\\u30b3\\\\u30b5\\\\u30a4\\\\u30f3\\\\u985e\\\\u4f3c\\\\u5ea6\\\\u3092\\\\u6700\\\\u5c0f\\\\u5316\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u3067\\\\u3001\\\\u30de\\\\u30eb\\\\u30c1\\\\u30e2\\\\u30fc\\\\u30c0\\\\u30eb\\\\u306a\\\\u57cb\\\\u3081\\\\u8fbc\\\\u307f\\\\u7a7a\\\\u9593\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u307e\\\\u305f\\\\u3001\\\\u3053\\\\u306e\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u5bfe\\\\u7167\\\\u7684\\\\u306a\\\\u8868\\\\u73fe\\\\u5b66\\\\u7fd2\\\\u306e\\\\u305f\\\\u3081\\\\u306e\\\\u52b9\\\\u7387\\\\u7684\\\\u306a\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u65b9\\\\u6cd5\\\\u306e\\\\u9078\\\\u629e\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u3082\\\\u8aac\\\\u660e\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u5177\\\\u4f53\\\\u7684\\\\u306b\\\\u306f\\\\u3001\\\\u5bfe\\\\u7167\\\\u7684\\\\u306a\\\\u640d\\\\u5931\\\\u95a2\\\\u6570\\\\u3092\\\\u6700\\\\u9069\\\\u5316\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u306b\\\\u3088\\\\u3063\\\\u3066\\\\u3001\\\\u985e\\\\u4f3c\\\\u5ea6\\\\u30b9\\\\u30b3\\\\u30a2\\\\u3092\\\\u6700\\\\u9069\\\\u5316\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u3055\\\\u3089\\\\u306b\\\\u3001\\\\u3053\\\\u306e\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001CLIP\\\\u306e\\\\u5b9f\\\\u88c5\\\\u306e\\\\u4e2d\\\\u6838\\\\u90e8\\\\u5206\\\\u306e\\\\u7591\\\\u4f3c\\\\u30b3\\\\u30fc\\\\u30c9\\\\u3082\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u3053\\\\u306e\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u753b\\\\u50cf\\\\u30a8\\\\u30f3\\\\u30b3\\\\u30fc\\\\u30c0\\\\u3092ImageNet\\\\u306e\\\\u91cd\\\\u307f\\\\u3067\\\\u521d\\\\u671f\\\\u5316\\\\u305b\\\\u305a\\\\u3001\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u30a8\\\\u30f3\\\\u30b3\\\\u30fc\\\\u30c0\\\\u3092\\\\u4e8b\\\\u524d\\\\u5b66\\\\u7fd2\\\\u6e08\\\\u307f\\\\u306e\\\\u91cd\\\\u307f\\\\u3067\\\\u521d\\\\u671f\\\\u5316\\\\u305b\\\\u305a\\\\u306b\\\\u3001CLIP\\\\u3092\\\\u30bc\\\\u30ed\\\\u304b\\\\u3089\\\\u30c8\\\\u30ec\\\\u30fc\\\\u30cb\\\\u30f3\\\\u30b0\\\\u3059\\\\u308b\\\\u65b9\\\\u6cd5\\\\u3082\\\\u8aac\\\\u660e\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u307e\\\\u305f\\\\u3001\\\\u3053\\\\u306e\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u753b\\\\u50cf\\\\u3068\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5909\\\\u63db\\\\u95a2\\\\u6570\\\\u3084\\\\u30c7\\\\u30fc\\\\u30bf\\\\u62e1\\\\u5f35\\\\u306e\\\\u8a73\\\\u7d30\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u3082\\\\u8aac\\\\u660e\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u6700\\\\u5f8c\\\\u306b\\\\u3001\\\\u3053\\\\u306e\\\\u8ad6\\\\u6587\\\\u3067\\\\u306f\\\\u3001\\\\u30bd\\\\u30d5\\\\u30c8\\\\u30de\\\\u30c3\\\\u30af\\\\u30b9\\\\u306e\\\\u30ed\\\\u30b8\\\\u30c3\\\\u30c8\\\\u306e\\\\u7bc4\\\\u56f2\\\\u3092\\\\u5236\\\\u5fa1\\\\u3059\\\\u308b\\\\u6e29\\\\u5ea6\\\\u30d1\\\\u30e9\\\\u30e1\\\\u30fc\\\\u30bf\\\\u03c4\\\\u304c\\\\u3001\\\\u30c8\\\\u30ec\\\\u30fc\\\\u30cb\\\\u30f3\\\\u30b0\\\\u4e2d\\\\u306b\\\\u76f4\\\\u63a5\\\\u6700\\\\u9069\\\\u5316\\\\u3055\\\\u308c\\\\u308b\\\\u65b9\\\\u6cd5\\\\u306b\\\\u3064\\\\u3044\\\\u3066\\\\u3082\\\\u8aac\\\\u660e\\\\u3055\\\\u308c\\\\u3066\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\n\\\\n\\\\u4ee5\\\\u4e0a\\\\u304c\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u8981\\\\u7d04\\\\u3067\\\\u3059\\\\u3002\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=16867 request_id=0d490d58df806737740f5d6a5393a7ad response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:このテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」という論文のセクション2.3からの抜粋です。このセクションでは、効率的な事前学習方法の選択について説明されています。\n",
      "\n",
      "まず、最新のコンピュータビジョンシステムは、非常に大量の計算を使用しています。これらのシステムは、画像のキャプションを予測するために画像CNNとテキストトランスフォーマを共同でトレーニングする手法を試しましたが、効率的なスケーリングには困難がありました。\n",
      "\n",
      "さらに、トランスフォーマ言語モデルは、同じテキストのbag-of-wordsエンコーディングを予測するよりもImageNetクラスを3倍遅く認識することが示されました。\n",
      "\n",
      "この問題に対処するために、テキストと画像の組み合わせを予測するタスクにシステムをトレーニングすることを試みました。これにより、ImageNetへのゼロショット転送の効率がさらに4倍改善されました。\n",
      "\n",
      "最終的に、CLIPと呼ばれるシステムを開発しました。CLIPは、画像とテキストの組み合わせを予測するために、高い相互情報量を持つ単語や一定の検索ボリュームを持つWikipedia記事の名前などを学習します。\n",
      "\n",
      "このように、このセクションでは、効率的な事前学習方法の選択に関する研究と、画像とテキストの組み合わせを予測するためのCLIPシステムの開発について説明されています。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 4: このテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」という論文のセクション2.3からの抜粋です。このセクションでは、効率的な事前学習方法の選択について説明されています。\n",
      "\n",
      "まず、最新のコンピュータビジョンシステムは、非常に大量の計算を使用しています。これらのシステムは、画像のキャプションを予測するために画像CNNとテキストトランスフォーマを共同でトレーニングする手法を試しましたが、効率的なスケーリングには困難がありました。\n",
      "\n",
      "さらに、トランスフォーマ言語モデルは、同じテキストのbag-of-wordsエンコーディングを予測するよりもImageNetクラスを3倍遅く認識することが示されました。\n",
      "\n",
      "この問題に対処するために、テキストと画像の組み合わせを予測するタスクにシステムをトレーニングすることを試みました。これにより、ImageNetへのゼロショット転送の効率がさらに4倍改善されました。\n",
      "\n",
      "最終的に、CLIPと呼ばれるシステムを開発しました。CLIPは、画像とテキストの組み合わせを予測するために、高い相互情報量を持つ単語や一定の検索ボリュームを持つWikipedia記事の名前などを学習します。\n",
      "\n",
      "このように、このセクションでは、効率的な事前学習方法の選択に関する研究と、画像とテキストの組み合わせを予測するためのCLIPシステムの開発について説明されています。\n",
      "current doc id: 5\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe consider two different architectures for the image encoder.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor the first, we use ResNet-50 (He et al., 2016a) as the base architecture for the image encoder due to its widespread adoption and proven performance.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe make several modifications to the original version using the ResNet-D improvements from He et al.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2019) and the antialiased rect-2 blur pooling from Zhang (2019).\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe also replace the global average pooling layer with an attention pooling mechanism.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe attention pooling is implemented as a single layer of \\\\\"transformer-style\\\\\" multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al., 2020).\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.The text encoder is a Transformer (Vaswani et al., 2017) with the architecture modifications described in Radford et al.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2019).\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAs a base size we use a 63M-parameter 12layer 512-wide model with 8 attention heads.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sennrich et al., 2015).\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor computational efficiency, the max sequence length was capped at 76.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nMasked self-attention was used in the text encoder to preserve the ability to initialize with a pre-trained language model or add language modeling as an auxiliary objective, though exploration of this is left as future work.While previous computer vision research has often scaled models by increasing the width (Mahajan et al., 2018) or depth (He et al., 2016a) in isolation, for the ResNet image encoders we adapt the approach of Tan & Le (2019) which found that allocating additional compute across all of width, depth, and resolution outperforms only allocating it to only one dimension of the model.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhile Tan & Le (2019) tune the ratio of compute allocated to each dimension for their EfficientNet architecture, we use a simple baseline of allocating additional compute equally to increasing the width, depth, and resolution of the model.\\\\n\\\\nSection No.: 2.4.\\\\nSection Title: Choosing and Scaling a Model\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor the text encoder, we only scale the width of the model to be proportional to the calculated increase in width of the ResNet and do not scale the depth at all, as we found CLIP\\'s performance to be less sensitive to the capacity of the text encoder.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=21314 request_id=10f3e356dcc8f8de1e9a44868fde4096 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストには、\"Learning Transferable Visual Models From Natural Language Supervision\"というタイトルの論文のセクション2.4に関する情報が含まれています。このセクションでは、画像エンコーダの選択とスケーリングについて説明されています。\n",
      "\n",
      "最初のアーキテクチャでは、広く採用されているResNet-50を画像エンコーダのベースアーキテクチャとして使用しています。また、ResNet-Dの改良版やZhangのアンチエイリアシングrect-2ブラープーリングなど、いくつかの変更も行っています。さらに、グローバル平均プーリング層をアテンションプーリングメカニズムに置き換えています。\n",
      "\n",
      "2つ目のアーキテクチャでは、最近導入されたVision Transformer（ViT）を使用して実験しています。彼らは、Dosovitskiyらの実装を細かく追い、トランスフォーマーと組み合わせたパッチと位置の埋め込みに追加のレイヤー正規化を行い、わずかに異なる初期化スキームを使用しています。\n",
      "\n",
      "テキストエンコーダは、トランスフォーマーを使用しており、Radfordらによって説明されたアーキテクチャの変更が行われています。また、テキストの特徴表現として、トランスフォーマーの最上位レイヤーの[EOS]トークンでのアクティベーションが使用されています。\n",
      "\n",
      "これらのモデルのスケーリングに関しては、Tan＆Le（2019）のアプローチを採用しており、幅、深さ、解像度のすべての次元に追加の計算リソースを割り当てることが性能向上に寄与することがわかっています。\n",
      "\n",
      "テキストエンコーダの容量に対するCLIPの性能の感度が低いことから、テキストエンコーダの幅のみを計算された幅の増加に比例させ、深さは全くスケーリングしていないことが示されています。\n",
      "\n",
      "以上が、提供されたテキストの要約です。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 5: 提供されたテキストには、\"Learning Transferable Visual Models From Natural Language Supervision\"というタイトルの論文のセクション2.4に関する情報が含まれています。このセクションでは、画像エンコーダの選択とスケーリングについて説明されています。\n",
      "\n",
      "最初のアーキテクチャでは、広く採用されているResNet-50を画像エンコーダのベースアーキテクチャとして使用しています。また、ResNet-Dの改良版やZhangのアンチエイリアシングrect-2ブラープーリングなど、いくつかの変更も行っています。さらに、グローバル平均プーリング層をアテンションプーリングメカニズムに置き換えています。\n",
      "\n",
      "2つ目のアーキテクチャでは、最近導入されたVision Transformer（ViT）を使用して実験しています。彼らは、Dosovitskiyらの実装を細かく追い、トランスフォーマーと組み合わせたパッチと位置の埋め込みに追加のレイヤー正規化を行い、わずかに異なる初期化スキームを使用しています。\n",
      "\n",
      "テキストエンコーダは、トランスフォーマーを使用しており、Radfordらによって説明されたアーキテクチャの変更が行われています。また、テキストの特徴表現として、トランスフォーマーの最上位レイヤーの[EOS]トークンでのアクティベーションが使用されています。\n",
      "\n",
      "これらのモデルのスケーリングに関しては、Tan＆Le（2019）のアプローチを採用しており、幅、深さ、解像度のすべての次元に追加の計算リソースを割り当てることが性能向上に寄与することがわかっています。\n",
      "\n",
      "テキストエンコーダの容量に対するCLIPの性能の感度が低いことから、テキストエンコーダの幅のみを計算された幅の増加に比例させ、深さは全くスケーリングしていないことが示されています。\n",
      "\n",
      "以上が、提供されたテキストの要約です。\n",
      "current doc id: 6\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe train a series of 5 ResNets and 3 Vision Transformers.For the ResNets we train a ResNet-50, a ResNet-101, and then 3 more which follow EfficientNet-style model scaling and use approximately 4x, 16x, and 64x the compute of a ResNet-50.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThey are denoted as RN50x4, RN50x16, and RN50x64 respectively.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor the Vision Transformers we train a ViT-B/32, a ViT-B/16, and a ViT-L/14.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe train all models for 32 epochs.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe use the Adam optimizer (Kingma & Ba, 2014) with decoupled weight decay regularization (Loshchilov & Hutter, 2017) applied to all weights that are not gains or biases, and decay the learning rate using a cosine schedule (Loshchilov & Hutter, 2016).\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nInitial hyperparameters were set using a combination of grid searches, random search, and manual tuning on the baseline ResNet-50 model when trained for 1 epoch.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nHyper-parameters were then adapted heuristically for larger models due to computational constraints.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe learnable temperature parameter \\\\u03c4 was initialized to the equivalent of 0.07 from (Wu et al., 2018) and clipped to prevent scaling the logits by more than 100 which we found necessary to prevent training instability.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe use a very large minibatch size of 32,768.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nMixed-precision (Micikevicius et al., 2017) was used to accelerate training and save memory.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo save additional memory, gradient checkpointing (Griewank & Walther, 2000;Chen et al., 2016), half-precision Adam statistics (Dhariwal et al., 2020), and half-precision stochastically rounded text encoder weights were used.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe calculation of embedding similarities was also sharded with individual GPUs computing only the subset of the pairwise similarities necessary for their local batch of embeddings.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor the ViT-L/14 we also pre-train at a higher 336 pixel resolution for one additional epoch to boost performance similar to FixRes (Touvron et al., 2019).\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe denote this model as ViT-L/14@336px.\\\\n\\\\nSection No.: 2.5.\\\\nSection Title: Training\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nUnless otherwise specified, all results reported in this paper as \\\\\"CLIP\\\\\" use this model which we found to perform best.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=25286 request_id=116a02e21c2503c0fe1f3cbea1d7958c response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストには、以下のポイントが含まれています。\n",
      "\n",
      "1. 5つのResNetと3つのVision Transformerをトレーニングしました。\n",
      "2. ResNetには、ResNet-50、ResNet-101、およびEfficientNetスタイルのモデルスケーリングに従った3つのモデルがあります。\n",
      "3. Vision Transformerには、ViT-B/32、ViT-B/16、およびViT-L/14があります。\n",
      "4. すべてのモデルを32エポックでトレーニングしました。\n",
      "5. Adamオプティマイザを使用し、重み減衰正則化を適用し、学習率をコサインスケジュールで減衰させました。\n",
      "6. 初期ハイパーパラメータは、1エポックでトレーニングしたベースラインのResNet-50モデルを使用して設定されました。\n",
      "7. 大きなモデルのためにハイパーパラメータをヒューリスティックに適応させました。\n",
      "8. 学習可能な温度パラメータτは、0.07から初期化され、100を超えるスケーリングを防ぐためにクリップされました。\n",
      "9. 非常に大きなミニバッチサイズ（32,768）を使用しました。\n",
      "10. 混合精度（Mixed-precision）を使用してトレーニングを高速化し、メモリを節約しました。\n",
      "11. 追加のメモリを節約するために、勾配チェックポイント、半精度のAdam統計、および半精度の確率的に丸められたテキストエンコーダの重みを使用しました。\n",
      "12. 埋め込みの類似性の計算も、各GPUがローカルバッチの埋め込みのサブセットのみを計算するようにシャーディングされました。\n",
      "13. 最大のResNetモデル（RN50x64）は592のV100 GPUで18日間、最大のVision Transformerは256のV100 GPUで12日間かかりました。\n",
      "14. ViT-L/14には、FixRes（Touvron et al., 2019）と同様のパフォーマンス向上のために、336ピクセルの解像度で1つの追加エポックで事前トレーニングを行いました。\n",
      "15. \"CLIP\"として報告される結果は、このモデルが最も優れていることがわかりました。\n",
      "\n",
      "以上が、提供されたテキストの要約です。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 6: 提供されたテキストには、以下のポイントが含まれています。\n",
      "\n",
      "1. 5つのResNetと3つのVision Transformerをトレーニングしました。\n",
      "2. ResNetには、ResNet-50、ResNet-101、およびEfficientNetスタイルのモデルスケーリングに従った3つのモデルがあります。\n",
      "3. Vision Transformerには、ViT-B/32、ViT-B/16、およびViT-L/14があります。\n",
      "4. すべてのモデルを32エポックでトレーニングしました。\n",
      "5. Adamオプティマイザを使用し、重み減衰正則化を適用し、学習率をコサインスケジュールで減衰させました。\n",
      "6. 初期ハイパーパラメータは、1エポックでトレーニングしたベースラインのResNet-50モデルを使用して設定されました。\n",
      "7. 大きなモデルのためにハイパーパラメータをヒューリスティックに適応させました。\n",
      "8. 学習可能な温度パラメータτは、0.07から初期化され、100を超えるスケーリングを防ぐためにクリップされました。\n",
      "9. 非常に大きなミニバッチサイズ（32,768）を使用しました。\n",
      "10. 混合精度（Mixed-precision）を使用してトレーニングを高速化し、メモリを節約しました。\n",
      "11. 追加のメモリを節約するために、勾配チェックポイント、半精度のAdam統計、および半精度の確率的に丸められたテキストエンコーダの重みを使用しました。\n",
      "12. 埋め込みの類似性の計算も、各GPUがローカルバッチの埋め込みのサブセットのみを計算するようにシャーディングされました。\n",
      "13. 最大のResNetモデル（RN50x64）は592のV100 GPUで18日間、最大のVision Transformerは256のV100 GPUで12日間かかりました。\n",
      "14. ViT-L/14には、FixRes（Touvron et al., 2019）と同様のパフォーマンス向上のために、336ピクセルの解像度で1つの追加エポックで事前トレーニングを行いました。\n",
      "15. \"CLIP\"として報告される結果は、このモデルが最も優れていることがわかりました。\n",
      "\n",
      "以上が、提供されたテキストの要約です。\n",
      "current doc id: 7\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 3.\\\\nSection Title: Experiments\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n3.1.\\\\n\\\\nSection No.: 3.\\\\nSection Title: Experiments\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nZero-Shot Transfer\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=23626 request_id=3067cd7b1dac9433f180faa212b19fca response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストは、論文「Learning Transferable Visual Models From Natural Language Supervision」のセクション3の一部です。このセクションでは、実験について説明されています。具体的には、ゼロショット転送に焦点を当てています。\n",
      "\n",
      "ゼロショット転送は、自然言語の教示から視覚モデルを学習する手法です。この手法では、画像と自然言語のペアを使用して、視覚モデルを訓練します。訓練されたモデルは、新しいタスクやドメインにおいて、事前に学習された知識を転送することができます。つまり、訓練データに含まれていない新しいタスクに対しても高い性能を発揮することが期待されます。\n",
      "\n",
      "この手法は、コンピュータビジョンの分野において特に有用です。例えば、画像分類や物体検出などのタスクにおいて、ゼロショット転送を使用することで、新しいクラスや新しい環境においても高い精度を実現することができます。\n",
      "\n",
      "この論文では、ゼロショット転送の有効性を実験によって評価しています。具体的には、自然言語の教示を使用して訓練された視覚モデルが、新しいタスクやドメインにおいてどのように性能を発揮するかを調査しています。実験結果は、ゼロショット転送が有望な手法であることを示しています。\n",
      "\n",
      "この情報を要約すると、このテキストは「Learning Transferable Visual Models From Natural Language Supervision」という論文のセクション3で、ゼロショット転送についての実験結果が述べられています。ゼロショット転送は、自然言語の教示から視覚モデルを学習し、新しいタスクやドメインにおいても高い性能を発揮する手法です。この手法の有効性は、実験によって評価されています。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 7: 提供されたテキストは、論文「Learning Transferable Visual Models From Natural Language Supervision」のセクション3の一部です。このセクションでは、実験について説明されています。具体的には、ゼロショット転送に焦点を当てています。\n",
      "\n",
      "ゼロショット転送は、自然言語の教示から視覚モデルを学習する手法です。この手法では、画像と自然言語のペアを使用して、視覚モデルを訓練します。訓練されたモデルは、新しいタスクやドメインにおいて、事前に学習された知識を転送することができます。つまり、訓練データに含まれていない新しいタスクに対しても高い性能を発揮することが期待されます。\n",
      "\n",
      "この手法は、コンピュータビジョンの分野において特に有用です。例えば、画像分類や物体検出などのタスクにおいて、ゼロショット転送を使用することで、新しいクラスや新しい環境においても高い精度を実現することができます。\n",
      "\n",
      "この論文では、ゼロショット転送の有効性を実験によって評価しています。具体的には、自然言語の教示を使用して訓練された視覚モデルが、新しいタスクやドメインにおいてどのように性能を発揮するかを調査しています。実験結果は、ゼロショット転送が有望な手法であることを示しています。\n",
      "\n",
      "この情報を要約すると、このテキストは「Learning Transferable Visual Models From Natural Language Supervision」という論文のセクション3で、ゼロショット転送についての実験結果が述べられています。ゼロショット転送は、自然言語の教示から視覚モデルを学習し、新しいタスクやドメインにおいても高い性能を発揮する手法です。この手法の有効性は、実験によって評価されています。\n",
      "current doc id: 8\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn computer vision, zero-shot learning usually refers to the study of generalizing to unseen object categories in image classification (Lampert et al., 2009).\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe instead use the term in a broader sense and study generalization to unseen datasets.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe motivate this as a proxy for performing unseen tasks, as aspired to in the zero-data learning paper of Larochelle et al.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2008).\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhile much research in the field of unsupervised learning focuses on the representation learning capabilities of machine learning systems, we motivate studying zero-shot transfer as a way of measuring the tasklearning capabilities of machine learning systems.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn this view, a dataset evaluates performance on a task on a specific distribution.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nHowever, many popular computer vision datasets were created by the research community primarily as benchmarks to guide the development of generic image classification methods rather than measuring performance on a specific task.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhile it is reasonable to say that the SVHN dataset measures the task of street number transcription on the distribution of Google Street View photos, it is unclear what \\\\\"real\\\\\" task the CIFAR-10 dataset measures.It is clear, however, what distribution CIFAR-10 is drawn from -TinyImages (Torralba et al., 2008).\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nOn these kinds of datasets, zero-shot transfer is more an evaluation of CLIP\\'s robustness to distribution shift and domain generalization rather than task generalization.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nPlease see Section 3.3 for analysis focused on this.To our knowledge, Visual N-Grams (Li et al., 2017) first studied zero-shot transfer to existing image classification datasets in the manner described above.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIt is also the only other work we are aware of that has studied zero-shot transfer to standard image classification datasets using a generically pre-trained model and serves as the best reference point for contextualizing CLIP.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTheir approach learns the parameters of a dictionary of 142,806 visual n-grams (spanning 1-to 5-grams) and optimizes these n-grams using a differential version of Jelinek-Mercer smoothing to maximize the probability of all text n-grams for a given image.In order to perform zero-shot transfer, they first convert the text of each of the dataset\\'s class names into its n-gram representation and then compute its probability according to their model, predicting the one with the highest score.Our focus on studying zero-shot transfer as an evaluation of task learning is inspired by work demonstrating task learning in the field of NLP.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo our knowledge Liu et al.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2018) first identified task learning as an \\\\\"unexpected side-effect\\\\\" when a language model trained to generate Wikipedia articles learned to reliably transliterate names between languages.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhile GPT-1 (Radford et al., 2018) focused on pre-training as a transfer learning method to improve supervised fine-tuning, it also included an ablation study demonstrating that the performance of four heuristic zero-shot transfer methods improved steadily over the course of pre-training, without any supervised adaption.\\\\n\\\\nSection No.: 3.1.1.\\\\nSection Title: MOTIVATION\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis analysis served as the basis for GPT-2 (Radford et al., 2019) which focused exclusively on studying the task-learning capabilities of language models via zero-shot transfer.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=18406 request_id=05a0a0869e62190bb477815195055c99 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストは、\"Learning Transferable Visual Models From Natural Language Supervision\"というタイトルの論文の一部です。この論文では、ゼロショット学習という概念について説明されています。ゼロショット学習は、画像分類の未知のオブジェクトカテゴリに対して一般化することを指します。この論文では、ゼロショット学習をより広い意味で捉え、未知のデータセットに対する一般化を研究しています。また、ゼロショット学習は、機械学習システムのタスク学習能力を測定する手段として位置付けられています。さらに、一部のコンピュータビジョンのデータセットは、特定のタスクのパフォーマンスを測定するためではなく、一般的な画像分類手法の開発をガイドするために作成されたことが指摘されています。この論文では、ゼロショット学習の評価に焦点を当てた分析も行われています。この論文は、ゼロショット学習に関する先行研究や関連研究を参照しており、その中でも特にVisual N-Gramsという手法が言及されています。Visual N-Gramsは、画像分類データセットへのゼロショット転送を研究した最初の手法であり、CLIPというモデルの文脈化の最適な参照点とされています。この論文では、NLPの分野でのタスク学習の実証を示す研究に触発されて、ゼロショット転送の研究に焦点を当てています。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 8: 提供されたテキストは、\"Learning Transferable Visual Models From Natural Language Supervision\"というタイトルの論文の一部です。この論文では、ゼロショット学習という概念について説明されています。ゼロショット学習は、画像分類の未知のオブジェクトカテゴリに対して一般化することを指します。この論文では、ゼロショット学習をより広い意味で捉え、未知のデータセットに対する一般化を研究しています。また、ゼロショット学習は、機械学習システムのタスク学習能力を測定する手段として位置付けられています。さらに、一部のコンピュータビジョンのデータセットは、特定のタスクのパフォーマンスを測定するためではなく、一般的な画像分類手法の開発をガイドするために作成されたことが指摘されています。この論文では、ゼロショット学習の評価に焦点を当てた分析も行われています。この論文は、ゼロショット学習に関する先行研究や関連研究を参照しており、その中でも特にVisual N-Gramsという手法が言及されています。Visual N-Gramsは、画像分類データセットへのゼロショット転送を研究した最初の手法であり、CLIPというモデルの文脈化の最適な参照点とされています。この論文では、NLPの分野でのタスク学習の実証を示す研究に触発されて、ゼロショット転送の研究に焦点を当てています。\n",
      "current doc id: 9\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nCLIP is pre-trained to predict if an image and a text snippet are paired together in its dataset.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo perform zero-shot classification, we reuse this capability.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor each dataset, we use the names of all the classes in the dataset as the set of potential text pairings and predict the most probable (image, text) pair according to CLIP.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn a bit more detail, we first compute the feature embedding of the image and the feature embedding of the set of possible texts by their respective encoders.The cosine similarity of these embeddings is then calculated, scaled by a temperature parameter \\\\u03c4 , and normalized into a probability distribution via a softmax.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nNote that this prediction layer is a multinomial logistic regression classifier with L2-normalized inputs, L2-normalized weights, no bias, and temperature scaling.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhen interpreted this way, the image encoder is the computer vision backbone which computes a feature representation for the image and the text encoder is a hypernetwork (Ha et al., 2016) which generates the weights of a linear classifier based on the text specifying the visual concepts that the classes represent.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nLei Ba et al.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2015) first introduced a zero-shot image classifier of this form while the idea of generating a classifier from natural language dates back to at least Elhoseiny et al.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2013).\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nContinuing with this interpretation, every step of CLIP pre-training can be viewed as optimizing the performance of a randomly created proxy to a computer vision dataset which contains 1 example per class and has 32,768 total classes defined via natural language descriptions.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor zero-shot evaluation, we cache the zero-shot classifier once it has been computed by the text encoder and reuse it for all subsequent predictions.\\\\n\\\\nSection No.: 3.1.2.\\\\nSection Title: USING CLIP FOR ZERO-SHOT TRANSFER\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis allows the cost of generating it to be amortized across all the predictions in a dataset.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=22166 request_id=d83343ea9ce2aa3d7672d8228434dc58 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストには、以下のポイントが含まれています。\n",
      "\n",
      "1. CLIPは、画像とテキストの組み合わせを予測するために事前学習されています。\n",
      "2. ゼロショット分類を行うために、この能力を再利用します。\n",
      "3. 各データセットについて、データセット内のすべてのクラスの名前を潜在的なテキストの組み合わせとして使用し、CLIPによって最も確率の高い（画像、テキスト）の組み合わせを予測します。\n",
      "4. 画像と可能なテキストの特徴埋め込みを計算し、それらの埋め込みのコサイン類似度を計算し、温度パラメータτでスケーリングし、softmaxを使用して確率分布に正規化します。\n",
      "5. 予測層は、L2正規化された入力、L2正規化された重み、バイアスのない多項ロジスティック回帰分類器であり、温度スケーリングも行われています。\n",
      "6. 画像エンコーダは、画像の特徴表現を計算するコンピュータビジョンのバックボーンであり、テキストエンコーダはテキストに基づいて線形分類器の重みを生成するハイパーネットワークです。\n",
      "7. ゼロショットイメージ分類器のアイデアは、Baら（2015）によって最初に紹介されましたが、自然言語から分類器を生成するアイデアは、少なくともElhoseinyらにさかのぼります（2013）。\n",
      "8. CLIPの事前学習の各ステップは、自然言語の説明によって定義された32,768のクラスを持つコンピュータビジョンデータセットのランダムに作成されたプロキシのパフォーマンスを最適化するものと見なすことができます。\n",
      "9. ゼロショット評価では、テキストエンコーダによって計算されたゼロショット分類器をキャッシュし、その後のすべての予測に再利用します。\n",
      "10. これにより、データセット内のすべての予測にかかる生成コストを分散することができます。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 9: 提供されたテキストには、以下のポイントが含まれています。\n",
      "\n",
      "1. CLIPは、画像とテキストの組み合わせを予測するために事前学習されています。\n",
      "2. ゼロショット分類を行うために、この能力を再利用します。\n",
      "3. 各データセットについて、データセット内のすべてのクラスの名前を潜在的なテキストの組み合わせとして使用し、CLIPによって最も確率の高い（画像、テキスト）の組み合わせを予測します。\n",
      "4. 画像と可能なテキストの特徴埋め込みを計算し、それらの埋め込みのコサイン類似度を計算し、温度パラメータτでスケーリングし、softmaxを使用して確率分布に正規化します。\n",
      "5. 予測層は、L2正規化された入力、L2正規化された重み、バイアスのない多項ロジスティック回帰分類器であり、温度スケーリングも行われています。\n",
      "6. 画像エンコーダは、画像の特徴表現を計算するコンピュータビジョンのバックボーンであり、テキストエンコーダはテキストに基づいて線形分類器の重みを生成するハイパーネットワークです。\n",
      "7. ゼロショットイメージ分類器のアイデアは、Baら（2015）によって最初に紹介されましたが、自然言語から分類器を生成するアイデアは、少なくともElhoseinyらにさかのぼります（2013）。\n",
      "8. CLIPの事前学習の各ステップは、自然言語の説明によって定義された32,768のクラスを持つコンピュータビジョンデータセットのランダムに作成されたプロキシのパフォーマンスを最適化するものと見なすことができます。\n",
      "9. ゼロショット評価では、テキストエンコーダによって計算されたゼロショット分類器をキャッシュし、その後のすべての予測に再利用します。\n",
      "10. これにより、データセット内のすべての予測にかかる生成コストを分散することができます。\n",
      "current doc id: 10\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn .\\\\n\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nCLIP improves performance on all three datasets by a large amount.\\\\n\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis improvement reflects many differences in the 4 years since the development of Visual N-Grams (Li et al., 2017).CLIP is a significant step towards flexible and practical zero-shot computer vision classifiers.\\\\n\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAs mentioned above, the comparison to Visual N-Grams is meant for contextualizing the performance of CLIP and should not be interpreted as a direct methods comparison between CLIP and Visual N-Grams as many performance relevant differences between the two systems were not controlled for.\\\\n\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor instance, we train on a dataset that is 10x larger, use a vision model that requires nearly 100x more compute per prediction, likely used over 1000x their training compute, and use a transformer-based model which did not exist when Visual N-Grams was published.\\\\n\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAs a closer comparison, we trained a CLIP ResNet-50 on the same YFCC100M dataset that Visual N-Grams was trained on and found it matched their reported ImageNet performance within a V100 GPU day.\\\\n\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis baseline was also trained from scratch instead of being initialized from pre-trained ImageNet weights as in Visual N-Grams.CLIP also outperforms Visual N-Grams on the other 2 reported datasets.\\\\n\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nOn aYahoo, CLIP achieves a 95% reduction in the number of errors, and on SUN, CLIP more than doubles the accuracy of Visual N-Grams.\\\\n\\\\nSection No.: 3.1.3.\\\\nSection Title: INITIAL COMPARISON TO VISUAL N-GRAMS\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo conduct a more comprehensive analysis and stress test, we implement a much larger evaluation suite detailed in Appendix A. In total we expand from the 3 datasets reported in Visual N-Grams to include over 30 datasets and compare to over 50 existing computer vision systems to contextualize results.\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=15023 request_id=41797894fb8111fd5c1e232ec7311faf response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:提供されたテキストには、以下のポイントが含まれています。\n",
      "\n",
      "1. CLIPは、自然言語の監督から転移可能な視覚モデルを学習するための手法です。\n",
      "2. CLIPは、Visual N-Gramsと比較して性能が大幅に向上しています。\n",
      "3. CLIPは、柔軟で実用的なゼロショットのコンピュータビジョン分類器への重要な進歩です。\n",
      "4. CLIPは、Visual N-Gramsとの比較は、CLIPの性能を文脈化するためのものであり、直接的な方法の比較ではありません。\n",
      "5. CLIPは、Visual N-Gramsよりも大規模なデータセットでトレーニングされ、より高い計算量を必要とするビジョンモデルを使用しています。\n",
      "6. CLIPは、Visual N-Gramsよりも優れたパフォーマンスを発揮しています。\n",
      "7. CLIPは、Yahooデータセットではエラー数を95％削減し、SUNデータセットではVisual N-Gramsの精度を2倍以上に向上させています。\n",
      "8. CLIPは、Visual N-Gramsと比較して他の2つのデータセットでも優れた結果を示しています。\n",
      "9. CLIPの評価は、Visual N-Gramsの報告された3つのデータセットから30以上のデータセットに拡張され、50以上の既存のコンピュータビジョンシステムと比較されています。\n",
      "\n",
      "以上が、提供されたテキストの要約です。\n",
      "INFO:llama_index.indices.document_summary.base:> Generated summary for doc 10: 提供されたテキストには、以下のポイントが含まれています。\n",
      "\n",
      "1. CLIPは、自然言語の監督から転移可能な視覚モデルを学習するための手法です。\n",
      "2. CLIPは、Visual N-Gramsと比較して性能が大幅に向上しています。\n",
      "3. CLIPは、柔軟で実用的なゼロショットのコンピュータビジョン分類器への重要な進歩です。\n",
      "4. CLIPは、Visual N-Gramsとの比較は、CLIPの性能を文脈化するためのものであり、直接的な方法の比較ではありません。\n",
      "5. CLIPは、Visual N-Gramsよりも大規模なデータセットでトレーニングされ、より高い計算量を必要とするビジョンモデルを使用しています。\n",
      "6. CLIPは、Visual N-Gramsよりも優れたパフォーマンスを発揮しています。\n",
      "7. CLIPは、Yahooデータセットではエラー数を95％削減し、SUNデータセットではVisual N-Gramsの精度を2倍以上に向上させています。\n",
      "8. CLIPは、Visual N-Gramsと比較して他の2つのデータセットでも優れた結果を示しています。\n",
      "9. CLIPの評価は、Visual N-Gramsの報告された3つのデータセットから30以上のデータセットに拡張され、50以上の既存のコンピュータビジョンシステムと比較されています。\n",
      "\n",
      "以上が、提供されたテキストの要約です。\n",
      "current doc id: 11\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nMost standard image classification datasets treat the information naming or describing classes which enables natural language based zero-shot transfer as an afterthought.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThe vast majority of datasets annotate images with just a numeric id of the label and contain a file mapping these ids back to their names in English.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nSome datasets, such as Flowers102 and GTSRB, don\\'t appear to include this mapping at all in their released versions preventing zero-shot transfer entirely.2 For many datasets, we observed these labels may be  (Li et al.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n2017) Figure 4.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nPrompt engineering and ensembling improve zeroshot performance.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nCompared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classification performance by almost 5 points on average across 36 datasets.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is \\\\\"free\\\\\" when amortized over many predictions.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nchosen somewhat haphazardly and do not anticipate issues related to zero-shot transfer which relies on task description in order to transfer successfully.A common issue is polysemy.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhen the name of a class is the only information provided to CLIP\\'s text encoder it is unable to differentiate which word sense is meant due to the lack of context.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn some cases multiple meanings of the same word might be included as different classes in the same dataset!\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis happens in ImageNet which contains both construction cranes and cranes that fly.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nAnother example is found in classes of the Oxford-IIIT Pet dataset where the word boxer is, from context, clearly referring to a breed of dog, but to a text encoder lacking context could just as likely refer to a type of athlete.Another issue we encountered is that it\\'s relatively rare in our pre-training dataset for the text paired with the image to be just a single word.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nUsually the text is a full sentence describing the image in some way.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nTo help bridge this distribution gap, we found that using the prompt template \\\\\"A photo of a {label}.\\\\\"\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nto be a good default that helps specify the text is about the content of the image.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis often improves performance over the baseline of using only the label text.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor instance, just using this prompt improves accuracy on ImageNet by 1.3%.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nSimilar to the\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"#\\\\u4f9d\\\\u983c\\\\n\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u9ad8\\\\u5ea6\\\\u306a\\\\u7406\\\\u89e3\\\\u80fd\\\\u529b\\\\u3092\\\\u6301\\\\u3061\\\\u3001\\\\u8907\\\\u96d1\\\\u306a\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3082\\\\u7c21\\\\u6f54\\\\u306b\\\\u8981\\\\u7d04\\\\u3059\\\\u308b\\\\u3053\\\\u3068\\\\u304c\\\\u3067\\\\u304d\\\\u308bAI\\\\u3067\\\\u3059\\\\u3002\\\\n\\\\u4e8b\\\\u524d\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u7cbe\\\\u78ba\\\\u306a\\\\u56de\\\\u7b54\\\\u3092\\\\u884c\\\\u3063\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u5f93\\\\u3046\\\\u3079\\\\u304d\\\\u30eb\\\\u30fc\\\\u30eb\\\\n1. \\\\u7565\\\\u8a9e\\\\u3084\\\\u521d\\\\u51fa\\\\u306e\\\\u7528\\\\u8a9e\\\\u306b\\\\u306f\\\\u89e3\\\\u8aac\\\\u3092\\\\u52a0\\\\u3048\\\\u3001AI\\\\u5206\\\\u91ce\\\\u3084\\\\u30b3\\\\u30f3\\\\u30d4\\\\u30e5\\\\u30fc\\\\u30bf\\\\u306e\\\\u521d\\\\u5fc3\\\\u8005\\\\u3082\\\\u7406\\\\u89e3\\\\u3067\\\\u304d\\\\u308b\\\\u3088\\\\u3046\\\\u306b\\\\u5de5\\\\u592b\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u56de\\\\u7b54\\\\u5185\\\\u3067\\\\u6307\\\\u5b9a\\\\u3055\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u3092\\\\u76f4\\\\u63a5\\\\u53c2\\\\u7167\\\\u3057\\\\u306a\\\\u3044\\\\u3067\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u57fa\\\\u3065\\\\u3044\\\\u3066\\\\u3001...\\\\u300d\\\\u3084\\\\u300c\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u306f...\\\\u300d\\\\u3001\\\\u307e\\\\u305f\\\\u306f\\\\u305d\\\\u308c\\\\u306b\\\\u985e\\\\u3059\\\\u308b\\\\u3088\\\\u3046\\\\u306a\\\\u8a18\\\\u8ff0\\\\u306f\\\\u907f\\\\u3051\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n#\\\\u624b\\\\u9806\\\\n1. \\\\u4e0e\\\\u3048\\\\u3089\\\\u308c\\\\u305f\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306b\\\\u542b\\\\u307e\\\\u308c\\\\u308b\\\\u4e3b\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u3092\\\\u7d30\\\\u304b\\\\u304f\\\\u5206\\\\u89e3\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n2. \\\\u305d\\\\u308c\\\\u305e\\\\u308c\\\\u306e\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3084\\\\u30b3\\\\u30f3\\\\u30bb\\\\u30d7\\\\u30c8\\\\u306b\\\\u5bfe\\\\u3057\\\\u3066\\\\u8a73\\\\u7d30\\\\u306a\\\\u8aac\\\\u660e\\\\u3092\\\\u52a0\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n3. \\\\u307e\\\\u305a\\\\u306f\\\\u6307\\\\u793a\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u306e\\\\u521d\\\\u7248\\\\u3092\\\\u4f5c\\\\u6210\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n4. \\\\u4f5c\\\\u6210\\\\u3057\\\\u305f\\\\u521d\\\\u7248\\\\u3092\\\\u30eb\\\\u30fc\\\\u30eb\\\\u306b\\\\u5f93\\\\u3063\\\\u3066\\\\u3044\\\\u308b\\\\u304b\\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n5. \\\\u81ea\\\\u5df1\\\\u5206\\\\u6790\\\\u306e\\\\u7d50\\\\u679c\\\\u3092\\\\u8e0f\\\\u307e\\\\u3048\\\\u3066\\\\u3001\\\\u6587\\\\u66f8\\\\u3092\\\\u6539\\\\u5584\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\"}, {\"role\": \"user\", \"content\": \"\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u30b3\\\\u30f3\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u60c5\\\\u5831\\\\u3092\\\\u4ee5\\\\u4e0b\\\\u306b\\\\u793a\\\\u3057\\\\u307e\\\\u3059\\\\u3002\\\\n---------------------\\\\nNo.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor instance, just using this prompt improves accuracy on ImageNet by 1.3%.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nSimilar to the \\\\\"prompt engineering\\\\\" discussion around GPT-3 (Brown et al., 2020;Gao et al., 2020), we have also observed that zero-shot performance can be significantly improved by customizing the prompt text to each task.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nA few, non exhaustive, examples follow.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe found on several fine-grained image classification datasets that it helped to specify the category.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor example on Oxford-IIIT Pets, using \\\\\"A photo of a {label}, a type of pet.\\\\\"\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nto help provide context worked well.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nLikewise, on Food101 specifying a type of food and on FGVC Aircraft a type of aircraft helped too.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFor OCR datasets, we found that putting quotes around the text or number to be recognized improved performance.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nFinally, we found that on satellite image classification datasets it helped to specify that the images were of this form and we use variants of \\\\\"a satellite photo of a {label}.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n\\\\\".We also experimented with ensembling over multiple zeroshot classifiers as another way of improving performance.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThese classifiers are computed by using different context prompts such as \\'A photo of a big {label}\\\\\" and \\\\\"A photo of a small {label}\\\\\".\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe construct the ensemble over the embedding space instead of probability space.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nThis allows us to cache a single set of averaged text embeddings so that the compute cost of the ensemble is the same as using a single classifier when amortized over many predictions.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWe\\'ve observed ensembling across many generated zero-shot classifiers to reliably improve performance and use it for the majority of datasets.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nOn ImageNet, we ensemble 80 different context prompts and this improves performance by an additional 3.5% over the single default prompt discussed above.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nWhen considered together, prompt engineering and ensembling improve ImageNet accuracy by almost 5%.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\nIn Figure 4 we visualize how prompt engineering and ensembling change the performance of a set of CLIP models compared to the contextless baseline approach of directly embedding the class name as done in Li et al.\\\\n\\\\nSection No.: 3.1.4.\\\\nSection Title: PROMPT ENGINEERING AND ENSEMBLING\\\\nTitle: Learning Transferable Visual Models From Natural Language Supervision\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\nIdno: arXiv:2103.00020v1[cs.CV]\\\\nPublished: 26 Feb 2021\\\\nLanguage: en\\\\n\\\\n(2017).\\\\n---------------------\\\\n\\\\u4e88\\\\u5099\\\\u77e5\\\\u8b58\\\\u3067\\\\u306f\\\\u306a\\\\u304f\\\\u3001\\\\u8907\\\\u6570\\\\u306e\\\\u30bd\\\\u30fc\\\\u30b9\\\\u304b\\\\u3089\\\\u306e\\\\u60c5\\\\u5831\\\\u3092\\\\u8003\\\\u616e\\\\u3057\\\\u3066\\\\u8cea\\\\u554f\\\\u306b\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n\\\\u7591\\\\u554f\\\\u304c\\\\u3042\\\\u308b\\\\u5834\\\\u5408\\\\u306f\\\\u3001\\\\u300c\\\\u60c5\\\\u5831\\\\u7121\\\\u3057\\\\u300d\\\\u3068\\\\u7b54\\\\u3048\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nQuery: \\\\u63d0\\\\u4f9b\\\\u3055\\\\u308c\\\\u305f\\\\u30c6\\\\u30ad\\\\u30b9\\\\u30c8\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u8981\\\\u7d04\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\nAnswer: \"}], \"stream\": false, \"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"max_tokens\": null}' message='Post details'\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=17540 request_id=7a1c69f967ff1af5c55dd8a1a2742984 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:このコンテキストでは、画像分類タスクにおいてプロンプトエンジニアリングとアンサンブリングの手法が紹介されています。プロンプトエンジニアリングは、タスクごとにプロンプトテキストをカスタマイズすることでゼロショットパフォーマンスを向上させる手法です。例えば、カテゴリを指定することで、細かい画像分類データセットでの精度向上が確認されています。また、OCRデータセットでは、認識するテキストや数字に引用符を付けることでパフォーマンスが向上することがわかっています。さらに、衛星画像分類データセットでは、画像がその形式であることを明示することや、\"a satellite photo of a {label}\"のようなバリエーションを使用することで精度が向上します。\n",
      "\n",
      "アンサンブリングでは、複数のゼロショット分類器を組み合わせることでパフォーマンスを向上させます。異なるコンテキストプロンプト（例： \"A photo of a big {label}\"や\"A photo of a small {label}\"）を使用して、分類器を計算します。アンサンブルは確率空間ではなく埋め込み空間で行われ、平均化されたテキスト埋め込みのセットをキャッシュすることで、アンサンブルの計算コストを単一の分類器と同じにすることができます。\n",
      "\n",
      "これらの手法を組み合わせることで、ImageNetの精度を約5%向上させることができます。また、図4では、プロンプトエンジニアリングとアンサンブリングがCLIPモデルのパフォーマンスに与える影響が視覚化されています。\n",
      "\n",
      "以上が、提供されたテキストの要約です。\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7f3851fbe650>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/paper_translator/lib/SentenceWindowNodeParser.ipynb セル 27\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mindices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_summary\u001b[39;00m \u001b[39mimport\u001b[39;00m DocumentSummaryIndex\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# DocumentSummaryIndexの準備\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m doc_summary_index \u001b[39m=\u001b[39m DocumentSummaryIndex\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     docs,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     service_context\u001b[39m=\u001b[39;49mctx,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     response_synthesizer\u001b[39m=\u001b[39;49mresponse_synthesizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     summary_query\u001b[39m=\u001b[39;49mSUMMARY_QUERY,  \u001b[39m# 要約クエリ\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/SentenceWindowNodeParser.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py:102\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     docstore\u001b[39m.\u001b[39mset_document_hash(doc\u001b[39m.\u001b[39mget_doc_id(), doc\u001b[39m.\u001b[39mhash)\n\u001b[1;32m     98\u001b[0m nodes \u001b[39m=\u001b[39m service_context\u001b[39m.\u001b[39mnode_parser\u001b[39m.\u001b[39mget_nodes_from_documents(\n\u001b[1;32m     99\u001b[0m     documents, show_progress\u001b[39m=\u001b[39mshow_progress\n\u001b[1;32m    100\u001b[0m )\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    103\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    104\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m    105\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m    106\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py:77\u001b[0m, in \u001b[0;36mDocumentSummaryIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, response_synthesizer, summary_query, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_synthesizer \u001b[39m=\u001b[39m response_synthesizer \u001b[39mor\u001b[39;00m get_response_synthesizer(\n\u001b[1;32m     74\u001b[0m     service_context\u001b[39m=\u001b[39mservice_context, response_mode\u001b[39m=\u001b[39mResponseMode\u001b[39m.\u001b[39mTREE_SUMMARIZE\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_query \u001b[39m=\u001b[39m summary_query \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msummarize:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 77\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     78\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     79\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     80\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m     81\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[1;32m     82\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     83\u001b[0m )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py:71\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m index_struct \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[39massert\u001b[39;00m nodes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     index_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_index_from_nodes(nodes)\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct \u001b[39m=\u001b[39m index_struct\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mindex_store\u001b[39m.\u001b[39madd_index_struct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py:171\u001b[0m, in \u001b[0;36mBaseIndex.build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Build the index from nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docstore\u001b[39m.\u001b[39madd_documents(nodes, allow_update\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_index_from_nodes(nodes)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py:175\u001b[0m, in \u001b[0;36mDocumentSummaryIndex._build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m# first get doc_id to nodes_dict, generate a summary for each doc_id,\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m# then build the index struct\u001b[39;00m\n\u001b[1;32m    174\u001b[0m index_struct \u001b[39m=\u001b[39m IndexDocumentSummary()\n\u001b[0;32m--> 175\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_nodes_to_index(index_struct, nodes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_show_progress)\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py:149\u001b[0m, in \u001b[0;36mDocumentSummaryIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress)\u001b[0m\n\u001b[1;32m    147\u001b[0m nodes_with_scores \u001b[39m=\u001b[39m [NodeWithScore(node\u001b[39m=\u001b[39mn) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nodes]\n\u001b[1;32m    148\u001b[0m \u001b[39m# get the summary for each doc_id\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m summary_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    150\u001b[0m     query\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_summary_query,\n\u001b[1;32m    151\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes_with_scores,\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    153\u001b[0m summary_response \u001b[39m=\u001b[39m cast(Response, summary_response)\n\u001b[1;32m    154\u001b[0m summary_node_dict[doc_id] \u001b[39m=\u001b[39m TextNode(\n\u001b[1;32m    155\u001b[0m     text\u001b[39m=\u001b[39msummary_response\u001b[39m.\u001b[39mresponse,\n\u001b[1;32m    156\u001b[0m     relationships\u001b[39m=\u001b[39m{\n\u001b[1;32m    157\u001b[0m         NodeRelationship\u001b[39m.\u001b[39mSOURCE: RelatedNodeInfo(node_id\u001b[39m=\u001b[39mdoc_id)\n\u001b[1;32m    158\u001b[0m     },\n\u001b[1;32m    159\u001b[0m )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:128\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes)\u001b[0m\n\u001b[1;32m    123\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    125\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    126\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    127\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 128\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    129\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    130\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    131\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    132\u001b[0m         ],\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    136\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/tree_summarize.py:131\u001b[0m, in \u001b[0;36mTreeSummarize.get_response\u001b[0;34m(self, query_str, text_chunks, **response_kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async:\n\u001b[1;32m    123\u001b[0m     tasks \u001b[39m=\u001b[39m [\n\u001b[1;32m    124\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39mllm_predictor\u001b[39m.\u001b[39mapredict(\n\u001b[1;32m    125\u001b[0m             summary_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks\n\u001b[1;32m    129\u001b[0m     ]\n\u001b[0;32m--> 131\u001b[0m     summaries: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m run_async_tasks(tasks)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     summaries \u001b[39m=\u001b[39m [\n\u001b[1;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39mllm_predictor\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m    135\u001b[0m             summary_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks\n\u001b[1;32m    139\u001b[0m     ]\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py:39\u001b[0m, in \u001b[0;36mrun_async_tasks\u001b[0;34m(tasks, show_progress, progress_bar_desc)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks_to_execute)\n\u001b[0;32m---> 39\u001b[0m outputs: List[Any] \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39;49mrun(_gather())\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(task)\n\u001b[1;32m     32\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once()\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     95\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py:116\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     heappop(scheduled)\n\u001b[1;32m    111\u001b[0m timeout \u001b[39m=\u001b[39m (\n\u001b[1;32m    112\u001b[0m     \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m ready \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping\n\u001b[1;32m    113\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(\u001b[39mmax\u001b[39m(\n\u001b[1;32m    114\u001b[0m         scheduled[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_when \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime(), \u001b[39m0\u001b[39m), \u001b[39m86400\u001b[39m) \u001b[39mif\u001b[39;00m scheduled\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 116\u001b[0m event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    119\u001b[0m end_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/usr/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "\n",
    "# DocumentSummaryIndexの準備\n",
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    service_context=ctx,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    summary_query=SUMMARY_QUERY,  # 要約クエリ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "提供されたテキストは、Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel Goh、Sandhini Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark、Gretchen Krueger、Ilya Sutskeverによって書かれた「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文の一部です。この論文では、自然言語の監督から転移可能なビジュアルモデルの学習について説明されています。また、NLPの分野では直接生のテキストから学習する事前学習方法が革新的な成果を上げている一方、コンピュータビジョンの分野では従来通りクラウドラベル付きのデータセットでモデルを事前学習するのが一般的であることが指摘されています。さらに、過去の研究や実験結果も紹介されています。\n",
      "1\n",
      "doc_id: 1 is no text.\n",
      "2\n",
      "提供されたテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文の一部です。この論文では、自然言語に含まれる監督信号から知覚を学習するアプローチについて説明されています。また、他の研究者による関連研究も言及されており、自然言語からの学習が他のトレーニング方法よりもいくつかの利点を持つことが強調されています。具体的なアプローチについては、後続のサブセクションで詳細が説明されています。\n",
      "3\n",
      "提供されたテキストによれば、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文があります。この論文では、既存のデータセットが十分な大きさを持っていないことが指摘されており、MS-COCO、Visual Genome、YFCC100Mという3つのデータセットが主に使用されていることが述べられています。しかし、これらのデータセットは現代の基準では小さく、それぞれ約10万枚のトレーニング写真を持っています。一方、他のコンピュータビジョンシステムは最大で35億枚のInstagramの写真でトレーニングされていることが示されています。YFCC100Mは1億枚の写真を持っていますが、各画像のメタデータは希薄で品質も異なると述べられています。そのため、研究者らはインターネット上で公開されているさまざまなソースから4億枚の（画像、テキスト）ペアの新しいデータセットを構築しました。このデータセットは、ImageNetとほぼ同じサイズであり、自然言語の監督学習の大量のデータがインターネット上で公開されていることが主な動機とされています。また、このデータセットはWebTextデータセットと同じくらいの総単語数を持っていると述べられています。このデータセットはWebImageText（WIT）と呼ばれています。\n",
      "4\n",
      "提供されたテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」という論文の一部です。この論文では、自然言語の監督から転移可能なビジュアルモデルの学習について説明されています。論文では、最先端のコンピュータビジョンシステムについて言及され、画像とテキストの関連付けを行うための効率的な事前学習方法の選択についても議論されています。さらに、コントラスティブ学習や生成モデルに関する関連研究の結果も紹介されています。具体的には、CLIPという実装のコアの疑似コードや、異なる損失関数の紹介、モデルのトレーニング方法の詳細などが述べられています。また、画像とテキストの対応関係を学習するためのデータ拡張や、モデルのパラメータの最適化方法についても触れられています。\n",
      "5\n",
      "提供されたテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文の一部です。この論文では、画像エンコーダのための2つの異なるアーキテクチャについて説明されています。最初のアーキテクチャでは、ResNet-50が使用されており、広く採用されているため信頼性があります。2つ目のアーキテクチャでは、Vision Transformer（ViT）が使用されており、その実装にはいくつかの変更が加えられています。また、テキストエンコーダにはTransformerが使用されており、いくつかのアーキテクチャの変更が行われています。これらのモデルは、自然言語の監督から学習可能な視覚モデルを開発するために使用されます。\n",
      "6\n",
      "提供されたテキストには、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文の情報が含まれています。この論文では、ResNetとVision Transformerという2つの異なるモデルをトレーニングしました。ResNetでは、ResNet-50、ResNet-101、およびEfficientNetスタイルのモデルを使用しました。Vision Transformerでは、ViT-B/32、ViT-B/16、およびViT-L/14を使用しました。すべてのモデルは32エポックでトレーニングされ、Adamオプティマイザと学習率の減衰スケジュールが使用されました。さらに、ハイパーパラメータはグリッドサーチ、ランダムサーチ、および手動調整によって設定されました。モデルのトレーニングには、大規模なミニバッチサイズ、混合精度、勾配チェックポイントなどのテクニックが使用されました。最終的なモデルは「CLIP」と呼ばれ、最も優れたパフォーマンスを示しました。\n",
      "7\n",
      "提供されたテキストは、タイトルが「Learning Transferable Visual Models From Natural Language Supervision」である論文のセクション3の内容です。このセクションでは、実験について説明されています。具体的には、Zero-Shot Transferについて触れられています。\n",
      "8\n",
      "提供されたテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文の一部です。この論文では、ゼロショット学習という概念について議論されています。ゼロショット学習は、画像分類において未知のオブジェクトカテゴリに対して一般化することを指します。また、この論文では、ゼロショット転送を未知のデータセットに一般化することについても研究されています。さらに、この論文では、機械学習システムのタスク学習能力を測定する手法としてゼロショット転送を提案しています。この論文は2021年2月26日に発表されました。\n",
      "9\n",
      "提供されたテキストによると、CLIPは画像とテキストの組み合わせを予測するために事前学習されています。ゼロショット分類を実行するために、この能力を再利用します。データセットごとに、データセット内のすべてのクラスの名前を潜在的なテキストの組み合わせのセットとして使用し、CLIPによって最も確率の高い（画像、テキスト）の組み合わせを予測します。画像エンコーダとテキストエンコーダを使用して、画像の特徴埋め込みと可能なテキストの特徴埋め込みを計算し、これらの埋め込みのコサイン類似度を計算し、温度パラメータτでスケーリングし、ソフトマックスを使用して確率分布に正規化します。また、画像エンコーダは画像の特徴表現を計算するコンピュータビジョンのバックボーンであり、テキストエンコーダはテキストに基づいてクラスを指定するビジュアルコンセプトの重みを生成するハイパーネットワークです。ゼロショット分類器は、L2正規化された入力、L2正規化された重み、バイアスのない温度スケーリングを持つ多項ロジスティック回帰分類器です。ゼロショット評価では、テキストエンコーダによって計算されたゼロショット分類器をキャッシュし、すべての後続の予測に再利用します。これにより、生成コストをデータセット内のすべての予測に分散することができます。\n",
      "10\n",
      "提供されたテキストは、論文「Learning Transferable Visual Models From Natural Language Supervision」に関する情報です。この論文では、CLIPというモデルが紹介されており、Visual N-Gramsと比較してその性能が向上していることが述べられています。CLIPは、大規模なデータセットでトレーニングされ、より高い計算能力を必要とするビジョンモデルを使用しています。また、Visual N-Gramsと比較して他の2つのデータセットでも優れた性能を示しています。さらに、追加の評価スイートを使用して、30以上のデータセットと50以上の既存のコンピュータビジョンシステムと比較しています。\n",
      "11\n",
      "提供されたテキストによれば、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」という論文では、自然言語に基づくゼロショット転送を可能にするために、クラスの命名や説明を含む情報が重要であることが指摘されています。また、プロンプトエンジニアリングとアンサンブリングの手法がゼロショット分類の性能向上に役立つことが示されています。さらに、特定のカテゴリを指定することや、OCRデータセットではテキストや数字を引用符で囲むことが性能向上に役立つことも報告されています。衛星画像分類データセットでは、画像の形式を明示することや、バリエーションを使用することが有効であることが示されています。これらの手法を組み合わせることで、ImageNetの精度が向上することが報告されています。\n",
      "12\n",
      "提供されたテキストには、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文の一部が含まれています。この論文では、CLIPと呼ばれるモデルを使用して、ゼロショット分類器の性能を評価しています。ゼロショットCLIPは、一部のデータセットでロジスティック回帰分類器を上回る結果を示していますが、特殊なタスクでは性能が低下することもあります。また、ゼロショットCLIPは一部の一般的なオブジェクト分類データセットで良好なパフォーマンスを示していますが、より複雑なタスクでは能力が制限される可能性があります。さらに、論文ではゼロショット学習とフューショット学習のパフォーマンスの比較や、ゼロショット学習とフューショット学習の組み合わせについての研究も行われています。\n",
      "13\n",
      "提供されたテキストには、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文に関する情報が含まれています。この論文では、CLIPというモデルについての研究結果が述べられています。CLIPは、自然言語の教示を通じて学習されたビジョンモデルであり、他の既存のモデルよりも優れた性能と計算効率を持っています。さまざまなタスクでの評価や他のシステムとの比較においても優れた結果を示しています。ただし、一部のデータセットではEfficientNetと比較して性能が劣ることもあります。\n",
      "14\n",
      "提供されたテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文の一部です。この論文では、自然な分布のシフトに対するモデルの頑健性について議論されています。モデルの性能がImageNetのテストセットで人間の性能を上回ることが発表されたものの、後続の研究ではこれらのモデルが依然として多くの単純なミスを comit していることが明らかになりました。さらに、これらのモデルのパフォーマンスは、ImageNetの精度や人間の精度よりもはるかに低いことが多いと報告されています。この差異の理由については、さまざまなアイデアが提案されて研究されています。提案された説明の共通点は、深層学習モデルがトレーニングデータセット全体で有効な相関関係とパターンを見つける能力が非常に高いため、分布内のパフォーマンスが向上するということです。しかし、これらの相関関係やパターンの多くは実際には見かけ値であり、他の分布では成立せず、他のデータセットでのパフォーマンスの大幅な低下を引き起こします。これらの研究の多くは、ImageNetでトレーニングされたモデルに対して評価を制限していることに注意が必要です。さらに、これらの失敗は深層学習、ImageNet、またはその両方に帰因されるのか、その組み合わせなのかをどの程度まで考慮すべきかについても議論されています。この問題を別の視点から調査するために、自然言語の監督\n",
      "15\n",
      "提供されたテキストは、論文「Learning Transferable Visual Models From Natural Language Supervision」のセクション4に関する情報です。このセクションでは、CLIP（Contrastive Language-Image Pretraining）と人間のパフォーマンスおよび学習の比較について説明されています。著者は、人間のパフォーマンスを評価するためにいくつかのタスクを行い、人間のゼロショットパフォーマンスの強さや、画像のサンプルを見せることで人間のパフォーマンスがどれだけ改善されるかを調査しました。さらに、人間とCLIPのタスクの難易度を比較し、相関や違いを特定することも試みました。人間のパフォーマンスは、少数のトレーニング例で大幅に向上することが示されていますが、CLIPと人間のサンプル効率の差を減らすためのアルゴリズムの改善の余地があると指摘されています。\n",
      "16\n",
      "提供されたテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」という論文の一部です。この論文では、データの重複分析について説明されています。大規模なインターネットデータセットでの事前トレーニングには、ダウンストリームの評価データとの意図しない重複が懸念されます。重複を防ぐためのオプションとして、モデルのトレーニングの前にすべての重複を特定して削除する方法が提案されています。重複の程度やパフォーマンスの変化を文書化することで、真のホールドアウトパフォーマンスを報告することが保証されます。また、重複の量を評価するために二項検定も実行されます。さらに、データの汚染度や重複の影響を評価するために、さまざまなデータセットに対して分析が行われています。\n",
      "17\n",
      "提供されたテキストには、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」という論文のセクション6に関する情報が含まれています。このセクションでは、CLIP（Contrastive Language-Image Pretraining）というモデルの制約事項について説明されています。CLIPは、自然言語の教示から転移可能な視覚モデルを学習することを目的としています。CLIPの制約事項として、特定のタスクやデータセットにおいてパフォーマンスが低いこと、細かい分類や抽象的なタスク、新しいタスクに対してもパフォーマンスが低いことが指摘されています。CLIPのパフォーマンスを向上させるためには、さらなる研究が必要であり、計算やデータの効率性を改善する必要があります。また、CLIPは一部のタスクにおいては単純なベースラインよりも性能が低いこともあります。これらの制約事項を克服するためには、CLIPのモデルやトレーニング方法の改善が必要です。さらに、CLIPはゼロショット分類器の概念に制限されており、画像キャプションのような柔軟なアプローチと比較して制約があることも指摘されています。CLIPのトレーニングには数十億の画像が必要であり、未フィルターおよび未編集の画像テキストペアを使用するため、社会的バイアスを学習する可能性もあります。\n",
      "18\n",
      "提供されたテキストは、タイトルが「Learning Transferable Visual Models From Natural Language Supervision」の論文の一部です。この論文では、CLIPと呼ばれるモデルについて説明されています。CLIPは、自然言語の監督を受けて、画像分類のタスクを実行する能力を持つモデルです。CLIPは、任意の画像分類タスクを実行することができるため、広範な応用が可能です。また、CLIPは独自のカテゴリを作成することも可能であり、追加のデータやトレーニングの必要性がありません。さらに、CLIPは画像検索や検索などのタスクにも適用できます。ただし、CLIPの性能や適合性は評価される必要があり、その広範な影響も文脈において分析される必要があります。また、CLIPの能力はGPT-3などの大規模な生成モデルと同様の課題をもたらす可能性があります。さらに、CLIPは監視などのタスクにも使用できますが、その社会的な影響についても考慮する必要があります。最後に、AI開発者が一般的なコンピュータビジョンモデルのバイアスをよりよく特徴付けるために、より広範で文脈に即したテスト手法の開発が必要です。\n",
      "19\n",
      "提供されたテキストは、タイトルが「Learning Transferable Visual Models From Natural Language Supervision」である論文の情報です。この論文はAlec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel Goh、Sandhini Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark、Gretchen Krueger、Ilya Sutskeverによって執筆され、arXiv:2103.00020v1[cs.CV]という識別番号で2021年2月26日に公開されました。この論文は英語で書かれています。また、セクション7.1のタイトルは「Bias」であり、テキストには「Race」というキーワードも含まれています。\n",
      "20\n",
      "提供されたテキストには、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」という論文の一部が含まれています。この論文では、AIシステムの使用による社会的なバイアスや不平等について議論されています。具体的には、CLIPモデルのバイアスに関する予備的な分析や、FairFaceデータセットを使用したモデルの性能評価が行われています。また、性別や人種の分類に関する問題や、モデルの性能と公平性の関係についても言及されています。さらに、モデルのパフォーマンスやバイアスの発生に影響を与える設計上の決定についても言及されています。\n",
      "21\n",
      "提供されたテキストは、Alec Radfordらによる「Learning Transferable Visual Models From Natural Language Supervision」というタイトルの論文に関する情報です。この論文では、自然言語の監督から転移可能なビジュアルモデルの学習について説明されています。論文では、CLIPと呼ばれるモデルの性能と応用について詳しく説明されており、CLIPは画像とテキストの関連性を学習することで、様々なタスクに対してゼロショット学習を行うことができます。また、論文ではCLIPの性能評価や将来の影響についての議論も行われています。さらに、監視や画像分類に関連する実験結果も報告されています。\n",
      "22\n",
      "提供されたテキストは、関連する研究についての情報を含んでいます。この研究は、一般的なコンピュータビジョンモデルの課題やバイアス、影響を示すために行われました。研究者は、これらのモデルの能力、制約、バイアスを特徴付けるための将来の研究を促進し、研究コミュニティとの協力を期待しています。また、モデルの性能が有望なアプリケーション領域や性能が低下する可能性のある領域を特定するための探索が重要であると述べています。さらに、この研究は、モデルの利用可能性を高めるための特性の特徴付けやバイアスの警告、テストの作成、さらなる研究のための領域の特定に役立つことを期待しています。\n",
      "23\n",
      "提供されたテキストは、自然言語の監督を使用して視覚モデルを学習する方法に関する研究についての情報です。この研究では、自然言語の監督を利用する方法が広く研究されており、画像検索やオブジェクト分類などのタスクのパフォーマンス向上に役立つことが示されています。さらに、自然言語の説明やフィードバックを使用してタスクの分類を改善するための研究や、対話ベースの学習や意味解析を利用した研究も行われています。また、学習可能な視覚モデルを自然言語の監督から転移学習する方法に関する研究も行われており、画像とテキストの関連性を学習するための共同モデルの学習にも関連しています。\n",
      "24\n",
      "doc_id: 24 is no text.\n",
      "25\n",
      "doc_id: 25 is no text.\n",
      "26\n",
      "doc_id: 26 is no text.\n",
      "27\n",
      "doc_id: 27 is no text.\n",
      "28\n",
      "doc_id: 28 is no text.\n",
      "29\n",
      "doc_id: 29 is no text.\n",
      "30\n",
      "doc_id: 30 is no text.\n",
      "31\n",
      "doc_id: 31 is no text.\n",
      "32\n",
      "doc_id: 32 is no text.\n",
      "33\n",
      "doc_id: 33 is no text.\n",
      "34\n",
      "doc_id: 34 is no text.\n",
      "35\n",
      "doc_id: 35 is no text.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc_summary_index.index_id)):\n",
    "    print(f\"{i}\")\n",
    "    try:\n",
    "        print(doc_summary_index.get_document_summary(f\"{i}\"))\n",
    "    except ValueError:\n",
    "        print(f\"doc_id: {i} is no text.\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
