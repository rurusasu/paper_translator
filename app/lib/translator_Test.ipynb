{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.XMLUtils import make_xml_file, get_sections\n",
    "from src.OpenAIUtils import get_message\n",
    "\n",
    "dir_path = \"/home/paper_translator/data/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    "pdf_name = \"Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = make_xml_file(dir_path=dir_path, pdf_name=pdf_name, is_debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = get_sections(root=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = sections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"\n",
    "### 指示 ###\n",
    "論文の内容を理解した上で，重要なポイントを箇条書きで3点書いてください。\n",
    "\n",
    "### 箇条書きの制約 ###\n",
    "- 最大3個\n",
    "- 日本語\n",
    "- 箇条書き1個を100文字以内\n",
    "\n",
    "### 対象とする論文の内容 ###\n",
    "{text}\n",
    "\n",
    "### 出力形式 ###\n",
    "- 箇条書き1\n",
    "- 箇条書き2\n",
    "- 箇条書き3\n",
    "\"\"\"\n",
    "\n",
    "response = get_message(text=section.body, system=SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Pre-training methods directly from raw text have scaled significantly across many orders of magnitude in compute and data. \n",
      "2. Studies have shown the potential of using natural language supervision for image representation learning, but demonstrated performances on common benchmarks are still lower than alternative approaches. \n",
      "3. CLIP (Contrastive Language-Image Pre-training) is a new dataset of 400 million (image, text) pairs and has shown to be an efficient method of learning from natural language supervision. It is much more efficient at zero-shot transfer than image caption baselines and competitive with prior task-specific supervised models.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fugumt-en-ja を使って翻訳する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e34c3abee924cda834b031692c46eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bc272cfb7c46d989bbcbe86a6848fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ef0ba8725e44019b9d666693e570a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e4c3d8f6f542848672a0e78bab2898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/791k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304e163fe9764e72b6ad1419d94bb242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de88ac9c4d5444ee88f3f1fecf1179d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/121M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83b2cc832f9475092dd8677d1a9632c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "model_name = \"staka/fugumt-en-ja\"\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#summarizer = pipeline(\"summarization\", model=\"kworts/BARTxiv\")\n",
    "translator = pipeline(\"translation\", model=model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text = translator(response)[0][\"translation_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未知写本の原形が直接的に数桁の処理をすることで,様々な規模の学習が可能であることが, 学習のベースとなる画像の処理の効率が高まると見込まれることが分かること, 未知なる言語の学習にも大きな期待が集まることもその一部分から見れることが明らかにされるようにも見えることが明らかにされるようにも見えるようにも見えるようにも見えるようにも見えることがそのようにも見えるようにも見えるようにもなることがそのようにも見えるようにもいわれるようにも見えることが多くなってい いからそのようにも見えるようにも見えるようにもいってい る もまだにそのように もなる もつぎの の の の の の の の の の の の の の を ''''''''-('''('本学の対人としての) の の ' の の ' の ' の ' の ' の ' の ' の ' の ' の ' の ' の ' ''''-の の の の の ' の ' の の ' の ' ' ' の ' ' の ' ' の の の の ' の ' の の の の の ' ' の の の の の の ' の の の の の の ' の の の の の ' の の の の の の の の の は の の の の の の の の ' の の の の の の の の の の の の の の の の の の の の の の の の の の の の の の の の の の の の の のの の のののののののののののののののの,ののののののもは まだであるのののの\n"
     ]
    }
   ],
   "source": [
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weblab-10Bを使って翻訳する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザーとモデルの準備\n",
    "quantized_model_dir = \"dahara1/weblab-10b-instruction-sft-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit-128g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8b1224a61f44f8bae2bfc2c47dce3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a0c2a515ab467f9f734f2e5b2b022a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051ebc303595417ca87c03c710312222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a5ace7f9774619bc476252006cec90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfae314405548008c6dd99cbb438093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05475534b63b4bad9dcce72f765cdae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26b8846e0714c2bba5e70ff4bc1924d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7599489f477344f49c2d85c5f1784f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dba85b7fb94cc982da98b1f220922f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750988dd24bf4ca6b81b2a6ee3368455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"kworts/BARTxiv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    quantized_model_dir, use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc035c7345cc4af9991112d9daa9b58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/737 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbde465cfc14b4385b75e010afed2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)quantize_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3012b1d0bf401094a2366a15847902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)bit-128g.safetensors:   0%|          | 0.00/6.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPTNeoXGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "GPTNeoXGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    quantized_model_dir,\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Pre-training methods directly from raw text have scaled significantly across many orders of magnitude in compute and data. \n",
      "2. Studies have shown the potential of using natural language supervision for image representation learning, but demonstrated performances on common benchmarks are still lower than alternative approaches. \n",
      "3. CLIP (Contrastive Language-Image Pre-training) is a new dataset of 400 million (image, text) pairs and has shown to be an efficient method of learning from natural language supervision. It is much more efficient at zero-shot transfer than image caption baselines and competitive with prior task-specific supervised models.\n"
     ]
    }
   ],
   "source": [
    "prompt_text = response\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "### 指示 ###\n",
    "対象とする文章を制約に基づいて翻訳してください。\n",
    "\n",
    "### 対象とする文章 ###\n",
    "{prompt_text}\n",
    "\n",
    "### 文章の制約 ###\n",
    "- 日本語で出力してください。\n",
    "- 箇条書きの文章ごとに翻訳してください。\n",
    "- 1つの項目ごとに100文字以内で翻訳してください。\n",
    "- 箇条書きで出力してください。\n",
    "\n",
    "### 出力 ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 指示 ###\n",
      "対象とする文章を制約に基づいて翻訳してください。\n",
      "\n",
      "### 対象とする文章 ###\n",
      "1. Pre-training methods directly from raw text have scaled significantly across many orders of magnitude in compute and data. \n",
      "2. Studies have shown the potential of using natural language supervision for image representation learning, but demonstrated performances on common benchmarks are still lower than alternative approaches. \n",
      "3. CLIP (Contrastive Language-Image Pre-training) is a new dataset of 400 million (image, text) pairs and has shown to be an efficient method of learning from natural language supervision. It is much more efficient at zero-shot transfer than image caption baselines and competitive with prior task-specific supervised models.\n",
      "\n",
      "### 文章の制約 ###\n",
      "- 日本語で出力してください。\n",
      "- 箇条書きの文章ごとに翻訳してください。\n",
      "- 1つの項目ごとに100文字以内で翻訳してください。\n",
      "- 箇条書きで出力してください。\n",
      "\n",
      "### 出力 ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT_TEMPLATE.format(prompt_text=prompt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  187,  4118,   209, 34811, 30420,   209,  4118,   187, 40292, 44795,\n",
      "          6088, 15244, 20855, 22701,   243,  6449, 34179, 20557,   215,  5444,\n",
      "         41140,   765,   100, 36313,   163,   125,   121, 14945,   113, 18309,\n",
      "         14016, 13973, 13129,  5151,  4340,   187,   187,  4118,   209, 40292,\n",
      "         44795,  6088, 15244, 20855, 22701,   243,   209,  4118,   187,    18,\n",
      "            15,  5729,    14, 31158,  3082,  3587,   432,  9305,  2505,   452,\n",
      "         24337,  3012,  2439,  1142,  7367,   273,  9777,   275, 11897,   285,\n",
      "           941,    15,   209,   187,    19,    15, 11709,   452,  2011,   253,\n",
      "          2442,   273,   970,  3626,  3448, 20446,   323,  2460,  6779,  4715,\n",
      "            13,   533,  5183, 16226,   327,  1846, 49602,   403,  1335,  2406,\n",
      "           685,  5795,  7274,    15,   209,   187,    20,    15,  8951,  3123,\n",
      "           313,  2861, 42836,   422, 18847,    14,  6586,  5729,    14, 31158,\n",
      "            10,   310,   247,   747, 10895,   273,  9166,  3041,   313,  5695,\n",
      "            13,  2505,    10,  8557,   285,   556,  2011,   281,   320,   271,\n",
      "          5919,  1332,   273,  4715,   432,  3626,  3448, 20446,    15,   733,\n",
      "           310,  1199,   625,  5919,   387,  5058,    14, 11860,  3700,   685,\n",
      "          2460, 11743,  1666, 25379,   285, 12085,   342,  2720,  4836,    14,\n",
      "          6160, 22296,  3210,    15,   187,   187,  4118,   209, 20855, 22701,\n",
      "           243,  3917, 34179, 20557,   215,   209,  4118,   187,    14,   209,\n",
      "         49868, 19119,   241,  6344, 18678, 32365, 18309, 14016, 13973, 13129,\n",
      "          5151,  4340,   187,    14,   209, 25541,   218, 41351, 47008, 14321,\n",
      "          3917, 20855, 22701,   243, 46843,  6088,  5444,   163,   125,   121,\n",
      "         14945,   113, 18309, 14016, 13973, 13129,  5151,  4340,   187,    14,\n",
      "           337, 20691,  3917, 27661,   216, 28154, 46843,  6088,  5444,  2313,\n",
      "         20855, 30886, 16877, 28327,  6344,   163,   125,   121, 14945,   113,\n",
      "         18309, 14016, 13973, 13129,  5151,  4340,   187,    14,   209, 25541,\n",
      "           218, 41351, 47008, 14321,  6344, 18678, 32365, 18309, 14016, 13973,\n",
      "         13129,  5151,  4340,   187,   187,  4118,   209, 18678, 32365,   209,\n",
      "          4118,   187]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tokens = (\n",
    "    tokenizer(PROMPT_TEMPLATE.format(prompt_text=prompt_text), return_tensors=\"pt\")\n",
    "    .to(\"cuda:0\")\n",
    "    .input_ids\n",
    ")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=tokens,\n",
    "    eos_token_id=0,\n",
    "    pad_token_id=1,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 指示 ###\n",
      "対象とする文章を制約に基づいて翻訳してください。\n",
      "\n",
      "### 対象とする文章 ###\n",
      "1. Pre-training methods directly from raw text have scaled significantly across many orders of magnitude in compute and data. \n",
      "2. Studies have shown the potential of using natural language supervision for image representation learning, but demonstrated performances on common benchmarks are still lower than alternative approaches. \n",
      "3. CLIP (Contrastive Language-Image Pre-training) is a new dataset of 400 million (image, text) pairs and has shown to be an efficient method of learning from natural language supervision. It is much more efficient at zero-shot transfer than image caption baselines and competitive with prior task-specific supervised models.\n",
      "\n",
      "### 文章の制約 ###\n",
      "- 日本語で出力してください。\n",
      "- 箇条書きの文章ごとに翻訳してください。\n",
      "- 1つの項目ごとに100文字以内で翻訳してください。\n",
      "- 箇条書きで出力してください。\n",
      "\n",
      "### 出力 ###\n",
      "1. 自然言語の教師あり学習は、コンピュータとデータの両方で大きな進歩を遂げた。\n",
      "2. 画像表現のための自然言語教師あり学習の可能性を示す研究が行われたが、一般的なバウンディングボックスの成績は、他のアプローチよりも劣っていた。\n",
      "3. CLIP(Contrastive Language-Image Pre-training)は、400,000,000(画像、テキスト)のペアと、自然言語教師あり学習のための新しいデータセットである。これは、画像のキャプションのバイナリベースの学習よりも効率的であり、既存のタスク特化型の教師あり学習モデルよりも優れている。\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "translated_text = tokenizer.decode(output[0])\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
