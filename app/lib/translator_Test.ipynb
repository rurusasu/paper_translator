{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.XMLUtils import make_xml_file, get_sections\n",
    "from src.OpenAIUtils import get_message\n",
    "\n",
    "dir_path = \"/home/paper_translator/data/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    "pdf_name = \"Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = make_xml_file(dir_path=dir_path, pdf_name=pdf_name, is_debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = get_sections(root=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = sections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"\n",
    "### 指示 ###\n",
    "論文の内容を理解した上で，重要なポイントを箇条書きで3点書いてください。\n",
    "\n",
    "### 箇条書きの制約 ###\n",
    "- 最大3個\n",
    "- 日本語\n",
    "- 箇条書き1個を100文字以内\n",
    "\n",
    "### 対象とする論文の内容 ###\n",
    "{text}\n",
    "\n",
    "### 出力形式 ###\n",
    "- 箇条書き1\n",
    "- 箇条書き2\n",
    "- 箇条書き3\n",
    "\"\"\"\n",
    "\n",
    "response = get_message(text=section.body, system=SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pre-training methods that learn from raw text have revolutionized NLP and web-scale collections of text provide more aggregate supervision than high-quality crowd-labeled NLP datasets. \n",
      "- Natural language supervision can be used in computer vision for narrow and well-targeted tasks, such as predicting ImageNet-related hashtags on Instagram images and pre-training models to predict the classes of the noisily labeled JFT-300M dataset. \n",
      "- CLIP, a method of learning from natural language supervision, is an efficient way to perform a wide set of tasks during pre-training, including OCR, geo-localization, and action recognition. CLIP is more efficient at zero-shot transfer than an image caption baseline and is more robust than equivalent accuracy supervised ImageNet models.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fugumt-en-ja を使って翻訳する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "model_name = \"staka/fugumt-en-ja\"\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#summarizer = pipeline(\"summarization\", model=\"kworts/BARTxiv\")\n",
    "translator = pipeline(\"translation\", model=model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text = translator(response)[0][\"translation_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "言語スーパーバイサーが使用可能な学習方法は,従来よりも優れたものである反省が,新たな方法では,より低い性能を示すことができる。.. MRIの学習に先立つような,新たな対人関係の可能な,新たな画像生成のアプローチ。この技術を用いるための,トレーニングに有効な,新たな手法を提案する,新たな手法を提案する,新たな,新たな,新たな,新たな,新たな手法を提案する,新たな,新たな,新たな,新たな,新たな,新たな手法を提案する,新たな,新たな,新たな,より効率的な,より効率的な,より効率的な,より効率的な,より優れた,優れた,優れた,優れた,最先端の,学習技術手法を提案することができる,という,その目的から,現在30年以上にわたって,より優れたデータから,より優れた,より効率的な,より効率的な,より効率的な,より効率的な,より優れた,より優れた,より強力な,より優れた,という特徴をも持つことができる,という特徴が,その上では,現在,そのようにしていまだ見受けられるようにも,そのように,そのようにしていまだにそのようにしていまだにそのようにしては,そのようにしては,そのようにして,そのようにしていまだにそのようにしていまだにそのようにしていまだにそのようにしては,そのようにしては,そのようにしていまだ,そのようにしていまだにそのようにしていまだに,そのようにしていまだに,そのようにしていちが,そのように,そのように,そのようにしていまだにそのようにしては,そのように,そのようにしていまだにそのようにしていちが,そのようにしてはいちが,そのように,そのように,そのように,そのように,そのように,そのように,そのように,そのように,そのように,そのように,4....で,そのように、4万万万万,というように,というように,そのように,そのように,そのようにという,4万,4万,4万万万万万,という,という,という,という,という,という,という,4という,という,という,という,という,という,という,という,という,という,という,という,という,という,という,という、という、という、という、という、という、という、という、という、という,という、という、という、という、という、という、という、という、という、という、という、という、という、という、という、という、という、という、という、という、という、\n"
     ]
    }
   ],
   "source": [
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weblab-10Bを使って翻訳する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザーとモデルの準備\n",
    "quantized_model_dir = \"dahara1/weblab-10b-instruction-sft-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit-128g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"kworts/BARTxiv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    quantized_model_dir, use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPTNeoXGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "GPTNeoXGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n",
      "GPTNeoXGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    quantized_model_dir,\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pre-training methods that learn from raw text have revolutionized NLP and web-scale collections of text provide more aggregate supervision than high-quality crowd-labeled NLP datasets. \n",
      "- Natural language supervision can be used in computer vision for narrow and well-targeted tasks, such as predicting ImageNet-related hashtags on Instagram images and pre-training models to predict the classes of the noisily labeled JFT-300M dataset. \n",
      "- CLIP, a method of learning from natural language supervision, is an efficient way to perform a wide set of tasks during pre-training, including OCR, geo-localization, and action recognition. CLIP is more efficient at zero-shot transfer than an image caption baseline and is more robust than equivalent accuracy supervised ImageNet models.\n"
     ]
    }
   ],
   "source": [
    "prompt_text = response\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "### 指示 ###\n",
    "対象とする文章を制約に基づいて翻訳してください。\n",
    "\n",
    "### 対象とする文章 ###\n",
    "{prompt_text}\n",
    "\n",
    "### 文章の制約 ###\n",
    "- 日本語で出力してください。\n",
    "- 箇条書きの文章ごとに翻訳してください。\n",
    "- 1つの項目ごとに100文字以内で翻訳してください。\n",
    "- 箇条書きで出力してください。\n",
    "\n",
    "### 出力 ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 指示 ###\n",
      "対象とする文章を制約に基づいて翻訳してください。\n",
      "\n",
      "### 対象とする文章 ###\n",
      "- Pre-training methods that learn from raw text have revolutionized NLP and web-scale collections of text provide more aggregate supervision than high-quality crowd-labeled NLP datasets. \n",
      "- Natural language supervision can be used in computer vision for narrow and well-targeted tasks, such as predicting ImageNet-related hashtags on Instagram images and pre-training models to predict the classes of the noisily labeled JFT-300M dataset. \n",
      "- CLIP, a method of learning from natural language supervision, is an efficient way to perform a wide set of tasks during pre-training, including OCR, geo-localization, and action recognition. CLIP is more efficient at zero-shot transfer than an image caption baseline and is more robust than equivalent accuracy supervised ImageNet models.\n",
      "\n",
      "### 文章の制約 ###\n",
      "- 日本語で出力してください。\n",
      "- 箇条書きの文章ごとに翻訳してください。\n",
      "- 1つの項目ごとに100文字以内で翻訳してください。\n",
      "- 箇条書きで出力してください。\n",
      "\n",
      "### 出力 ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT_TEMPLATE.format(prompt_text=prompt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  187,  4118,   209, 34811, 30420,   209,  4118,   187, 40292, 44795,\n",
      "          6088, 15244, 20855, 22701,   243,  6449, 34179, 20557,   215,  5444,\n",
      "         41140,   765,   100, 36313,   163,   125,   121, 14945,   113, 18309,\n",
      "         14016, 13973, 13129,  5151,  4340,   187,   187,  4118,   209, 40292,\n",
      "         44795,  6088, 15244, 20855, 22701,   243,   209,  4118,   187,    14,\n",
      "          5729,    14, 31158,  3082,   326,  3037,   432,  9305,  2505,   452,\n",
      "         10532,  1025,   427, 13010,   285,  4384,    14,  7527, 18406,   273,\n",
      "          2505,  2085,   625, 19737, 20446,   685,  1029,    14, 15177,  9539,\n",
      "            14, 22027,   427, 13010, 15302,    15,   209,   187,    14, 14673,\n",
      "          3448, 20446,   476,   320,   908,   275,  4382,  8113,   323,  6891,\n",
      "           285,   973,    14, 44490,  8892,    13,   824,   347, 21565, 10882,\n",
      "          8695,    14,  4919, 41035,  3544,   327, 19614,  3888,   285,   638,\n",
      "            14, 31158,  3210,   281,  3283,   253,  5971,   273,   253,   642,\n",
      "           261,  1031, 13130,   500,  5518,    14,  7554,    46, 10895,    15,\n",
      "           209,   187,    14,  8951,  3123,    13,   247,  1332,   273,  4715,\n",
      "           432,  3626,  3448, 20446,    13,   310,   271,  5919,  1039,   281,\n",
      "          1347,   247,  4618,   873,   273,  8892,  1309,   638,    14, 31158,\n",
      "            13,  1690,   473,  1311,    13, 45267,    14,  6790,  1320,    13,\n",
      "           285,  2250,  8981,    15,  8951,  3123,   310,   625,  5919,   387,\n",
      "          5058,    14, 11860,  3700,   685,   271,  2460, 11743,  8245,   285,\n",
      "           310,   625, 10237,   685,  6425,  7200, 22296, 10882,  8695,  3210,\n",
      "            15,   187,   187,  4118,   209, 20855, 22701,   243,  3917, 34179,\n",
      "         20557,   215,   209,  4118,   187,    14,   209, 49868, 19119,   241,\n",
      "          6344, 18678, 32365, 18309, 14016, 13973, 13129,  5151,  4340,   187,\n",
      "            14,   209, 25541,   218, 41351, 47008, 14321,  3917, 20855, 22701,\n",
      "           243, 46843,  6088,  5444,   163,   125,   121, 14945,   113, 18309,\n",
      "         14016, 13973, 13129,  5151,  4340,   187,    14,   337, 20691,  3917,\n",
      "         27661,   216, 28154, 46843,  6088,  5444,  2313, 20855, 30886, 16877,\n",
      "         28327,  6344,   163,   125,   121, 14945,   113, 18309, 14016, 13973,\n",
      "         13129,  5151,  4340,   187,    14,   209, 25541,   218, 41351, 47008,\n",
      "         14321,  6344, 18678, 32365, 18309, 14016, 13973, 13129,  5151,  4340,\n",
      "           187,   187,  4118,   209, 18678, 32365,   209,  4118,   187]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tokens = (\n",
    "    tokenizer(PROMPT_TEMPLATE.format(prompt_text=prompt_text), return_tensors=\"pt\")\n",
    "    .to(\"cuda:0\")\n",
    "    .input_ids\n",
    ")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/paper_translator/lib/translator_Test.ipynb セル 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m tokenizer\u001b[39m.\u001b[39mpad_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     tokenizer\u001b[39m.\u001b[39mpad_token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39meos_token\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mtokens,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c707974686f6e5c5c70617065725f7472616e736c6174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f643a2f4d795f70726f6772616d696e672f707974686f6e2f70617065725f7472616e736c6174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/paper_translator/lib/translator_Test.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/auto_gptq/modeling/_base.py:443\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype):\n\u001b[0;32m--> 443\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1642\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1634\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1635\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1636\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1637\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1638\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1639\u001b[0m     )\n\u001b[1;32m   1641\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1642\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1643\u001b[0m         input_ids,\n\u001b[1;32m   1644\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1645\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1646\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1647\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1648\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1649\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1650\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1651\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1652\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1653\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1654\u001b[0m     )\n\u001b[1;32m   1656\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1657\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1659\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1660\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1666\u001b[0m     )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2724\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2721\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2723\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2724\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2725\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2726\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2727\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2728\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2729\u001b[0m )\n\u001b[1;32m   2731\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2732\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:767\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[39m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    765\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 767\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    768\u001b[0m     input_ids,\n\u001b[1;32m    769\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    770\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    771\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    772\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    773\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    774\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    775\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    776\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    777\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    778\u001b[0m )\n\u001b[1;32m    780\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    781\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:658\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    650\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    651\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    652\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    655\u001b[0m         head_mask[i],\n\u001b[1;32m    656\u001b[0m     )\n\u001b[1;32m    657\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    659\u001b[0m         hidden_states,\n\u001b[1;32m    660\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    661\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    662\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    663\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    664\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    665\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    666\u001b[0m     )\n\u001b[1;32m    667\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    668\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:420\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     hidden_states: Optional[torch\u001b[39m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    419\u001b[0m ):\n\u001b[0;32m--> 420\u001b[0m     attention_layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    421\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states),\n\u001b[1;32m    422\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    423\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    424\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    425\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    426\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    427\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    429\u001b[0m     attn_output \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_dropout(attn_output)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:185\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m has_layer_past:\n\u001b[1;32m    184\u001b[0m     seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layer_past[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 185\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value, seq_len\u001b[39m=\u001b[39;49mseq_len)\n\u001b[1;32m    186\u001b[0m query, key \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n\u001b[1;32m    187\u001b[0m query \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((query, query_pass), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:320\u001b[0m, in \u001b[0;36mGPTNeoXRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m seq_len \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_len_cached:\n\u001b[1;32m    319\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_cos_sin_cache(seq_len\u001b[39m=\u001b[39mseq_len, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 320\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcos_cached[:seq_len, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdevice), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin_cached[:seq_len, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=tokens,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 指示 ###\n",
      "対象とする文章を制約に基づいて翻訳してください。\n",
      "\n",
      "### 文章の制約 ###\n",
      "- 日本語で出力してください。\n",
      "- 箇条書きの文章ごとに翻訳してください。\n",
      "- 1つの項目ごとに100文字以内で翻訳してください。\n",
      "- 箇条書きで出力してください。\n",
      "\n",
      "### 対象とする文章 ###\n",
      "- Pre-training methods that learn from raw text have revolutionized NLP and web-scale collections of text provide more aggregate supervision than high-quality crowd-labeled NLP datasets. \n",
      "- Natural language supervision can be used in computer vision for narrow and well-targeted tasks, such as predicting ImageNet-related hashtags on Instagram images and pre-training models to predict the classes of the noisily labeled JFT-300M dataset. \n",
      "- CLIP, a method of learning from natural language supervision, is an efficient way to perform a wide set of tasks during pre-training, including OCR, geo-localization, and action recognition. CLIP is more efficient at zero-shot transfer than an image caption baseline and is more robust than equivalent accuracy supervised ImageNet models.\n",
      "\n",
      "### 出力 ###\n",
      "- 日本語で出力してください。\n",
      "- 箇条書きの文章ごとに翻訳してください。\n",
      "- 1つの項目ごとに100文字以内で翻訳してください。\n",
      "- 箇条書きで出力してください。\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "translated_text = tokenizer.decode(output[0])\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
