{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_debug_handler = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_memory = {\"0\": \"10GiB\", \"cpu\": \"5GiB\"}\n",
    "\n",
    "model_id = \"mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_id, \n",
    "                                           #device=device,\n",
    "                                           #max_memory=max_memory,\n",
    "                                           device_map=\"auto\",\n",
    "                                           use_safetensors=True,\n",
    "                                           use_fast_inference=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaGPTQForCausalLM' is not supported for translation_xx_to_yy. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"translation_xx_to_yy\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                max_length=5120,\n",
    "                #max_new_tokens=4096,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#from llama_index import LangchainEmbedding\n",
    "from llama_index.embeddings import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=model_id, max_length=512, device=device)\n",
    "#embed_model = LangchainEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "from llama_index import ServiceContext, StorageContext, get_response_synthesizer\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "\n",
    "# 非同期処理の有効化\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceWindowNodeParser(window_size=3, \n",
    "                                  window_metadata_key=\"sentence_window\", \n",
    "                                  original_text_metadata_key=\"original_text\"\n",
    "                                  )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore(),\n",
    "    vector_store=SimpleVectorStore(),\n",
    "    index_store=SimpleIndexStore(),\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm = llm, \n",
    "    embed_model=embed_model, \n",
    "    node_parser=node_parser, \n",
    "    callback_manager=callback_manager,\n",
    "    chunk_size=514,\n",
    "    chunk_overlap=20,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.XMLUtils import DocumentCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/paper_translator/data\"\n",
    "document_name = \"Ask_more_know_better_Reinforce-Learned_Prompt_Questions_for_Decision_Making_with_Large_Language_Models\"\n",
    "document_path = f\"{base_path}/documents/{document_name}\"\n",
    "xml_path = f\"{document_path}/{document_name}.tei.xml\"\n",
    "\n",
    "creator = DocumentCreator()\n",
    "creator.load_xml(xml_path, contain_abst=False)\n",
    "documents = creator.create_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.base import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAシステムプロンプト\n",
    "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n",
    "    content=(\n",
    "                \"#依頼\\n\"\n",
    "                \"あなたは高度な理解能力を持ち、複雑なテキストも簡潔に要約することができるAIです。\\n\"\n",
    "                \"事前知識ではなく、提供されたコンテキストに基づいて精確な回答を行ってください。\\n\"\n",
    "                \"#従うべきルール\\n\"\n",
    "                \"1. 略語や初出の用語には解説を加え、AI分野やコンピュータの初心者も理解できるように工夫してください。\\n\"\n",
    "                \"2. 回答内で指定されたコンテキストを直接参照しないでください。\\n\"\n",
    "                \"3. 「コンテキストに基づいて、...」や「コンテキスト情報は...」、またはそれに類するような記述は避けてください。\\n\"\n",
    "                \"4. 出力は日本語で行ってください。\"\n",
    "                \"#手順\\n\"\n",
    "                \"1. 与えられたコンテキストに含まれる主要なポイントやコンセプトを細かく分解してください。\\n\"\n",
    "                \"2. それぞれのポイントやコンセプトに対して詳細な説明を加えてください。\\n\"\n",
    "                \"3. まずは指示に従って、文書の初版を作成してください。\\n\"\n",
    "                \"4. 作成した初版をルールに従っているか自己分析してください。\\n\"\n",
    "                \"5. 自己分析の結果を踏まえて、文書を改善してください。\\n\"\n",
    "    ),\n",
    "    role=MessageRole.SYSTEM,\n",
    ")\n",
    "\n",
    "# QAプロンプトテンプレートメッセージ\n",
    "TEXT_QA_PROMPT_TMPL_MSGS = [\n",
    "            TEXT_QA_SYSTEM_PROMPT,\n",
    "            ChatMessage(\n",
    "                content=(\n",
    "                    \"複数のソースからのコンテキスト情報を以下に示します。\\n\"\n",
    "                    \"---------------------\\n\"\n",
    "                    \"{context_str}\\n\"\n",
    "                    \"---------------------\\n\"\n",
    "                    \"予備知識ではなく、複数のソースからの情報を考慮して質問に答えてください。\\n\"\n",
    "                    \"疑問がある場合は、「情報無し」と答えてください。\\n\"\n",
    "                    \"Query: {query_str}\\n\"\n",
    "                    \"Answer: \"\n",
    "                ),\n",
    "                role=MessageRole.USER,\n",
    "            ),\n",
    "]\n",
    "\n",
    "# チャットQAプロンプト\n",
    "CHAT_TEXT_QA_PROMPT = ChatPromptTemplate(\n",
    "            message_templates=TEXT_QA_PROMPT_TMPL_MSGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAシステムプロンプト\n",
    "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n",
    "    content=(\n",
    "                \"#依頼\\n\"\n",
    "                \"あなたは高度な理解能力を持ち、複雑なテキストも簡潔に要約することができるAIです。\\n\"\n",
    "                \"事前知識ではなく、提供されたコンテキストに基づいて精確な回答を行ってください。\\n\"\n",
    "                \"#従うべきルール\\n\"\n",
    "                \"1. 略語や初出の用語には解説を加え、AI分野やコンピュータの初心者も理解できるように工夫してください。\\n\"\n",
    "                \"2. 回答内で指定されたコンテキストを直接参照しないでください。\\n\"\n",
    "                \"3. 「コンテキストに基づいて、...」や「コンテキスト情報は...」、またはそれに類するような記述は避けてください。\\n\"\n",
    "                \"4. 出力は日本語で行ってください。\"\n",
    "                \"#手順\\n\"\n",
    "                \"1. 与えられたコンテキストに含まれる主要なポイントやコンセプトを細かく分解してください。\\n\"\n",
    "                \"2. それぞれのポイントやコンセプトに対して詳細な説明を加えてください。\\n\"\n",
    "                \"3. まずは指示に従って、文書の初版を作成してください。\\n\"\n",
    "                \"4. 作成した初版をルールに従っているか自己分析してください。\\n\"\n",
    "                \"5. 自己分析の結果を踏まえて、文書を改善してください。\\n\"\n",
    "    ),\n",
    "    role=MessageRole.SYSTEM,\n",
    ")\n",
    "\n",
    "# ツリー要約プロンプトメッセージ\n",
    "TREE_SUMMARIZE_PROMPT_TMPL_MSGS = [\n",
    "            TEXT_QA_SYSTEM_PROMPT,\n",
    "            ChatMessage(\n",
    "                content=(\n",
    "                    \"複数のソースからのコンテキスト情報を以下に示します。\\n\"\n",
    "                    \"---------------------\\n\"\n",
    "                    \"{context_str}\\n\"\n",
    "                    \"---------------------\\n\"\n",
    "                    \"予備知識ではなく、複数のソースからの情報を考慮して質問に答えてください。\\n\"\n",
    "                    \"疑問がある場合は、「情報無し」と答えてください。\\n\"\n",
    "                    \"Query: {query_str}\\n\"\n",
    "                    \"Answer: \"\n",
    "                ),\n",
    "                role=MessageRole.USER,\n",
    "            ),\n",
    "]\n",
    "\n",
    "# ツリー要約プロンプト\n",
    "CHAT_TREE_SUMMARIZE_PROMPT = ChatPromptTemplate(\n",
    "            message_templates=TREE_SUMMARIZE_PROMPT_TMPL_MSGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "            service_context=service_context,\n",
    "            text_qa_template=CHAT_TEXT_QA_PROMPT,  # QAプロンプト\n",
    "            summary_template=CHAT_TREE_SUMMARIZE_PROMPT,  # TreeSummarizeプロンプト\n",
    "            response_mode=\"tree_summarize\",\n",
    "            callback_manager=callback_manager,\n",
    "            use_async=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 0\n",
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.SYNTHESIZE ->  13.234265 seconds\n",
      "      |_CBEventType.TEMPLATING ->  4.4e-05 seconds\n",
      "      |_CBEventType.LLM ->  0.0 seconds\n",
      "      |_CBEventType.LLM ->  0.0 seconds\n",
      "      |_CBEventType.TEMPLATING ->  4.3e-05 seconds\n",
      "      |_CBEventType.LLM ->  0.0 seconds\n",
      "      |_CBEventType.LLM ->  0.0 seconds\n",
      "      |_CBEventType.TEMPLATING ->  4.3e-05 seconds\n",
      "      |_CBEventType.LLM ->  0.0 seconds\n",
      "      |_CBEventType.LLM ->  0.0 seconds\n",
      "      |_CBEventType.EXCEPTION ->  0.0 seconds\n",
      "      |_CBEventType.EXCEPTION ->  0.0 seconds\n",
      "        |_CBEventType.EXCEPTION ->  0.0 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1' coro=<run_async_tasks.<locals>._gather() done, defined at /home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py:36> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_510/2671373537.py\", line 1, in <module>\n",
      "    doc_summary_index = DocumentSummaryIndex.from_documents(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 102, in from_documents\n",
      "    return cls(\n",
      "           ^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 77, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 71, in __init__\n",
      "    index_struct = self.build_index_from_nodes(nodes)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 171, in build_index_from_nodes\n",
      "    return self._build_index_from_nodes(nodes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 175, in _build_index_from_nodes\n",
      "    self._add_nodes_to_index(index_struct, nodes, self._show_progress)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 149, in _add_nodes_to_index\n",
      "    summary_response = self._response_synthesizer.synthesize(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py\", line 128, in synthesize\n",
      "    response_str = self.get_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/tree_summarize.py\", line 131, in get_response\n",
      "    summaries: List[str] = run_async_tasks(tasks)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py\", line 39, in run_async_tasks\n",
      "    outputs: List[Any] = asyncio.run(_gather())\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 36, in run\n",
      "    loop.run_until_complete(task)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 129, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 352, in __wakeup\n",
      "    self.__step(exc)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 205, in step\n",
      "    step_orig(task, exc)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 279, in __step\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py\", line 37, in _gather\n",
      "    return await asyncio.gather(*tasks_to_execute)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 31, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 129, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 205, in step\n",
      "    step_orig(task, exc)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llm_predictor/base.py\", line 186, in apredict\n",
      "    response = await self._llm.acomplete(formatted_prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py\", line 234, in wrapped_async_llm_predict\n",
      "    f_return_val = await f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py\", line 131, in acomplete\n",
      "    return self.complete(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py\", line 277, in wrapped_llm_predict\n",
      "    f_return_val = f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py\", line 63, in complete\n",
      "    output_str = self._llm.predict(prompt, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 871, in predict\n",
      "    return self(text, stop=_stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 831, in __call__\n",
      "    self.generate(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 627, in generate\n",
      "    output = self._generate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 529, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 516, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 1006, in _generate\n",
      "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/huggingface_pipeline.py\", line 167, in _call\n",
      "    response = self.pipeline(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 367, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 165, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1140, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1147, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1046, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 187, in _forward\n",
      "    output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/auto_gptq/modeling/_base.py\", line 443, in generate\n",
      "    return self.model.generate(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1602, in generate\n",
      "    return self.greedy_search(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2511, in greedy_search\n",
      "    if stopping_criteria(input_ids, scores):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py\", line 125, in __call__\n",
      "    @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n",
      "    \n",
      "KeyboardInterrupt\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-5' coro=<run_async_tasks.<locals>._gather() done, defined at /home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py:36> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_510/2671373537.py\", line 1, in <module>\n",
      "    doc_summary_index = DocumentSummaryIndex.from_documents(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 102, in from_documents\n",
      "    return cls(\n",
      "           ^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 77, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 71, in __init__\n",
      "    index_struct = self.build_index_from_nodes(nodes)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 171, in build_index_from_nodes\n",
      "    return self._build_index_from_nodes(nodes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 175, in _build_index_from_nodes\n",
      "    self._add_nodes_to_index(index_struct, nodes, self._show_progress)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 149, in _add_nodes_to_index\n",
      "    summary_response = self._response_synthesizer.synthesize(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py\", line 128, in synthesize\n",
      "    response_str = self.get_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/tree_summarize.py\", line 131, in get_response\n",
      "    summaries: List[str] = run_async_tasks(tasks)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py\", line 39, in run_async_tasks\n",
      "    outputs: List[Any] = asyncio.run(_gather())\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 36, in run\n",
      "    loop.run_until_complete(task)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 129, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 352, in __wakeup\n",
      "    self.__step(exc)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 205, in step\n",
      "    step_orig(task, exc)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 279, in __step\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py\", line 37, in _gather\n",
      "    return await asyncio.gather(*tasks_to_execute)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 31, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 129, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 205, in step\n",
      "    step_orig(task, exc)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llm_predictor/base.py\", line 186, in apredict\n",
      "    response = await self._llm.acomplete(formatted_prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py\", line 234, in wrapped_async_llm_predict\n",
      "    f_return_val = await f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py\", line 131, in acomplete\n",
      "    return self.complete(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py\", line 277, in wrapped_llm_predict\n",
      "    f_return_val = f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py\", line 63, in complete\n",
      "    output_str = self._llm.predict(prompt, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 871, in predict\n",
      "    return self(text, stop=_stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 831, in __call__\n",
      "    self.generate(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 627, in generate\n",
      "    output = self._generate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 529, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 516, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 1006, in _generate\n",
      "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/huggingface_pipeline.py\", line 167, in _call\n",
      "    response = self.pipeline(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 367, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 165, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1140, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1147, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1046, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 187, in _forward\n",
      "    output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/auto_gptq/modeling/_base.py\", line 443, in generate\n",
      "    return self.model.generate(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1602, in generate\n",
      "    return self.greedy_search(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2511, in greedy_search\n",
      "    if stopping_criteria(input_ids, scores):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py\", line 125, in __call__\n",
      "    @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n",
      "    \n",
      "KeyboardInterrupt\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-9' coro=<run_async_tasks.<locals>._gather() done, defined at /home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py:36> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1602, in generate\n",
      "    return self.greedy_search(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2450, in greedy_search\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 820, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 708, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 437, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 220, in forward\n",
      "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 6.00 GiB total capacity; 11.11 GiB already allocated; 0 bytes free; 12.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_510/2671373537.py\", line 1, in <module>\n",
      "    doc_summary_index = DocumentSummaryIndex.from_documents(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 102, in from_documents\n",
      "    return cls(\n",
      "           ^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 77, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 71, in __init__\n",
      "    index_struct = self.build_index_from_nodes(nodes)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 171, in build_index_from_nodes\n",
      "    return self._build_index_from_nodes(nodes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 175, in _build_index_from_nodes\n",
      "    self._add_nodes_to_index(index_struct, nodes, self._show_progress)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 149, in _add_nodes_to_index\n",
      "    summary_response = self._response_synthesizer.synthesize(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py\", line 128, in synthesize\n",
      "    response_str = self.get_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/tree_summarize.py\", line 131, in get_response\n",
      "    summaries: List[str] = run_async_tasks(tasks)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py\", line 39, in run_async_tasks\n",
      "    outputs: List[Any] = asyncio.run(_gather())\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 36, in run\n",
      "    loop.run_until_complete(task)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 129, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 352, in __wakeup\n",
      "    self.__step(exc)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 205, in step\n",
      "    step_orig(task, exc)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 279, in __step\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py\", line 37, in _gather\n",
      "    return await asyncio.gather(*tasks_to_execute)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 31, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 129, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 205, in step\n",
      "    step_orig(task, exc)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llm_predictor/base.py\", line 186, in apredict\n",
      "    response = await self._llm.acomplete(formatted_prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py\", line 234, in wrapped_async_llm_predict\n",
      "    f_return_val = await f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py\", line 131, in acomplete\n",
      "    return self.complete(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py\", line 277, in wrapped_llm_predict\n",
      "    f_return_val = f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py\", line 63, in complete\n",
      "    output_str = self._llm.predict(prompt, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 871, in predict\n",
      "    return self(text, stop=_stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 831, in __call__\n",
      "    self.generate(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 627, in generate\n",
      "    output = self._generate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 529, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 516, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 1006, in _generate\n",
      "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/huggingface_pipeline.py\", line 167, in _call\n",
      "    response = self.pipeline(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 367, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 165, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1140, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1147, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1046, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 187, in _forward\n",
      "    output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/auto_gptq/modeling/_base.py\", line 443, in generate\n",
      "    return self.model.generate(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 114, in decorate_context\n",
      "    with ctx_factory():\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/autograd/grad_mode.py\", line 57, in __exit__\n",
      "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_510/2671373537.py\", line 1, in <module>\n",
      "    doc_summary_index = DocumentSummaryIndex.from_documents(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 102, in from_documents\n",
      "    return cls(\n",
      "           ^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 77, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 71, in __init__\n",
      "    index_struct = self.build_index_from_nodes(nodes)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py\", line 171, in build_index_from_nodes\n",
      "    return self._build_index_from_nodes(nodes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 175, in _build_index_from_nodes\n",
      "    self._add_nodes_to_index(index_struct, nodes, self._show_progress)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/document_summary/base.py\", line 149, in _add_nodes_to_index\n",
      "    summary_response = self._response_synthesizer.synthesize(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py\", line 128, in synthesize\n",
      "    response_str = self.get_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/tree_summarize.py\", line 131, in get_response\n",
      "    summaries: List[str] = run_async_tasks(tasks)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py\", line 39, in run_async_tasks\n",
      "    outputs: List[Any] = asyncio.run(_gather())\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 31, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/nest_asyncio.py\", line 99, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 279, in __step\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/async_utils.py\", line 37, in _gather\n",
      "    return await asyncio.gather(*tasks_to_execute)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llm_predictor/base.py\", line 186, in apredict\n",
      "    response = await self._llm.acomplete(formatted_prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py\", line 234, in wrapped_async_llm_predict\n",
      "    f_return_val = await f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py\", line 131, in acomplete\n",
      "    return self.complete(prompt, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py\", line 277, in wrapped_llm_predict\n",
      "    f_return_val = f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py\", line 63, in complete\n",
      "    output_str = self._llm.predict(prompt, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 871, in predict\n",
      "    return self(text, stop=_stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 831, in __call__\n",
      "    self.generate(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 627, in generate\n",
      "    output = self._generate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 529, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 516, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/base.py\", line 1006, in _generate\n",
      "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/langchain/llms/huggingface_pipeline.py\", line 167, in _call\n",
      "    response = self.pipeline(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 367, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 165, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1140, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1147, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1046, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 187, in _forward\n",
      "    output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/auto_gptq/modeling/_base.py\", line 443, in generate\n",
      "    return self.model.generate(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1602, in generate\n",
      "    return self.greedy_search(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2450, in greedy_search\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 820, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 708, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 437, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 220, in forward\n",
      "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/activations.py\", line 150, in forward\n",
      "    return nn.functional.silu(input)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 2059, in silu\n",
      "    return torch._C._nn.silu(input)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 6.00 GiB total capacity; 11.20 GiB already allocated; 0 bytes free; 12.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1160, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/stack_data/utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/executing/executing.py\", line 264, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/executing/executing.py\", line 183, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/executing/executing.py\", line 212, in for_filename\n",
      "    return cls._for_filename_and_lines(filename, tuple(lines))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/executing/executing.py\", line 223, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paper_translator/.venv/lib/python3.11/site-packages/executing/executing.py\", line 163, in __init__\n",
      "    self.tree = ast.parse(self.text, filename=filename)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "SystemError: AST constructor recursion depth mismatch (before=129, after=185)\n"
     ]
    }
   ],
   "source": [
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    summary_query=\"提供されたテキストの内容を要約してください。\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
