{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自作メタデータの検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. モデルをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "drive_path = \"/home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eda189490cd49f09dbc241efe2f29f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_path = snapshot_download(repo_id=\"mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k\", local_dir=drive_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/paper_translator/lib',\n",
      " '/home/paper_translator',\n",
      " '/usr/lib/python311.zip',\n",
      " '/usr/lib/python3.11',\n",
      " '/usr/lib/python3.11/lib-dynload',\n",
      " '',\n",
      " '/home/paper_translator/.venv/lib/python3.11/site-packages',\n",
      " '/home/paper_translator/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pprint import pprint\n",
    "sys.path.append(\"/home/paper_translator/\")\n",
    "pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# ログレベルの設定\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "llmama_debug_handler = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llmama_debug_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ドキュメントの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/home/paper_translator/.venv/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_verify_locations cafile='/home/paper_translator/.venv/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:slack_bolt.App:Sending a request - url: https://www.slack.com/api/auth.test, query_params: {}, body_params: {}, files: {}, json_body: None, headers: {'Content-Type': 'application/x-www-form-urlencoded', 'Authorization': '(redacted)', 'User-Agent': 'Bolt/1.18.0 Python/3.11.5 slackclient/3.23.0 Linux/5.15.123.1-microsoft-standard-WSL2'}\n",
      "DEBUG:slack_bolt.App:Received the following response - status: 200, headers: {'date': 'Mon, 09 Oct 2023 03:55:59 GMT', 'server': 'Apache', 'vary': 'Accept-Encoding', 'x-slack-req-id': 'f29ff8797f41377afce59ac2a4000dcd', 'x-content-type-options': 'nosniff', 'x-xss-protection': '0', 'pragma': 'no-cache', 'cache-control': 'private, no-cache, no-store, must-revalidate', 'expires': 'Sat, 26 Jul 1997 05:00:00 GMT', 'content-type': 'application/json; charset=utf-8', 'x-oauth-scopes': 'chat:write,app_mentions:read,channels:history,chat:write.customize,chat:write.public,commands,groups:history,mpim:history,im:history', 'access-control-expose-headers': 'x-slack-req-id, retry-after', 'access-control-allow-headers': 'slack-route, x-slack-version-ts, x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, x-b3-flags', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'referrer-policy': 'no-referrer', 'x-slack-unique-id': 'ZSN5zxeN_LXnFxLaSvCqwwAAECs', 'x-slack-backend': 'r', 'access-control-allow-origin': '*', 'content-length': '194', 'via': '1.1 slack-prod.tinyspeck.com, envoy-www-canary-iad-buuckzqf, envoy-edge-nrt-rjkvypxq', 'x-envoy-attempt-count': '1', 'x-envoy-upstream-service-time': '201', 'x-backend': 'main_normal main_canary_with_overflow main_control_with_overflow', 'x-server': 'slack-www-hhvm-main-iad-vaqs', 'x-slack-shared-secret-outcome': 'no-match', 'x-edge-backend': 'envoy-www', 'x-slack-edge-shared-secret-outcome': 'no-match', 'connection': 'close'}, body: {\"ok\":true,\"url\":\"https:\\/\\/work-dlf7572.slack.com\\/\",\"team\":\"Work\",\"user\":\"papertranslator\",\"team_id\":\"T04HAF9RVQD\",\"user_id\":\"U05Q1615Y1X\",\"bot_id\":\"B05PNDXBT7D\",\"is_enterprise_install\":false}\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from src.XMLUtils import DocumentReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/paper_translator/data\"\n",
    "document_name = (\n",
    "    \"Learning_Transferable_Visual_Models_From_Natural_Language_Supervision\"\n",
    ")\n",
    "document_path = f\"{base_path}/documents/{document_name}\"\n",
    "xml_path = f\"{document_path}/{document_name}.tei.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.readers.file.base:> [SimpleDirectoryReader] Total files added: 1\n",
      "documents_1 metadata: \n",
      "{'page_label': '1', 'file_name': 'Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Llama Index で提供されている SimpleDirectoryReader を使用して、\n",
    "# ディレクトリ内の PDF ファイルをDocumentオブジェクトとして読み込む\n",
    "required_exts = [\".pdf\"]\n",
    "docs_1 = SimpleDirectoryReader(\n",
    "    input_dir=document_path, required_exts=required_exts, recursive=True\n",
    ").load_data()\n",
    "print(f\"documents_1 metadata: \\n{docs_1[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paper_translator/.venv/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents_2 metadata: \n",
      "{'Section No.': '1.', 'Section Title': 'Introduction and Motivating Work', 'Title': 'Learning Transferable Visual Models From Natural Language Supervision', 'All_Document_Summary': '\\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.\\n', 'Idno': 'arXiv:2103.00020v1[cs.CV]', 'Language': 'en', 'Published': '26 Feb 2021', 'Authors': ['Alec Radford', 'Jong Wook Kim', 'Chris Hallacy', 'Aditya Ramesh', 'Gabriel Goh', 'Sandhini Agarwal', 'Girish Sastry', 'Amanda Askell', 'Pamela Mishkin', 'Jack Clark', 'Gretchen Krueger', 'Ilya Sutskever']}\n"
     ]
    }
   ],
   "source": [
    "# 自作の DirectoryReader を使用して、\n",
    "# ディレクトリ内の xml ファイルをDocumentオブジェクトとして読み込む\n",
    "# run_grobid(dir_path, pdf_name)\n",
    "docs_2 = DocumentReader().load_data(xml_path)\n",
    "print(f\"documents_2 metadata: \\n{docs_2[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Context の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from src.translator.llama_cpp import create_llama_cpp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1660 Ti, compute capability 7.5\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 45043,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:            blk.3.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:            blk.4.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:            blk.7.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:            blk.9.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:           blk.10.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:           blk.11.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:           blk.14.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:           blk.15.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.17.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:           blk.21.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:           blk.22.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 45043,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:       general.source.hugginface.repository str     \n",
      "llama_model_loader: - kv   3:                   llama.tensor_data_layout str     \n",
      "llama_model_loader: - kv   4:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   5:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   6:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   8:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:               general.quantization_version u32     \n",
      "llama_model_loader: - kv  20:                          general.file_type u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 45043\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.85 B\n",
      "llm_load_print_meta: model size       = 3.87 GiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name   = ELYZA-japanese-Llama-2-7b-fast-instruct\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  = 3961.79 MB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/35 layers to GPU\n",
      "llm_load_tensors: VRAM used: 0.00 MB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  = 1950.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 281.25 MB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 275.37 MB\n",
      "llama_new_context_with_model: total VRAM used: 275.37 MB (model: 0.00 MB, context: 275.37 MB)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/paper_translator/data/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf\"\n",
    "llm = create_llama_cpp_model(package_name=\"llama_index\", model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import MetadataMode\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.node_parser.extractors import (\n",
    "    MetadataExtractor, \n",
    "    SummaryExtractor, \n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    EntityExtractor\n",
    ")\n",
    "\n",
    "from llama_index.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 3072\n",
    "text_splitter = TokenTextSplitter(separator=' ', chunk_size=chunk_size, chunk_overlap=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /tomaarsen/span-marker-mbert-base-multinerd/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140716592797008 on /root/.cache/huggingface/hub/models--tomaarsen--span-marker-mbert-base-multinerd/blobs/c83d88707f975691482aeb63020ef15f18f4f366.lock\n",
      "DEBUG:filelock:Lock 140716592797008 acquired on /root/.cache/huggingface/hub/models--tomaarsen--span-marker-mbert-base-multinerd/blobs/c83d88707f975691482aeb63020ef15f18f4f366.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /tomaarsen/span-marker-mbert-base-multinerd/resolve/main/config.json HTTP/1.1\" 200 5083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfd32f7007347d2af352cd2ced37e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/5.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to release lock 140716592797008 on /root/.cache/huggingface/hub/models--tomaarsen--span-marker-mbert-base-multinerd/blobs/c83d88707f975691482aeb63020ef15f18f4f366.lock\n",
      "DEBUG:filelock:Lock 140716592797008 released on /root/.cache/huggingface/hub/models--tomaarsen--span-marker-mbert-base-multinerd/blobs/c83d88707f975691482aeb63020ef15f18f4f366.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /tomaarsen/span-marker-mbert-base-multinerd/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /tomaarsen/span-marker-mbert-base-multinerd/resolve/main/model.safetensors HTTP/1.1\" 302 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140706966861008 on /root/.cache/huggingface/hub/models--tomaarsen--span-marker-mbert-base-multinerd/blobs/982693e472885f3bc1908b5a056c128fe69f1090deb8395df94167ba3eba1f04.lock\n",
      "DEBUG:filelock:Lock 140706966861008 acquired on /root/.cache/huggingface/hub/models--tomaarsen--span-marker-mbert-base-multinerd/blobs/982693e472885f3bc1908b5a056c128fe69f1090deb8395df94167ba3eba1f04.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn-lfs.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn-lfs.huggingface.co:443 \"GET /repos/98/56/98563f419139a1defc94889f9ae338f9246c06cccf567875f367b31bac29ce81/982693e472885f3bc1908b5a056c128fe69f1090deb8395df94167ba3eba1f04?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1697086534&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzA4NjUzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85OC81Ni85ODU2M2Y0MTkxMzlhMWRlZmM5NDg4OWY5YWUzMzhmOTI0NmMwNmNjY2Y1Njc4NzVmMzY3YjMxYmFjMjljZTgxLzk4MjY5M2U0NzI4ODVmM2JjMTkwOGI1YTA1NmMxMjhmZTY5ZjEwOTBkZWI4Mzk1ZGY5NDE2N2JhM2ViYTFmMDQ~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=hIC8gmYi6JBEwyFonIBGNg~dGofvTrwck1XLJsFXhpJwDlaH7ePBdKYb9Nf3iMMF2~bvPrOtLbe279aH8QBHfFLa4QZ0eLCEEABpuYTLLfCnot5kOLRG50OHmHFxYNWh2yR5f6C1NcSh01K8hqRNmwm92q6MTTqGvvAYWESyPhxuDGnmWivS~B0kC-pOSqAeQ5yBoebQBx6XbfHA6l-CnmOhLKU7qATIO9JV9faOCY5VaG4ToOZnt2ZtQU6OyWXGn~TITIzBg6wL4BG8soIhc1sSerTkPAWYMHk6cOkVN3TinHuw2hFMmUehe5XSZxzFUTMeVRQTtbSh6HFiLcdoHw__&Key-Pair-Id=KVTP0A1DKRTAX HTTP/1.1\" 200 711546600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b111dce6ff7485e8d849899937ce235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/712M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to release lock 140706966861008 on /root/.cache/huggingface/hub/models--tomaarsen--span-marker-mbert-base-multinerd/blobs/982693e472885f3bc1908b5a056c128fe69f1090deb8395df94167ba3eba1f04.lock\n",
      "DEBUG:filelock:Lock 140706966861008 released on /root/.cache/huggingface/hub/models--tomaarsen--span-marker-mbert-base-multinerd/blobs/982693e472885f3bc1908b5a056c128fe69f1090deb8395df94167ba3eba1f04.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140706968138064 on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/b122e74db13b415ea824c074da33c1c44f0d13a3.lock\n",
      "DEBUG:filelock:Lock 140706968138064 acquired on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/b122e74db13b415ea824c074da33c1c44f0d13a3.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324503493d5c426084349d8f59d919ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to release lock 140706968138064 on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/b122e74db13b415ea824c074da33c1c44f0d13a3.lock\n",
      "DEBUG:filelock:Lock 140706968138064 released on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/b122e74db13b415ea824c074da33c1c44f0d13a3.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140706968527312 on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/e3c6d456fb2616f01a9a6cd01a1be1a36353ed22.lock\n",
      "DEBUG:filelock:Lock 140706968527312 acquired on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/e3c6d456fb2616f01a9a6cd01a1be1a36353ed22.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /bert-base-multilingual-cased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01089faac13430883d6f707a6a30a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to release lock 140706968527312 on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/e3c6d456fb2616f01a9a6cd01a1be1a36353ed22.lock\n",
      "DEBUG:filelock:Lock 140706968527312 released on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/e3c6d456fb2616f01a9a6cd01a1be1a36353ed22.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140706306012624 on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/e837bab60a5d204e29622d127c2dafe508aa0731.lock\n",
      "DEBUG:filelock:Lock 140706306012624 acquired on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/e837bab60a5d204e29622d127c2dafe508aa0731.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /bert-base-multilingual-cased/resolve/main/vocab.txt HTTP/1.1\" 200 995526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143393683aca4443a043e8d6e0480969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to release lock 140706306012624 on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/e837bab60a5d204e29622d127c2dafe508aa0731.lock\n",
      "DEBUG:filelock:Lock 140706306012624 released on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/e837bab60a5d204e29622d127c2dafe508aa0731.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/tokenizer.json HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140706306012624 on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/420b0fc31334c64ddf53cc3e9222a6d4c59d0cae.lock\n",
      "DEBUG:filelock:Lock 140706306012624 acquired on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/420b0fc31334c64ddf53cc3e9222a6d4c59d0cae.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /bert-base-multilingual-cased/resolve/main/tokenizer.json HTTP/1.1\" 200 1961828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f54feefabb46788fba5d61504a914a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to release lock 140706306012624 on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/420b0fc31334c64ddf53cc3e9222a6d4c59d0cae.lock\n",
      "DEBUG:filelock:Lock 140706306012624 released on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/420b0fc31334c64ddf53cc3e9222a6d4c59d0cae.lock\n",
      "DEBUG:filelock:Lock 140706306012624 released on /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/blobs/420b0fc31334c64ddf53cc3e9222a6d4c59d0cae.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n"
     ]
    }
   ],
   "source": [
    "metadata_extractor = MetadataExtractor(\n",
    "    extractors=[\n",
    "        #TitleExtractor(llm=llm),\n",
    "        QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "        #SummaryExtractor(llm=llm, summaries=[\"prev\", \"self\"]),\n",
    "        KeywordExtractor(llm=llm),\n",
    "        EntityExtractor(prediction_threshold=0.5)\n",
    "    ],\n",
    "    in_place=False,\n",
    ")\n",
    "node_parser = SimpleNodeParser(text_splitter=text_splitter, metadata_extractor=metadata_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-l6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "embed_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    node_parser=node_parser,\n",
    "    callback_manager=callback_manager,\n",
    "    chunk_size=chunk_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Index の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaryクエリ\n",
    "SUMMARY_QUERY = \"提供されたテキストの内容を要約してください。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "# from llama_index import VectorStoreIndex\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "from llama_index.llms.base import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "# レスポンスシンセサイザーの準備\n",
    "#response_synthesizer = get_response_synthesizer(\n",
    "#    response_mode=\"tree_summarize\",\n",
    "#    use_async=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentSummaryIndexの準備\n",
    "vector_store_index_1 = VectorStoreIndex.from_documents(\n",
    "    docs_1,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) QAプロンプトの定義。\n",
    "# QAシステムプロンプト\n",
    "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n",
    "    content=(\n",
    "        \"あなたは世界中で信頼されているQAシステムです。\\n\"\n",
    "        \"事前知識ではなく、常に提供されたコンテキスト情報を使用してクエリに回答してください。\\n\"\n",
    "        \"従うべきいくつかのルール:\\n\"\n",
    "        \"1. 回答内で指定されたコンテキストを直接参照しないでください。\\n\"\n",
    "        \"2. 「コンテキストに基づいて、...」や「コンテキスト情報は...」、またはそれに類するような記述は避けてください。\"\n",
    "    ),\n",
    "    role=MessageRole.SYSTEM,\n",
    ")\n",
    "\n",
    "# QAプロンプトテンプレートメッセージ\n",
    "TEXT_QA_PROMPT_TMPL_MSGS = [\n",
    "    TEXT_QA_SYSTEM_PROMPT,\n",
    "    ChatMessage(\n",
    "        content=(\n",
    "            \"コンテキスト情報は以下のとおりです。\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"事前知識ではなくコンテキスト情報を考慮して、クエリに答えます。\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        ),\n",
    "        role=MessageRole.USER,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# チャットQAプロンプト\n",
    "CHAT_TEXT_QA_PROMPT = ChatPromptTemplate(message_templates=TEXT_QA_PROMPT_TMPL_MSGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) TreeSummarizeプロンプトの定義。\n",
    "# QAシステムプロンプト\n",
    "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n",
    "    content=(\n",
    "        \"あなたは世界中で信頼されているQAシステムです。\\n\"\n",
    "        \"事前知識ではなく、常に提供されたコンテキスト情報を使用してクエリに回答してください。\\n\"\n",
    "        \"従うべきいくつかのルール:\\n\"\n",
    "        \"1. 回答内で指定されたコンテキストを直接参照しないでください。\\n\"\n",
    "        \"2. 「コンテキストに基づいて、...」や「コンテキスト情報は...」、またはそれに類するような記述は避けてください。\"\n",
    "    ),\n",
    "    role=MessageRole.SYSTEM,\n",
    ")\n",
    "\n",
    "# ツリー要約プロンプトメッセージ\n",
    "TREE_SUMMARIZE_PROMPT_TMPL_MSGS = [\n",
    "    TEXT_QA_SYSTEM_PROMPT,\n",
    "    ChatMessage(\n",
    "        content=(\n",
    "            \"複数のソースからのコンテキスト情報を以下に示します。\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"予備知識ではなく、複数のソースからの情報を考慮して、質問に答えます。\\n\"\n",
    "            \"疑問がある場合は、「情報無し」と答えてください。\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        ),\n",
    "        role=MessageRole.USER,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ツリー要約プロンプト\n",
    "CHAT_TREE_SUMMARIZE_PROMPT = ChatPromptTemplate(\n",
    "    message_templates=TREE_SUMMARIZE_PROMPT_TMPL_MSGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage Context の作成\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore = SimpleDocumentStore(),\n",
    "    vector_store = SimpleVectorStore(),\n",
    "    index_store = SimpleIndexStore()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# 非同期処理の有効化\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# レスポンスシンセサイザーの準備\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    "    text_qa_template=CHAT_TEXT_QA_PROMPT,  # QAプロンプト\n",
    "    summary_template=CHAT_TREE_SUMMARIZE_PROMPT,  # TreeSummarizeプロンプト\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Pre-training methods which learn directly from ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: At the core of our approach is the idea of lear...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Existing work has mainly used three datasets, M...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: State-of-the-art computer vision systems use ve...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We consider two different architectures for the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We train a series of 5 ResNets and 3 Vision Tra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 3.1. Zero-Shot Transfer\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In computer vision, zero-shot learning usually ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP is pre-trained to predict if an image and ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In . CLIP improves performance on all three dat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Most standard image classification datasets tre...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Since task-agnostic zero-shot classifiers for c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: While we have extensively analyzed the task-lea...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In 2015, it was announced that a deep learning ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: How does CLIP compare to human performance and ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A concern with pre-training on a very large int...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: There are still many limitations to CLIP. While...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP has a wide range of capabilities due to it...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CLIP has a wide range of capabilities due to it...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Race\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Algorithmic decisions, training data, and choic...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We next sought to characterize model performanc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This preliminary analysis is intended to illust...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Any model that leverages written, spoken, signe...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72dfc4d70f7e4b6a81ee303e1e7d84d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting questions:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "**********\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/paper_translator/lib/prompt_test2.ipynb セル 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prompt_test2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# DocumentSummaryIndexの準備\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prompt_test2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m doc_summary_index \u001b[39m=\u001b[39m DocumentSummaryIndex\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prompt_test2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     docs_2,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prompt_test2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prompt_test2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prompt_test2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     response_synthesizer\u001b[39m=\u001b[39;49mresponse_synthesizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prompt_test2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     summary_query\u001b[39m=\u001b[39;49mSUMMARY_QUERY,  \u001b[39m# 要約クエリ\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f72222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22643a5c5c4d795f70726f6772616d696e675c5c70617065725f7472616e736c61746f725c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f642533412f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f643a2f4d795f70726f6772616d696e672f70617065725f7472616e736c61746f722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/home/paper_translator/lib/prompt_test2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/indices/base.py:98\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents:\n\u001b[1;32m     97\u001b[0m     docstore\u001b[39m.\u001b[39mset_document_hash(doc\u001b[39m.\u001b[39mget_doc_id(), doc\u001b[39m.\u001b[39mhash)\n\u001b[0;32m---> 98\u001b[0m nodes \u001b[39m=\u001b[39m service_context\u001b[39m.\u001b[39;49mnode_parser\u001b[39m.\u001b[39;49mget_nodes_from_documents(\n\u001b[1;32m     99\u001b[0m     documents, show_progress\u001b[39m=\u001b[39;49mshow_progress\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    103\u001b[0m     nodes\u001b[39m=\u001b[39mnodes,\n\u001b[1;32m    104\u001b[0m     storage_context\u001b[39m=\u001b[39mstorage_context,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/node_parser/simple.py:104\u001b[0m, in \u001b[0;36mSimpleNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents, show_progress)\u001b[0m\n\u001b[1;32m    101\u001b[0m         all_nodes\u001b[39m.\u001b[39mextend(nodes)\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         all_nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata_extractor\u001b[39m.\u001b[39;49mprocess_nodes(all_nodes)\n\u001b[1;32m    106\u001b[0m     event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: all_nodes})\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m all_nodes\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/node_parser/extractors/metadata_extractors.py:120\u001b[0m, in \u001b[0;36mMetadataExtractor.process_nodes\u001b[0;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys)\u001b[0m\n\u001b[1;32m    118\u001b[0m     new_nodes \u001b[39m=\u001b[39m [deepcopy(node) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m nodes]\n\u001b[1;32m    119\u001b[0m \u001b[39mfor\u001b[39;00m extractor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextractors:\n\u001b[0;32m--> 120\u001b[0m     cur_metadata_list \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39;49mextract(new_nodes)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m idx, node \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(new_nodes):\n\u001b[1;32m    122\u001b[0m         node\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mupdate(cur_metadata_list[idx])\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/node_parser/extractors/metadata_extractors.py:384\u001b[0m, in \u001b[0;36mQuestionsAnsweredExtractor.extract\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    382\u001b[0m context_str \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mget_content(metadata_mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata_mode)\n\u001b[1;32m    383\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate(template\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_template)\n\u001b[0;32m--> 384\u001b[0m questions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    385\u001b[0m     prompt, num_questions\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquestions, context_str\u001b[39m=\u001b[39;49mcontext_str\n\u001b[1;32m    386\u001b[0m )\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_only:\n\u001b[1;32m    389\u001b[0m     node\u001b[39m.\u001b[39mexcluded_llm_metadata_keys \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mquestions_this_excerpt_can_answer\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llm_predictor/base.py:149\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    147\u001b[0m     formatted_prompt \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[1;32m    148\u001b[0m     formatted_prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_prompt(formatted_prompt)\n\u001b[0;32m--> 149\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mcomplete(formatted_prompt)\n\u001b[1;32m    150\u001b[0m     output \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[1;32m    152\u001b[0m logger\u001b[39m.\u001b[39mdebug(output)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/base.py:277\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m    268\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    269\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    270\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m         },\n\u001b[1;32m    275\u001b[0m     )\n\u001b[0;32m--> 277\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    279\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponseGen:\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_index/llms/llama_cpp.py:216\u001b[0m, in \u001b[0;36mLlamaCPP.complete\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_formatted:\n\u001b[1;32m    214\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompletion_to_prompt(prompt)\n\u001b[0;32m--> 216\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(prompt\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m CompletionResponse(text\u001b[39m=\u001b[39mresponse[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], raw\u001b[39m=\u001b[39mresponse)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_cpp/llama.py:1491\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1446\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1447\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     grammar: Optional[LlamaGrammar] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1468\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Completion, Iterator[CompletionChunk]]:\n\u001b[1;32m   1469\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \n\u001b[1;32m   1471\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[39m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_completion(\n\u001b[1;32m   1492\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m   1493\u001b[0m         suffix\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m   1494\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m   1495\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m   1496\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m   1497\u001b[0m         logprobs\u001b[39m=\u001b[39;49mlogprobs,\n\u001b[1;32m   1498\u001b[0m         echo\u001b[39m=\u001b[39;49mecho,\n\u001b[1;32m   1499\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m   1500\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1501\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1502\u001b[0m         repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1503\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m   1504\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1505\u001b[0m         tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m   1506\u001b[0m         mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m   1507\u001b[0m         mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m   1508\u001b[0m         mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1509\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1510\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1511\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1512\u001b[0m         grammar\u001b[39m=\u001b[39;49mgrammar,\n\u001b[1;32m   1513\u001b[0m     )\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_cpp/llama.py:1442\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1441\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1442\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_cpp/llama.py:991\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m    989\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    990\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 991\u001b[0m \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    992\u001b[0m     prompt_tokens,\n\u001b[1;32m    993\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    994\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    995\u001b[0m     temp\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    996\u001b[0m     tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m    997\u001b[0m     mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m    998\u001b[0m     mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m    999\u001b[0m     mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1000\u001b[0m     frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1001\u001b[0m     presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1002\u001b[0m     repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1003\u001b[0m     stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1004\u001b[0m     logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1005\u001b[0m     grammar\u001b[39m=\u001b[39;49mgrammar,\n\u001b[1;32m   1006\u001b[0m ):\n\u001b[1;32m   1007\u001b[0m     \u001b[39mif\u001b[39;49;00m token \u001b[39m==\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_token_eos:\n\u001b[1;32m   1008\u001b[0m         text \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_cpp/llama.py:806\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    803\u001b[0m     grammar\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    805\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 806\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    807\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    808\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    809\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    819\u001b[0m         grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m     \u001b[39mif\u001b[39;00m stopping_criteria \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    822\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    823\u001b[0m     ):\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_cpp/llama.py:534\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    532\u001b[0m n_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids))\n\u001b[1;32m    533\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[0;32m--> 534\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    535\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    536\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    537\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mn_tokens,\n\u001b[1;32m    538\u001b[0m     n_past\u001b[39m=\u001b[39;49mn_past,\n\u001b[1;32m    539\u001b[0m )\n\u001b[1;32m    540\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    541\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/paper_translator/.venv/lib/python3.11/site-packages/llama_cpp/llama_cpp.py:999\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m    994\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    995\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[1;32m    996\u001b[0m     n_tokens: Union[c_int, \u001b[39mint\u001b[39m],\n\u001b[1;32m    997\u001b[0m     n_past: Union[c_int, \u001b[39mint\u001b[39m],\n\u001b[1;32m    998\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 999\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "# DocumentSummaryIndexの準備\n",
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    docs_2,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    summary_query=SUMMARY_QUERY,  # 要約クエリ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5b4235b10244f2885828a6160c737e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node 841fa9fa-4846-4e44-b537-c7f53e1ac44b] [Similarity score:             0.164076] 5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 ...\n",
      "> [Node 5017b3c1-9fac-4073-b19e-41e2569a855b] [Similarity score:             0.151905] Learning Transferable Visual Models From Natural Language Supervision 46\n",
      "Text Retrieval Image Ret...\n"
     ]
    }
   ],
   "source": [
    "query_engine_1 = vector_store_index_1.as_query_engine(service_context=service_context)\n",
    "response_1 = query_engine_1.query(\"各セクションを翻訳してください。\")\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提供されたテキストには、画像とテキストのペアからなるデータセットを使用して、自然言語による教師付き学習を行う方法についての説明が含まれています。この手法では、画像とキャプションの関連性を予測する事前学習タスクを使用して、画像表現を学習します。その後、学習されたビジュアルコンセプトを参照したり、新しいコンセプトを記述したりするために自然言語が使用されます。この手法は、OCR、動画のアクション認識、地理位置特定、細かいオブジェクト分類など、さまざまなコンピュータビジョンのタスクにおいて、既存のデータセットに特化したトレーニングなしでモデルを転移学習させることができます。また、この手法は、ImageNetなどのトレーニングデータを使用せずに、元のResNet-50と同等の精度を達成することができます。\n"
     ]
    }
   ],
   "source": [
    "print(doc_summary_index.get_document_summary(\"3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_summary_index.index_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "提供されたテキストは、画像とテキストのペアからなるデータセットを使用して、自然言語を介して学習することで、画像表現を学習する方法について説明しています。この方法では、予め定義されたオブジェクトカテゴリの予測に限定されず、より広範な監督情報を利用することができます。具体的には、画像とキャプションの対応関係を予測する事前学習タスクを行い、その後、自然言語を使用して学習した視覚的な概念を参照したり、新しい概念を記述したりすることができます。このアプローチは、OCR、動画のアクション認識、地理位置情報、細かいオブジェクト分類など、さまざまなコンピュータビジョンのタスクにおいて、多くのタスクにおいて非自明な転移性を持ち、完全に監督されたベースラインと競合することができます。\n",
      "1\n",
      "doc_id: {i} is no text.\n",
      "2\n",
      "提供されたテキストによれば、最新のコンピュータビジョンシステムは、事前に定義された一連のオブジェクトカテゴリを予測するように訓練されています。しかし、この制限された形式の教示は、追加のラベル付きデータが必要であるため、一般性と使いやすさが制限されます。代わりに、画像に関する生のテキストから直接学習することは、より広範な教示源を活用する有望な方法です。このアプローチでは、どのキャプションがどの画像に対応するかを予測するという単純な事前トレーニングタスクを使用して、インターネットから収集された4億組の（画像、テキスト）ペアのデータセットを用いて、ゼロからSOTA（State-of-the-art）の画像表現を効率的かつスケーラブルに学習することを示しています。事前トレーニング後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を説明したり）することで、モデルをダウンストリームタスクにゼロショットで転送することができます。このアプローチの性能を評価するために、OCR、ビデオ内のアクション認識、地理位置特定、および多くのタイプの細かいオブジェクト分類などのタスクを含む30以上の既存のコンピュータビジョンデータセットでベンチマークを行っています。このモデルは、ほとんどのタスクにおいて非自明に転送され、データセット固有のトレーニングを必要とせずに完全に監視されたベースラインと競合することが多いです。たとえば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それにトレーニングされた128万のトレーニング例のいずれも使用する必要はありません。\n",
      "3\n",
      "提供されたテキストには、画像とテキストのペアからなるデータセットを使用して、自然言語による教師付き学習を行う方法についての説明が含まれています。この手法では、画像とキャプションの関連性を予測する事前学習タスクを使用して、画像表現を学習します。その後、学習されたビジュアルコンセプトを参照したり、新しいコンセプトを記述したりするために自然言語が使用されます。この手法は、OCR、動画のアクション認識、地理位置特定、細かいオブジェクト分類など、さまざまなコンピュータビジョンのタスクにおいて、既存のデータセットに特化したトレーニングなしでモデルを転移学習させることができます。また、この手法は、ImageNetなどのトレーニングデータを使用せずに、元のResNet-50と同等の精度を達成することができます。\n",
      "4\n",
      "提供されたテキストによれば、最新のコンピュータビジョンシステムは、予め定義された一連のオブジェクトカテゴリを予測するように訓練されています。しかし、この制限された形式の監督は、他の視覚的な概念を指定するために追加のラベル付きデータが必要となり、その汎用性と利用性を制限しています。一方、画像に関する生のテキストから直接学習することは、より広範な監督情報源を活用する有望な代替手段です。この研究では、画像とキャプションのペアを予測するという単純な事前トレーニングタスクを使用して、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを用いて、ゼロからSOTA（State-of-the-art）の画像表現を効率的かつスケーラブルに学習する方法を示しています。事前トレーニング後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルを他のタスクにゼロショットで転送することができます。このアプローチの性能は、OCR、ビデオ内のアクション認識、地理位置特定、および多くのタイプの細粒度オブジェクト分類など、30以上の既存のコンピュータビジョンデータセットでベンチマークを行うことによって評価されています。このモデルは、ほとんどのタスクにおいて非自明に転送され、データセット固有のトレーニングを必要とせずに完全に監督されたベースラインと競合することが多いです。例えば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それにはResNet-50が訓練された128万のトレーニング例のいずれも使用する必要はありません。\n",
      "5\n",
      "提供されたテキストには、画像とテキストのペアから学習することで、高性能な画像表現を獲得する方法についての情報が含まれています。この手法は、予め定義されたオブジェクトカテゴリを予測するために訓練されたコンピュータビジョンシステムの制約を克服し、より一般的で使いやすいものにすることができます。この手法では、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを用いて、キャプションと画像の対応を予測する事前学習タスクを行います。事前学習後は、自然言語を使用して学習された視覚的な概念を参照したり、新しい概念を記述したりすることができます。この手法の性能は、OCR、ビデオ内のアクション認識、地理位置特定、細粒度のオブジェクト分類など、30以上の異なるコンピュータビジョンデータセットでベンチマークを行うことで評価されています。この手法は、ほとんどのタスクにおいて有効であり、データセット固有のトレーニングを必要とせずに完全に監視されたベースラインと競合することがあります。また、ImageNetのゼロショットタスクにおいて、1,280,000のトレーニング例を使用する必要なく、元のResNet-50の精度に匹敵することも示されています。\n",
      "6\n",
      "提供されたテキストによれば、最新のコンピュータビジョンシステムは、予め定義された一連のオブジェクトカテゴリを予測するために訓練されています。しかし、この制限された形式の教示は、追加のラベル付きデータが必要であり、他の視覚的な概念を指定するためには追加のデータが必要です。一方、画像に関する生のテキストから直接学習することは、より広範な教示源を活用する有望な代替手段です。この研究では、どのキャプションがどの画像に対応するかを予測するという単純な事前トレーニングタスクを使用して、インターネットから収集された4億組の（画像、テキスト）ペアのデータセットを用いて、ゼロから最先端の画像表現を効率的かつスケーラブルに学習する方法を示しています。事前トレーニング後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルを他のタスクにゼロショットで転送することができます。このアプローチの性能は、OCR、ビデオ内のアクション認識、地理位置特定、および多くのタイプの細かいオブジェクト分類など、30以上の既存のコンピュータビジョンデータセットでベンチマークを行うことによって評価されています。このモデルは、ほとんどのタスクにおいて非自明な転送が可能であり、データセット固有のトレーニングを必要とせずに完全に監視されたベースラインと競合することが多いです。たとえば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それにはResNet-50が訓練された128万のトレーニング例のいずれも使用する必要はありません。この研究のコードと事前学習済みモデルの重みは、https://github.com/OpenAI/CLIPで公開されています。\n",
      "7\n",
      "提供されたテキストは、自然言語の監督を利用して転移可能な視覚モデルを学習する方法について述べています。従来のコンピュータビジョンシステムは、予め定義された一連のオブジェクトカテゴリを予測するように訓練されていますが、この制約された形式の監督は、他の視覚的な概念を指定するために追加のラベル付きデータが必要となるため、一般性と使いやすさが制限されます。画像に関する生のテキストから直接学習することは、より広範な監督情報源を活用する有望な代替手段です。この研究では、どのキャプションがどの画像に対応するかを予測するという単純な事前学習タスクが、インターネットから収集された4億組の（画像、テキスト）ペアのデータセットを用いて、効率的かつスケーラブルな方法で最先端の画像表現をゼロから学習するための手段であることを示しています。事前学習後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルをダウンストリームのタスクにゼロショットで転移させることができます。この手法の性能を評価するために、OCR、ビデオ内のアクション認識、地理位置特定、および多くのタイプの細かいオブジェクト分類などのタスクを含む30以上の既存のコンピュータビジョンデータセットでベンチマークを行っています。このモデルは、ほとんどのタスクにおいて非自明に転移し、データセット固有のトレーニングを必要とせずに完全に監督されたベースラインと競合することが多いです。たとえば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それにトレーニングされた128万のトレーニング例のいずれも使用する必要はありません。コードと事前学習済みモデルの重みは、https://github.com/OpenAI/CLIPで公開されています。\n",
      "8\n",
      "提供されたテキストには、画像に関する自然言語の監督から転移可能な視覚モデルの学習に関する研究についての情報が含まれています。この研究では、画像とキャプションのペアを使用してモデルを事前学習し、その後、自然言語を使用して学習した視覚概念を参照したり新しい概念を記述したりすることができることが示されています。このアプローチは、OCR、動画のアクション認識、地理位置特定、細かいオブジェクト分類などのさまざまなコンピュータビジョンのタスクにおいて、既存のデータセットに対して非自明な転移性を持ち、完全に教師付きのベースラインと競合することが多いです。また、この研究では、ImageNetのゼロショットで元のResNet-50の精度を一致させることができることも示されています。\n",
      "9\n",
      "提供されたテキストによれば、最新のコンピュータビジョンシステムは、予め定義された一連のオブジェクトカテゴリを予測するように訓練されています。しかし、この制限された形式の教示は、追加のラベル付きデータが必要であり、他の視覚的な概念を指定するためには追加のデータが必要です。この制約を克服するために、生のテキストから画像に関する情報を学習することが提案されています。具体的には、画像とキャプションの対応関係を予測するという単純な事前トレーニングタスクを使用して、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを用いて、最先端の画像表現を効率的かつスケーラブルに学習する方法が示されています。この事前トレーニングの後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルを他のタスクに転移させることができます。このアプローチの性能は、OCR、ビデオ内のアクション認識、地理位置特定、および多くのタイプの細かいオブジェクト分類など、30以上の既存のコンピュータビジョンデータセットでベンチマークを行うことによって評価されています。このモデルは、ほとんどのタスクにおいて非自明な転移を実現し、データセット固有のトレーニングを必要とせずに完全に教師付きのベースラインと競合することが多いです。例えば、ImageNetのゼロショットで元のResNet-50の精度に匹敵することができますが、それにトレーニングに使用された128万のトレーニング例のいずれも使用する必要はありません。\n",
      "10\n",
      "提供されたテキストによれば、最新のコンピュータビジョンシステムは、事前に定義された一連のオブジェクトカテゴリを予測するように訓練されています。しかし、この制限された形式の監督は、他の視覚的な概念を指定するために追加のラベル付きデータが必要となり、その汎用性と使いやすさを制限しています。一方、画像に関する生のテキストから直接学習することは、より広範な監督情報源を活用する有望な代替手段です。この研究では、画像とキャプションのペアを予測するという単純な事前トレーニングタスクを使用して、インターネットから収集された4億組の（画像、テキスト）データセットを用いて、ゼロから最先端の画像表現を効率的かつスケーラブルに学習する方法を示しています。事前トレーニング後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルを他のタスクにゼロショットで転送することができます。このアプローチの性能は、OCR、ビデオ内のアクション認識、地理位置特定、およびさまざまなタイプの細粒度オブジェクト分類など、30以上の既存のコンピュータビジョンデータセットでベンチマークを行うことによって評価されています。このモデルは、ほとんどのタスクにおいて非自明な転送が可能であり、データセット固有のトレーニングを必要とせずに完全に監督されたベースラインと競合することが多いです。例えば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それにはResNet-50が訓練された128万のトレーニング例のいずれも使用する必要はありません。\n",
      "11\n",
      "提供されたテキストによれば、最新のコンピュータビジョンシステムは、予め定義された一連のオブジェクトカテゴリを予測するように訓練されています。しかし、この制限された形式の監督は、他の視覚的な概念を指定するために追加のラベル付きデータが必要となるため、一般性と使いやすさが制限されます。画像に関する生のテキストから直接学習することは、より広範な監督情報源を活用する有望な代替手段です。この研究では、どのキャプションがどの画像に対応するかを予測するという単純な事前学習タスクが、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを用いて、ゼロから最先端の画像表現を効率的かつスケーラブルに学習する方法であることが示されています。事前学習後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルを他のタスクにゼロショットで転送することが可能になります。このアプローチの性能は、OCR、ビデオ内のアクション認識、地理位置特定、および多種多様な細分化されたオブジェクト分類などのタスクを含む30以上の既存のコンピュータビジョンデータセットでベンチマークを行うことによって評価されています。このモデルは、多くのタスクにおいて非自明に転送され、データセット固有のトレーニングを必要とせずに完全に監督されたベースラインと競合することがしばしばあります。例えば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それにはResNet-50が訓練された128万のトレーニング例のいずれも使用する必要はありません。\n",
      "12\n",
      "提供されたテキストによると、従来のコンピュータビジョンシステムは、予め定義されたオブジェクトカテゴリの予測を行うように訓練されています。しかし、この制限された形式の監督は、他の視覚的な概念を指定するために追加のラベル付きデータが必要となり、一般性と使いやすさが制限されます。一方、生のテキストから画像に関する情報を学習することは、より広範な監督情報を活用する有望な代替手段です。この研究では、画像とキャプションのペアを用いたシンプルな事前学習タスクを通じて、インターネットから収集された4億のデータセットを用いて画像表現を学習する方法を示しています。この事前学習の後、自然言語を使用して学習された視覚的な概念を参照したり、新しい概念を記述したりすることができるため、モデルを他のタスクに転移させることが可能です。このアプローチの性能は、OCR、動画のアクション認識、地理位置特定、および細かいオブジェクト分類などの30以上の既存のコンピュータビジョンデータセットでベンチマークを行うことによって評価されています。このモデルは、多くのタスクにおいて非自明な転移を実現し、データセット固有のトレーニングを必要とせずに完全に監督されたベースラインと競合することが多いことが示されています。また、ImageNetのトレーニングデータを使用せずに、ImageNet zero-shotにおいて元のResNet-50の精度に匹敵することも示されています。\n",
      "13\n",
      "提供されたテキストによると、最新のコンピュータビジョンシステムは、予め定義された一連のオブジェクトカテゴリを予測するように訓練されています。しかし、この制限された形式の監督は、他の視覚的な概念を指定するために追加のラベル付きデータが必要となり、一般性と使いやすさが制限されます。画像に関する生のテキストから直接学習することは、より広範な監督情報を活用する有望な代替手段です。この研究では、どのキャプションがどの画像に対応するかを予測するという単純な事前学習タスクが、インターネットから収集された4億組の（画像、テキスト）ペアのデータセットを用いて、ゼロから最先端の画像表現を効率的かつスケーラブルに学習する方法であることが示されています。事前学習後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルを他のタスクにゼロショットで転送することが可能になります。このアプローチの性能は、OCR、動画のアクション認識、地理位置特定、および多くのタイプの細かいオブジェクト分類などのタスクをカバーする30以上の既存のコンピュータビジョンデータセットでベンチマークテストを行うことによって評価されています。このモデルは、ほとんどのタスクにおいて非自明に転送され、データセット固有のトレーニングを必要とせずに完全に監督されたベースラインと競合することが多いです。たとえば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それにはResNet-50が訓練に使用した128万のトレーニング例のいずれも使用する必要はありません。この研究では、コードと事前学習済みモデルの重みをhttps://github.com/OpenAI/CLIPで公開しています。\n",
      "14\n",
      "提供されたテキストには、画像とテキストのペアから学習することで、高性能な画像表現を獲得する方法についての研究が述べられています。この研究では、予め定義されたオブジェクトカテゴリを予測するために訓練されたコンピュータビジョンシステムの制約が指摘され、代わりに画像に関するテキスト情報を利用して学習する方法が提案されています。この方法では、画像とキャプションの対応関係を予測する事前学習タスクを行い、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを用いて画像表現を学習します。事前学習後は、自然言語を使用して学習された視覚的な概念を参照したり、新しい概念を記述したりすることができます。このモデルは、OCR、動画中のアクション認識、地理位置特定、細かいオブジェクト分類など、30以上の異なるコンピュータビジョンのタスクにおいて、多くの場合、完全に教師ありのベースラインと競合する性能を示します。また、ImageNetのゼロショットタスクにおいて、1,280,000の訓練例を使用する必要なく、元のResNet-50の精度に匹敵することも示されています。\n",
      "15\n",
      "提供されたテキストによれば、最新のコンピュータビジョンシステムは、予め定義された一連のオブジェクトカテゴリを予測するように訓練されています。しかし、この制限された形式の監督は、追加のラベル付きデータが必要となるため、一般性と使いやすさが制限されます。画像に関する生のテキストから直接学習することは、より広範な監督情報を活用する有望な代替手段です。この研究では、予めトレーニングされたモデルをゼロショットで他のタスクに転用することができることが示されています。さまざまなコンピュータビジョンのタスクにおいて、このアプローチの性能が評価され、多くのタスクで完全に監督されたベースラインと競合することがわかりました。また、このモデルは、ImageNetのデータセットを使用せずに、元のResNet-50と同等の精度を達成することも示されています。\n",
      "16\n",
      "提供されたテキストによれば、最新のコンピュータビジョンシステムは、予め定義された一連のオブジェクトカテゴリを予測するように訓練されています。しかし、この制約された形式の監督は、他の視覚的な概念を指定するために追加のラベル付きデータが必要となり、その汎用性と使いやすさを制限しています。一方、画像に関する生のテキストから直接学習することは、より広範な監督情報源を活用する有望な代替手段です。この研究では、どのキャプションがどの画像に対応するかを予測するという単純な事前トレーニングタスクが、インターネットから収集された4億組の（画像、テキスト）ペアのデータセットを用いて、ゼロから最先端の画像表現を効率的かつスケーラブルに学習する方法であることを示しています。事前トレーニング後、自然言語を使用して学習された視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルを他のタスクにゼロショットで転送することができます。このアプローチの性能は、OCR、ビデオ内のアクション認識、地理位置特定、および多くのタイプの細かいオブジェクト分類など、30以上の既存のコンピュータビジョンデータセットでベンチマークを行うことによって評価されています。このモデルは、ほとんどのタスクにおいて非自明に転送され、データセット固有のトレーニングを必要とせずに完全に監督されたベースラインと競合することが多いです。たとえば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それにトレーニングされた128万のトレーニング例のいずれも使用する必要はありません。この研究のコードと事前トレーニング済みのモデルの重みは、https://github.com/OpenAI/CLIPで公開されています。\n",
      "17\n",
      "提供されたテキストは、画像とテキストのペアから学習することで、高性能な画像表現を獲得することができるという、\"Learning Transferable Visual Models From Natural Language Supervision\"という論文に関する要約です。このアプローチは、事前学習タスクとしてキャプションと画像の対応を予測することで、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを使用して画像表現を学習します。事前学習後、自然言語を使用して学習された視覚的な概念を参照したり、新しい概念を記述したりすることができます。このモデルは、OCR、ビデオ内のアクション認識、地理位置特定、細かいオブジェクト分類など、30以上の異なるコンピュータビジョンのタスクにおいて、非自明な転移性を持ち、データセット固有のトレーニングを必要とせずに完全に教師ありのベースラインと競合することができます。この論文のコードと事前学習済みモデルの重みは、https://github.com/OpenAI/CLIPで公開されています。\n",
      "18\n",
      "提供されたテキストは、画像と自然言語の関連付けを予測することで画像表現を学習する方法について説明しています。この方法は、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを使用して、SOTA（State-of-the-art）の画像表現を効率的かつスケーラブルに学習するためのものです。この学習後、自然言語を使用して学習された視覚的な概念を参照したり、新しい概念を記述したりすることができます。このモデルは、OCR、ビデオ内のアクション認識、地理位置特定、細かいオブジェクト分類などのタスクを含む30以上の既存のコンピュータビジョンデータセットでベンチマークテストされ、多くのタスクにおいて非自明な転送が可能であり、データセット固有のトレーニングを必要とせずに完全に教師ありのベースラインと競合することが示されています。また、このモデルは、既存のトレーニング例を使用せずにImageNetのゼロショットで元のResNet-50の精度に一致することができます。\n",
      "19\n",
      "提供されたテキストは、最新のコンピュータビジョンシステムの制約について説明しています。これらのシステムは、予め定義された一連のオブジェクトカテゴリを予測するために訓練されています。しかし、この制約された形式の監督は、他の視覚的な概念を指定するために追加のラベル付きデータが必要となり、一般性と使いやすさが制限されます。このテキストでは、生のテキストから画像に関する情報を学習することが有望な代替手法として紹介されています。具体的には、画像とキャプションのペアを予測するという単純な事前学習タスクを使用して、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを用いて、SOTA（State-of-the-art）の画像表現を効率的かつスケーラブルに学習する方法が示されています。事前学習後、自然言語を使用して学習した視覚的な概念を参照したり（または新しい概念を記述したり）することで、モデルを他のタスクに転送することができます。このアプローチの性能は、OCR、ビデオ内のアクション認識、地理位置特定、および多くのタイプの細かいオブジェクト分類など、30以上の既存のコンピュータビジョンデータセットでベンチマークを行うことによって評価されています。このモデルは、ほとんどのタスクにおいて非自明に転送され、データセット固有のトレーニングを必要とせずに完全に監督されたベースラインと競合することが多いです。たとえば、ImageNetのゼロショットで元のResNet-50の精度に一致することができますが、それに使用された128万のトレーニング例のいずれも使用する必要はありません。\n",
      "20\n",
      "提供されたテキストは、画像とテキストのペアから学習することで、高性能な画像表現を獲得することができることを示しています。このアプローチは、予め定義されたオブジェクトカテゴリに限定されず、より広範な教師データを利用することができます。また、学習済みモデルは、自然言語を使用して学習された視覚的な概念を参照したり、新しい概念を記述したりすることができます。このモデルは、OCR、動画のアクション認識、地理位置情報、細かいオブジェクト分類など、さまざまなコンピュータビジョンのタスクにおいて、ほとんどのタスクにおいて非常に優れた性能を発揮します。また、このアプローチは、特定のデータセットのトレーニングを必要とせずに、完全に教師付きのベースラインと競合することができます。\n",
      "21\n",
      "提供されたテキストは、画像とテキストのペアから学習することで、高性能な画像表現を獲得する方法についての研究を紹介しています。この研究では、予め定義されたオブジェクトカテゴリを予測するために訓練されたコンピュータビジョンシステムの制約を克服するために、生のテキストから画像に関する情報を学習する方法が提案されています。この方法では、画像とキャプションの対応関係を予測するという単純なタスクを事前訓練として行い、インターネットから収集された4億の（画像、テキスト）ペアのデータセットを使用して画像表現を学習します。事前訓練後は、自然言語を使用して学習された視覚的な概念を参照したり、新しい概念を記述したりすることができます。このモデルは、OCR、動画中のアクション認識、地理位置特定、細かいオブジェクト分類など、30以上の異なるコンピュータビジョンのタスクでベンチマークを行い、ほとんどのタスクにおいてトランスファーラーニングが可能であり、特定のデータセットのトレーニングは必要ありません。この研究では、ImageNetのゼロショットタスクにおいて、元のResNet-50の精度を1.28万のトレーニング例を使用せずに一致させることができました。\n",
      "22\n",
      "提供されたテキストは、画像とテキストのペアから学習することで、高性能な画像表現を獲得する方法についての研究を紹介しています。この研究では、予め定義されたオブジェクトカテゴリを予測するために訓練されたコンピュータビジョンシステムの制約について言及されており、その代わりに画像に関する生のテキスト情報を使用して学習する方法が提案されています。この方法により、モデルはゼロショット転送が可能となり、他のタスクにも適用できることが示されています。さらに、このアプローチは、OCR、ビデオ内のアクション認識、地理位置特定、細かいオブジェクト分類など、さまざまなコンピュータビジョンのタスクにおいて、既存のベンチマークデータセットで競争力のあるパフォーマンスを示すことが報告されています。\n",
      "23\n",
      "提供されたテキストでは、画像とテキストのペアを使用して学習された画像表現モデルについて説明されています。このモデルは、予め定義されたオブジェクトカテゴリを予測するために訓練されたコンピュータビジョンシステムの制約を克服し、より一般的で使いやすいモデルを実現することができます。このモデルは、自然言語を使用して学習された視覚的な概念を参照したり、新しい概念を記述したりすることができます。さまざまなコンピュータビジョンのタスクにおいて、非自明な転送性能を持ち、データセット固有のトレーニングを必要とせずに監視されたベースラインと競合することが示されています。また、このアプローチでは、画像とキャプションの関連性を予測するタスクを事前学習し、その後、自然言語を使用して学習したビジュアルコンセプトを参照したり、新しいコンセプトを記述したりすることができます。このモデルは、OCR、ビデオ内のアクション認識、地理位置情報など、さまざまなコンピュータビジョンのタスクにおいて、一般的には完全に教師ありのベースラインと競争力を持ち、データセット固有のトレーニングを必要としません。また、ImageNetのゼロショットで元のResNet-50の精度に匹敵することも示されています。\n",
      "24\n",
      "doc_id: {i} is no text.\n",
      "25\n",
      "doc_id: {i} is no text.\n",
      "26\n",
      "doc_id: {i} is no text.\n",
      "27\n",
      "doc_id: {i} is no text.\n",
      "28\n",
      "doc_id: {i} is no text.\n",
      "29\n",
      "doc_id: {i} is no text.\n",
      "30\n",
      "doc_id: {i} is no text.\n",
      "31\n",
      "doc_id: {i} is no text.\n",
      "32\n",
      "doc_id: {i} is no text.\n",
      "33\n",
      "doc_id: {i} is no text.\n",
      "34\n",
      "doc_id: {i} is no text.\n",
      "35\n",
      "doc_id: {i} is no text.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc_summary_index.index_id)):\n",
    "    print(f\"{i}\")\n",
    "    try:\n",
    "        print(doc_summary_index.get_document_summary(f\"{i}\"))\n",
    "    except ValueError:\n",
    "        print(f\"doc_id: {i} is no text.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/home/paper_translator/.venv/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:slack_bolt.App:Sending a request - url: https://www.slack.com/api/auth.test, query_params: {}, body_params: {}, files: {}, json_body: None, headers: {'Content-Type': 'application/x-www-form-urlencoded', 'Authorization': '(redacted)', 'User-Agent': 'Bolt/1.18.0 Python/3.11.5 slackclient/3.23.0 Linux/5.15.123.1-microsoft-standard-WSL2'}\n",
      "DEBUG:slack_bolt.App:Received the following response - status: 200, headers: {'date': 'Sun, 08 Oct 2023 08:38:23 GMT', 'server': 'Apache', 'vary': 'Accept-Encoding', 'x-slack-req-id': 'ee8f1ab48d5900c39ba3a01604cda71e', 'x-content-type-options': 'nosniff', 'x-xss-protection': '0', 'pragma': 'no-cache', 'cache-control': 'private, no-cache, no-store, must-revalidate', 'expires': 'Sat, 26 Jul 1997 05:00:00 GMT', 'content-type': 'application/json; charset=utf-8', 'x-oauth-scopes': 'chat:write,app_mentions:read,channels:history,chat:write.customize,chat:write.public,commands,groups:history,mpim:history,im:history', 'access-control-expose-headers': 'x-slack-req-id, retry-after', 'access-control-allow-headers': 'slack-route, x-slack-version-ts, x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, x-b3-flags', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'referrer-policy': 'no-referrer', 'x-slack-unique-id': 'ZSJqf-rUJA18Iuvif7hZ7gAAABc', 'x-slack-backend': 'r', 'access-control-allow-origin': '*', 'content-length': '194', 'via': '1.1 slack-prod.tinyspeck.com, envoy-www-iad-srlkqcww, envoy-edge-nrt-fsovuclf', 'x-envoy-attempt-count': '1', 'x-envoy-upstream-service-time': '193', 'x-backend': 'main_normal main_canary_with_overflow main_control_with_overflow', 'x-server': 'slack-www-hhvm-main-iad-jrnj', 'x-slack-shared-secret-outcome': 'no-match', 'x-edge-backend': 'envoy-www', 'x-slack-edge-shared-secret-outcome': 'no-match', 'connection': 'close'}, body: {\"ok\":true,\"url\":\"https:\\/\\/work-dlf7572.slack.com\\/\",\"team\":\"Work\",\"user\":\"papertranslator\",\"team_id\":\"T04HAF9RVQD\",\"user_id\":\"U05Q1615Y1X\",\"bot_id\":\"B05PNDXBT7D\",\"is_enterprise_install\":false}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from xml.etree.ElementTree import Element\n",
    "\n",
    "from src.OpenAIUtils import get_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section:\n",
    "    def __init__(self, title: str = \"\", body: str = \"\") -> None:\n",
    "        \"\"\"セクションのタイトルと本文を保持するクラス\n",
    "        Args:\n",
    "            title (str, optional): セクションのタイトル. Defaults to \"\".\n",
    "            body (str, optional): セクションの本文. Defaults to \"\".\n",
    "        \"\"\"\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "def get_text(element: Element):\n",
    "    \"\"\"XML要素からテキストを取得\n",
    "    Args:\n",
    "        element (Element): XMLの要素\n",
    "    Returns:\n",
    "        text: テキスト\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    for elem in element.iter():\n",
    "        if elem.text:\n",
    "            text += elem.text\n",
    "        if elem.tail:\n",
    "            text += elem.tail\n",
    "    return text\n",
    "\n",
    "def get_sections(root: Element) -> List[str]:\n",
    "    \"\"\"XMLからセクションを取得する関数\n",
    "    Args:\n",
    "        root (Element): XMLのルート要素\n",
    "    Returns:\n",
    "        sections (List[Section]): セクションのリスト\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    for div in root[1][0]:\n",
    "        section = Section(\"\", \"\")\n",
    "        for element in div:\n",
    "            if element.tag == \"{http://www.tei-c.org/ns/1.0}head\":\n",
    "                section.title = element.text\n",
    "            if element.tag == \"{http://www.tei-c.org/ns/1.0}p\":\n",
    "                section.body += get_text(element=element)\n",
    "\n",
    "        if section.body != \"\":\n",
    "            sections.append(section)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"\n",
    "### 指示 ###\n",
    "論文の内容を理解した上で，重要なポイントを箇条書きで3点書いてください。\n",
    "### 箇条書きの制約 ###\n",
    "- 最大3個\n",
    "- 日本語\n",
    "- 箇条書き1個を100文字以内\n",
    "### 対象とする論文の内容 ###\n",
    "{text}\n",
    "### 出力形式 ###\n",
    "- 箇条書き1\n",
    "- 箇条書き2\n",
    "- 箇条書き3\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\n",
    "### 指示 ###\n",
    "次の文章を翻訳してください。\n",
    "{prompt_text}\n",
    "### 応答: \"\"\"\n",
    "\n",
    "\n",
    "def write_markdown(\n",
    "    sections: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    summarizer,\n",
    "    ) -> str:\n",
    "    \"\"\"Markdownファイルを作成する関数\n",
    "    Args:\n",
    "        sections (List[str]): セクションのリスト\n",
    "        model: GPTQモデル\n",
    "        tokenizer: トークナイザー\n",
    "        summarizer: サマライザー\n",
    "    Returns:\n",
    "        markdown_text (str): Markdownのテキスト\n",
    "    \"\"\"\n",
    "\n",
    "    markdown_text = \"\"\n",
    "    try:\n",
    "        for section in tqdm(sections):\n",
    "            # 144文字以下の場合は、全文を翻訳する\n",
    "            if (len(section.body.split(\" \"))) < 144:\n",
    "                # translated_text = translator(section.body)[0][\"translation_text\"]\n",
    "                prompt_text = section.body\n",
    "            # 144文字以上500文字以下の場合は、要約して翻訳する\n",
    "            elif len(section.body.split(\" \")) < 500:\n",
    "                summary = summarizer(section.body)[0][\"summary_text\"]\n",
    "                # translated_text = translator(summary)[0][\"translation_text\"]\n",
    "                prompt_text = summary\n",
    "            else:\n",
    "                response = get_message(text=section.body, system=SYSTEM)\n",
    "                # translated_text = translator(response)[0][\"translation_text\"]\n",
    "                prompt_text = response\n",
    "\n",
    "            tokens = (\n",
    "                tokenizer(PROMPT_TEMPLATE.format(prompt_text=prompt_text), return_tensors=\"pt\")\n",
    "                .to(\"cuda:0\")\n",
    "                .input_ids\n",
    "            )\n",
    "\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            output = model.generate(\n",
    "                input_ids=tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                max_new_tokens=500,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            translated_text = tokenizer.decode(output[0])\n",
    "            markdown_text += \"\\n\" + translated_text\n",
    "\n",
    "            if \"conclusion\" in section.title.lower():\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return markdown_text\n",
    "    else:\n",
    "        return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kworts/BARTxiv/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kworts/BARTxiv/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kworts/BARTxiv/resolve/main/generation_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1020: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paper_translator/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k/resolve/main/quantize_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k/resolve/main/gptq_model-4bit-128g.safetensors HTTP/1.1\" 302 0\n",
      "INFO:auto_gptq.modeling._base:lm_head not been quantized, will be ignored when make_quant.\n",
      "WARNING:auto_gptq.nn_modules.fused_llama_mlp:skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "model_dir_name=\"mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k\"\n",
    "# トークナイザーとモデルの準備\n",
    "quantized_model_dir = model_dir_name\n",
    "model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"kworts/BARTxiv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    quantized_model_dir, use_fast=True\n",
    ")\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    quantized_model_dir,\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    device=\"cuda:0\",\n",
    "    use_auth_token=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928df25cd021442faa8c041bd1790fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"Pre-training methods which learn directly from raw text have revolutionized NLP over the last few years (Dai & Le, 2015;Peters et al., 2018;Howard & Ruder, 2018;Radford et al., 2018;Devlin et al., 2018;Raffel et al., 2019).Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities. The development of \\\\\"text-to-text\\\\\" as a standardized input-output interface (McCann et al., 2018;Radford et al., 2019;Raffel et al., 2019) has enabled taskagnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization. Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data.These results suggest that the aggregate supervision accessible to modern pre-training methods within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets. However, in other fields such as computer vision it is still standard practice to pre-train models on crowd-labeled datasets such as ImageNet (Deng et al., 2009). Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision? Prior work is encouraging.Over 20 years ago Mori et al. (1999) explored improving content based image retrieval by training a model to predict the nouns and adjectives in text documents paired with images. Quattoni et al. (2007) demonstrated it was possible to learn more data efficient image representations via manifold learning in the weight space of classifiers trained to predict words in captions associated with images. Srivastava & Salakhutdinov (2012) explored deep representation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features. Joulin et al. (2016) modernized this line of work and demonstrated that CNNs trained to predict words in image captions learn useful image representations. They converted the title, description, and hashtag metadata of images in the YFCC100M dataset (Thomee et al., 2016) into a bag-ofwords multi-label classification task and showed that pretraining AlexNet (Krizhevsky et al., 2012) to predict these labels learned representations which preformed similarly to ImageNet-based pre-training on transfer tasks. Li et al. (2017) then extended this approach to predicting phrase ngrams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image classification datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score. Adopting more recent architectures and pre-training approaches, VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-VIRT (Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text.While exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on common benchmarks is much lower than alternative approaches. For example, Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accuracy of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012). Instead, more narrowly scoped but well-targeted uses of weak supervision have improved performance. Mahajan et al. (2018) showed that predicting ImageNet-related hashtags on Instagram images is an effective pre-training task. When fine-tuned to ImageNet these pre-trained models increased accuracy by over 5% and improved the overall state of the art at the time. Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset.This line of work represents the current pragmatic middle ground between learning from a limited amount of supervised \\\\\"gold-labels\\\\\" and learning from practically unlimited amounts of raw text. However, it is not without compro-mises. Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their \\\\\"zero-shot\\\\\" capabilities.A crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale. While Mahajan et al. (2018) and Kolesnikov et al. (2019) trained their models for accelerator years on millions to billions of images, VirTex, ICMLM, and ConVIRT trained for accelerator days on one to two hundred thousand images. In this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision. We study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute and observe that transfer performance is a smoothly predictable function of compute (Hestness et al., 2017;Kaplan et al., 2020). We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find  . CLIP is much more efficient at zero-shot transfer than our image caption baseline. Although highly expressive, we found that transformer-based language models are relatively weak at zero-shot ImageNet classification. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words (BoW) encoding of the text (Joulin et al., 2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efficiency another 4x.it can be competitive with prior task-specific supervised models. We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model\\'s capability. These results have significant policy and ethical implications, which we consider in Section 7.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4737 request_id=920b8c24044546c3f39c9a5cccdd2b60 response_code=200\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4737 request_id=920b8c24044546c3f39c9a5cccdd2b60 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"State-of-the-art computer vision systems use very large amounts of compute. Mahajan et al. (2018) required 19 GPU years to train their ResNeXt101-32x48d and Xie et al. (2020) required 33 TPUv3 core-years to train their Noisy Student EfficientNet-L2. When considering that both these systems were trained to predict only 1000 ImageNet classes, the task of learning an open set of visual concepts from natural language seems daunting. In the course of our efforts, we found training efficiency was key to successfully scaling natural language supervision and we selected our final pre-training method based on this metric.Our initial approach, similar to VirTex, jointly trained an image CNN and text transformer from scratch to predict the caption of an image. However, we encountered difficulties efficiently scaling this method. In Figure 2 we show that a 63 million parameter transformer language model, which already uses twice the compute of its ResNet-50 image encoder, learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-ofwords encoding of the same text.Both these approaches share a key similarity. They try to predict the exact words of the text accompanying each image. This is a difficult task due to the wide variety of descriptions, comments, and related text that co-occur with images. Recent work in contrastive representation learning for images has found that contrastive objectives can learn better representations than their equivalent predictive objective (Tian et al., 2019). Other work has found that although generative models of images can learn high quality image representations, they require over an order of magnitude more compute than contrastive models with the same performance (Chen et al., 2020a). Noting these findings, we explored training a system to solve the potentially easier proxy task of predicting only which text as a whole is paired with which image and not the exact words of that text. Starting with the same bag-of-words encoding baseline, we swapped the predictive objective for a contrastive objective in Figure 2 and observed a further 4x efficiency improvement in the rate of zero-shot transfer to ImageNet.Given a batch of N (image, text) pairs, CLIP is trained to predict which of the N \\\\u00d7 N possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. Finally all WordNet synsets not already in the query list are added. multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N 2 -N incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores. In Figure 3 we include pseudocode of the core of an implementation of CLIP. To our knowledge this batch construction technique and objective was first introduced in the area of deep metric learning as the multi-class N-pair loss Sohn (2016), was popularized for contrastive representation learning by Oord et al. (2018) as the InfoNCE loss, and was recently adapted for contrastive (text, image) representation learning in the domain of medical imaging by Zhang et al. (2020).Due to the large size of our pre-training dataset, over-fitting is not a major concern and the details of training CLIP are simplified compared to the implementation of Zhang et al. (2020). We train CLIP from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights. We do not use the non-linear projection between the representation and the contrastive embedding space, a change which was introduced by Bachman et al. ( 2019) and popularized by Chen et al. (2020b). We instead use only a linear projection to map from each encoder\\'s representation to the multi-modal embedding space. We did not notice a difference in training efficiency between the two versions and speculate that non-linear projections may be co-adapted with details of current image only in self-supervised representation learning methods. We also remove the text transformation function t u from Zhang et al. (2020) which samples a single sentence at uniform from the text since many of the (image, text) pairs in CLIP\\'s pretraining dataset are only a single sentence. We also simplify the image transformation function t v . A random square crop from resized images is the only data augmentation used during training. Finally, the temperature parameter which controls the range of the logits in the softmax, \\\\u03c4 , is directly optimized during training as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5925 request_id=e2a6e80d3540d77fbdf32d53db540d42 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"Most standard image classification datasets treat the information naming or describing classes which enables natural language based zero-shot transfer as an afterthought. The vast majority of datasets annotate images with just a numeric id of the label and contain a file mapping these ids back to their names in English. Some datasets, such as Flowers102 and GTSRB, don\\'t appear to include this mapping at all in their released versions preventing zero-shot transfer entirely.2 For many datasets, we observed these labels may be  (Li et al. 2017) Figure 4. Prompt engineering and ensembling improve zeroshot performance. Compared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classification performance by almost 5 points on average across 36 datasets. This improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is \\\\\"free\\\\\" when amortized over many predictions. chosen somewhat haphazardly and do not anticipate issues related to zero-shot transfer which relies on task description in order to transfer successfully.A common issue is polysemy. When the name of a class is the only information provided to CLIP\\'s text encoder it is unable to differentiate which word sense is meant due to the lack of context. In some cases multiple meanings of the same word might be included as different classes in the same dataset! This happens in ImageNet which contains both construction cranes and cranes that fly. Another example is found in classes of the Oxford-IIIT Pet dataset where the word boxer is, from context, clearly referring to a breed of dog, but to a text encoder lacking context could just as likely refer to a type of athlete.Another issue we encountered is that it\\'s relatively rare in our pre-training dataset for the text paired with the image to be just a single word. Usually the text is a full sentence describing the image in some way. To help bridge this distribution gap, we found that using the prompt template \\\\\"A photo of a {label}.\\\\\" to be a good default that helps specify the text is about the content of the image. This often improves performance over the baseline of using only the label text. For instance, just using this prompt improves accuracy on ImageNet by 1.3%. Similar to the \\\\\"prompt engineering\\\\\" discussion around GPT-3 (Brown et al., 2020;Gao et al., 2020), we have also observed that zero-shot performance can be significantly improved by customizing the prompt text to each task. A few, non exhaustive, examples follow. We found on several fine-grained image classification datasets that it helped to specify the category. For example on Oxford-IIIT Pets, using \\\\\"A photo of a {label}, a type of pet.\\\\\" to help provide context worked well. Likewise, on Food101 specifying a type of food and on FGVC Aircraft a type of aircraft helped too. For OCR datasets, we found that putting quotes around the text or number to be recognized improved performance. Finally, we found that on satellite image classification datasets it helped to specify that the images were of this form and we use variants of \\\\\"a satellite photo of a {label}.\\\\\".We also experimented with ensembling over multiple zeroshot classifiers as another way of improving performance. These classifiers are computed by using different context prompts such as \\'A photo of a big {label}\\\\\" and \\\\\"A photo of a small {label}\\\\\". We construct the ensemble over the embedding space instead of probability space. This allows us to cache a single set of averaged text embeddings so that the compute cost of the ensemble is the same as using a single classifier when amortized over many predictions. We\\'ve observed ensembling across many generated zero-shot classifiers to reliably improve performance and use it for the majority of datasets. On ImageNet, we ensemble 80 different context prompts and this improves performance by an additional 3.5% over the single default prompt discussed above. When considered together, prompt engineering and ensembling improve ImageNet accuracy by almost 5%. In Figure 4 we visualize how prompt engineering and ensembling change the performance of a set of CLIP models compared to the contextless baseline approach of directly embedding the class name as done in Li et al. (2017).\"}]}' message='Post details'\n",
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5928 request_id=951ba703af99a4d7c5e86e61b667b45e response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"Since task-agnostic zero-shot classifiers for computer vision have been understudied, CLIP provides a promising opportunity to gain a better understanding of this type of model. In this section, we conduct a study of various properties of CLIP\\'s zero-shot classifiers. As a first question, we look simply at how well zero-shot classifiers perform. To contextualize this, we compare to the performance of a simple off-the-shelf baseline: fitting a fully supervised, regularized, logistic regression classifier on the features of the canonical ResNet-50. In Figure 5  ten than not and wins on 16 of the 27 datasets. Looking at individual datasets reveals some interesting behavior. On fine-grained classification tasks, we observe a wide spread in performance. On two of these datasets, Stanford Cars and Food101, zero-shot CLIP outperforms logistic regression on ResNet-50 features by over 20% while on two others, Flowers102 and FGVCAircraft, zero-shot CLIP underperforms by over 10%. On OxfordPets and Birdsnap, performance is much closer. We suspect these difference are primarily due to varying amounts of per-task supervision between WIT and ImageNet. On \\\\\"general\\\\\" object classification datasets such as ImageNet, CIFAR10/100, STL10, and PascalVOC2007 performance is relatively similar with a slight advantage for zero-shot CLIP in all cases. On STL10, CLIP achieves 99.3% overall which appears to be a new state of the art despite not using any training examples. Zeroshot CLIP significantly outperforms a ResNet-50 on two datasets measuring action recognition in videos. On Kinet-ics700, CLIP outperforms a ResNet-50 by 14.5%. Zeroshot CLIP also outperforms a ResNet-50\\'s features by 7.7% on UCF101. We speculate this is due to natural language providing wider supervision for visual concepts involving verbs, compared to the noun-centric object supervision in ImageNet.Looking at where zero-shot CLIP notably underperforms, we see that zero-shot CLIP is quite weak on several specialized, complex, or abstract tasks such as satellite image classification (EuroSAT and RESISC45), lymph node tumor detection (PatchCamelyon), counting objects in synthetic scenes (CLEVRCounts), self-driving related tasks such as German traffic sign recognition (GTSRB), recognizing distance to the nearest car (KITTI Distance). These results highlight the poor capability of zero-shot CLIP on more complex tasks. By contrast, non-expert humans can robustly perform several of these tasks, such as counting, satellite image classification, and traffic sign recognition, suggesting significant room for improvement. However, we caution that it is unclear whether measuring zero-shot transfer, as opposed to few-shot transfer, is a meaningful evaluation for difficult tasks that a learner has no prior experience with, such as lymph node tumor classification for almost all humans (and possibly CLIP).While comparing zero-shot performance to fully supervised models contextualizes the task-learning capabilities of CLIP, comparing to few-shot methods is a more direct comparison, since zero-shot is its limit. In Figure 6, we visualize how zero-shot CLIP compares to few-shot logistic regression on the features of many image models including the best publicly available ImageNet models, self-supervised learning methods, and CLIP itself. While it is intuitive to expect zero-shot to underperform one-shot, we instead find that zero-shot CLIP matches the performance of 4-shot logistic regression on the same feature space. This is likely due to an important difference between the zero-shot and few-shot approach. First, CLIP\\'s zero-shot classifier is generated via natural language which allows for visual concepts to be directly specified (\\\\\"communicated\\\\\"). By contrast, \\\\\"normal\\\\\" supervised learning must infer concepts indirectly from training examples. Context-less example-based learning has the drawback that many different hypotheses can be consistent with the data, especially in the one-shot case.A single image often contains many different visual concepts. Although a capable learner is able to exploit visual cues and heuristics, such as assuming that the concept being demonstrated is the primary object in an image, there is no guarantee.A potential resolution of this discrepancy between zeroshot and few-shot performance is to use CLIP\\'s zero-shot classifier as a prior for the weights of the few-shot classifier.While adding an L2 penalty towards the generated weights is a straightforward implementation of this idea, we found that hyperparameter optimization would often select for such a large value of this regularizer that the resulting fewshot classifier was \\\\\"just\\\\\" the zero-shot classifier. Research into better methods of combining the strength of zero-shot transfer with flexibility of few-shot learning is a promising direction for future work.When comparing zero-shot CLIP to few-shot logistic regression on the features of other models, zero-shot CLIP roughly matches the performance of the best performing 16-shot classifier in our evaluation suite, which uses the features of a BiT-M ResNet-152x2 trained on ImageNet-21K.We are certain that a BiT-L model trained on JFT-300M would perform even better but these models have not been publicly released. That a BiT-M ResNet-152x2 performs best in a 16-shot setting is somewhat surprising since, as analyzed in Section 3.2, the Noisy Student EfficientNet-L2 outperforms it in a fully supervised setting by almost 5% on average across 27 datasets.In addition to studying the average performance of zero-shot CLIP and few-shot logistic regression, we also examine performance on individual datasets. In Figure 7, we show estimates for the number of labeled examples per class that a logistic regression classifier on the same feature space requires to match the performance of zero-shot CLIP. Since zero-shot CLIP is also a linear classifier, this estimates the effective data efficiency of zero-shot transfer in this setting.In order to avoid training thousands of linear classifiers, we estimate the effective data efficiency based on a loglinear interpolation of the performance of a 1, 2, 4, 8, 16shot (when possible), and a fully supervised linear classifier trained on each dataset. We find that zero-shot transfer can If we assume that evaluation datasets are large enough that the parameters of linear classifiers trained on them are well estimated, then, because CLIP\\'s zero-shot classifier is also a linear classifier, the performance of the fully supervised classifiers roughly sets an upper bound for what zero-shot transfer can achieve. In Figure 8 we compare CLIP\\'s zeroshot performance with fully supervised linear classifiers across datasets. The dashed, y = x line represents an \\\\\"optimal\\\\\" zero-shot classifier that matches the performance of its fully supervised equivalent. For most datasets, the performance of zero-shot classifiers still underperform fully supervised classifiers by 10% to 25%, suggesting that there is still plenty of headroom for improving CLIP\\'s task-learning and zero-shot transfer capabilities.There is a positive correlation of 0.82 (p-value < 10 -6 ) between zero-shot performance and fully supervised perfor- . Zero-shot performance is correlated with linear probe performance but still mostly sub-optimal. Comparing zero-shot and linear probe performance across datasets shows a strong correlation with zero-shot performance mostly shifted 10 to 25 points lower. On only 5 datasets does zero-shot performance approach linear probe performance (\\\\u22643 point difference). mance, suggesting that CLIP is relatively consistent at connecting underlying representation and task learning to zeroshot transfer. However, zero-shot CLIP only approaches fully supervised performance on 5 datasets: STL10, CI-FAR10, Food101, OxfordPets, and Caltech101. On all 5 datasets, both zero-shot accuracy and fully supervised accuracy are over 90%. This suggests that CLIP may be more effective at zero-shot transfer for tasks where its underlying representations are also high quality. The slope of a linear regression model predicting zero-shot performance as a function of fully supervised performance estimates that for every 1% improvement in fully supervised performance, zero-shot performance improves by 1.28%. However, the 95th-percentile confidence intervals still include values of less than 1 (0.93-1.79).Over the past few years, empirical studies of deep learning systems have documented that performance is predictable as a function of important quantities such as training compute and dataset size (Hestness et al., 2017;Kaplan et al., 2020). The GPT family of models has so far demonstrated consistent improvements in zero-shot performance across a 1000x increase in training compute. In Figure 9, we check whether the zero-shot performance of CLIP follows a similar scaling pattern. We plot the average error rate of the 5 ResNet CLIP models across 39 evaluations on 36 different datasets and find that a similar log-log linear scaling trend holds for CLIP across a 44x increase in model compute. While the overall trend is smooth, we found that performance on individual evaluations can be much noisier. We are unsure whether this is caused by high variance between individual training runs on sub-tasks (as documented in D\\'Amour et al. ( 2020)) masking a steadily improving trend or whether performance is actually non-monotonic as a function of compute on some tasks.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5201 request_id=c9ab3fbe730372dd94e4e5f2e18923ac response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"While we have extensively analyzed the task-learning capabilities of CLIP through zero-shot transfer in the previous section, it is more common to study the representation learning capabilities of a model. There exist many ways to evaluate the quality of representations as well as disagreements over what properties an \\\\\"ideal\\\\\" representation should have (Locatello et al., 2020). Fitting a linear classifier on a representation extracted from the model and measuring its performance on various datasets is a common approach. An alternative is measuring the performance of end-to-end fine-tuning of the model. This increases flexibility, and prior work has convincingly demonstrated that fine-tuning outperforms linear classification on most image classification datasets (Kornblith et al., 2019;Zhai et al., 2019). While the high performance of fine-tuning motivates its study for practical reasons, we still opt for linear classifier based evaluation for several reasons. Our work is focused on developing a high-performing task and dataset-agnostic pre-training approach. Fine-tuning, because it adapts representations to each dataset during the fine-tuning phase, can compensate for and potentially mask failures to learn general and robust representations during the pre-training phase. Linear classifiers, because of their limited flexibility, instead highlight these failures and provide clear feedback during development. For CLIP, training supervised linear classifiers has the added benefit of being very similar to the approach used for its zero-shot classifiers which enables extensive comparisons and analysis in Section 3.1. Finally, we aim to compare CLIP to a comprehensive set of existing models across many tasks. Studying 66 different models on 27 different datasets requires tuning 1782 different evaluations. Fine-tuning opens up a much larger design and hyperparameter space, which makes it difficult to fairly evaluate and computationally expensive to compare a diverse set of techniques as discussed in other large scale empirical studies (Lucic et al., 2018;Choi et al., 2019). By comparison, linear classifiers require minimal hyper-parameter tuning and have standardized implementations and evaluation procedures. Please see Appendix A for further details on evaluation.Figure 10 summarizes our findings. To minimize selection effects that could raise concerns of confirmation or reporting bias, we first study performance on the 12 dataset evaluation suite from Kornblith et al. (2019). While small CLIP models such as a ResNet-50 and ResNet-101 outperform other ResNets trained on ImageNet-1K (BiT-S and the originals), they underperform ResNets trained on ImageNet-21K (BiT-M). These small CLIP models also underperform models in the EfficientNet family with similar compute requirements. However, models trained with CLIP scale very well and the largest model we trained (ResNet-50x64) slightly outperforms the best performing existing model (a Noisy Student EfficientNet-L2) on both overall score and compute efficiency. We also find that CLIP vision transformers are about 3x more compute efficient than CLIP ResNets, which allows us to reach higher overall performance within our compute budget. These results qualitatively replicate the findings of Dosovitskiy et al. (2020) which reported that vision transformers are more compute efficient than convnets when trained on sufficiently large datasets. Our best overall model is a ViT-L/14 that is fine-tuned at a higher resolution of 336 pixels on our dataset for 1 additional epoch. This model outperforms the best existing model across this evaluation suite by an average of 2.6%.As Figure 21 qualitatively shows, CLIP models learn a wider set of tasks than has previously been demonstrated in a single computer vision model trained end-to-end from random initialization. These tasks include geo-localization, optical character recognition, facial emotion recognition, and action recognition. None of these tasks are measured in the evaluation suite of Kornblith et al. (2019). This could be argued to be a form of selection bias in Kornblith et al. (2019)\\'s study towards tasks that overlap with ImageNet. To address this, we also measure performance on a broader 27 dataset evaluation suite. This evaluation suite, detailed in Appendix A includes datasets representing the aforementioned tasks, German Traffic Signs Recognition Benchmark (Stallkamp et al., 2011), as well as several other datasets adapted from VTAB (Zhai et al., 2019). Linear probe average over all 27 datasets. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including EfficientNet (Tan & Le, 2019;Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;Touvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al., 2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019). (Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or evaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset.On this broader evaluation suite, the benefits of CLIP are more clear. All CLIP models, regardless of scale, outperform all evaluated systems in terms of compute efficiency. The improvement in average score of the best model over previous systems increases from 2.6% to 5%. We also find that self-supervised systems do noticeably better on our broader evaluation suite. For instance, while SimCLRv2 still underperforms BiT-M on average on the 12 datasets of Kornblith et al. (2019), SimCLRv2 outperforms BiT-M on our 27 dataset evaluation suite. These findings suggest continuing to expand task diversity and coverage in order to better understand the \\\\\"general\\\\\" performance of systems. We suspect additional evaluation efforts along the lines of VTAB to be valuable.In addition to the aggregate analysis above, we visualize per-dataset differences in the performance of the best CLIP model and the best model in our evaluation suite across all 27 datasets in Figure 11. CLIP outperforms the Noisy Student EfficientNet-L2 on 21 of the 27 datasets. CLIP improves the most on tasks which require OCR (SST2 and HatefulMemes), geo-localization and scene recognition (Country211, SUN397), and activity recognition in videos (Kinetics700 and UCF101). In addition CLIP also does much better on fine-grained car and traffic sign recognition (Stanford Cars and GTSRB). This may reflect a problem with overly narrow supervision in ImageNet. A result such as the 14.7% improvement on GTSRB could be indicative of an issue with ImageNet-1K, which has only a single label for all traffic and street signs. This could encourage a supervised representation to collapse intra-class details and hurt accuracy on a fine-grained downstream task. As mentioned, CLIP still underperforms the EfficientNet on several datasets. Unsurprisingly, the dataset that the Effi-cientNet does best relative to CLIP on is the one it was trained on: ImageNet. The EffcientNet also slightly outperforms CLIP on low-resolution datasets such as CIFAR10 and CIFAR100. We suspect this is at least partly due to the lack of scale-based data augmentation in CLIP. The Effi-cientNet also does slightly better on PatchCamelyon and CLEVRCounts, datasets where overall performance is still low for both approaches.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4655 request_id=1d8e15dd19f5cccdfce8b0b78b824e45 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"In 2015, it was announced that a deep learning model exceeded human performance on the ImageNet test set (He et al., 2015). However, research in the subsequent years has repeatedly found that these models still make many simple mistakes (Dodge & Karam, 2017;Geirhos et al., 2018;Alcorn et al., 2019), and new benchmarks testing these systems has often found their performance to be much lower than both their ImageNet accuracy and human accuracy (Recht et al., 2019;Barbu et al., 2019). What explains this discrepancy? Various ideas have been suggested and studied (Ilyas et al., 2019;Geirhos et al., 2020). A common theme of proposed explanations is that deep learning models are exceedingly adept at finding correlations and patterns which hold across their training dataset and thus improve in-distribution performance. However many of these correlations and patterns are actually spurious and do not hold for other distributions and result in large drops in performance on other datasets.We caution that, to date, most of these studies limit their evaluation to models trained on ImageNet. Recalling the topic of discussion, it may be a mistake to generalize too far from these initial findings. To what degree are these failures attributable to deep learning, ImageNet, or some combination of the two? CLIP models, which are trained via natural language supervision on a very large dataset and are capable of high zero-shot performance, are an opportunity to investigate this question from a different angle. Taori et al. (2020) is a recent comprehensive study moving towards quantifying and understanding these behaviors for ImageNet models. Taori et al. (2020) study how the performance of ImageNet models change when evaluated on natural distribution shifts. They measure performance on a set of 7 distribution shifts: ImageNetV2 (Recht et al., 2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB and ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu et al., 2019), ImageNet Adversarial (Hendrycks et al., 2019), and ImageNet Rendition (Hendrycks et al., 2020a). They distinguish these datasets, which all consist of novel images collected from a variety of sources, from synthetic distribution shifts such as ImageNet-C (Hendrycks & Dietterich, 2019), Stylized ImageNet (Geirhos et al., 2018), or adversarial attacks (Goodfellow et al., 2014) which are created by perturbing existing images in various ways. They propose this distinction because in part because they find that while several techniques have been demonstrated to improve performance on synthetic distribution shifts, they often fail to yield consistent improvements on natural distributions. 3Across these collected datasets, the accuracy of ImageNet models drop well below the expectation set by the Ima-geNet validation set. For the following summary discussion we report average accuracy across all 7 natural distribution shift datasets and average accuracy across the corresponding class subsets of ImageNet unless otherwise specified. Additionally, for Youtube-BB and ImageNet-Vid, which have two different evaluation settings, we use the average of pm-0 and pm-10 accuracy.A ResNet-101 makes 5 times as many mistakes when evaluated on these natural distribution shifts compared to the ImageNet validation set. Encouragingly however, Taori et al. (2020) find that accuracy under distribution shift increases predictably with ImageNet accuracy and is well modeled as a linear function of logit-transformed accuracy. Taori et al. (2020)  Linear probe average over 26 datasetsCLIP\\'s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.or fine-tuned on the ImageNet dataset. Returning to the discussion in the introduction to this section -is training or adapting to the ImageNet dataset distribution the cause of the observed robustness gap? Intuitively, a zero-shot model should not be able to exploit spurious correlations or patterns that hold only on a specific distribution, since it is not trained on that distribution. 4 Thus it is reasonable to expect zero-shot models to have much higher effective robustness. In Figure 13, we compare the performance of zero-shot CLIP with existing ImageNet models on natural distribution shifts. All zero-shot CLIP models improve effective robustness by a large amount and reduce the size of the gap between ImageNet accuracy and accuracy under distribution shift by up to 75%.While these results show that zero-shot models can be much more robust, they do not necessarily mean that supervised learning on ImageNet causes a robustness gap. Other details of CLIP, such as its large and diverse pre-training dataset or use of natural language supervision could also result in much more robust models regardless of whether they are zero-shot or fine-tuned. As an initial experiment to potentially begin narrowing this down, we also measure how the performance of CLIP models change after adapting to the ImageNet distribution via a L2 regularized logistic regression classifier fit to CLIP features on the ImageNet training set. We visualize how performance changes from the zero-shot classifier in Figure 14. Although adapting CLIP to the ImageNet distribution increases its ImageNet accuracy by 9.2% to 85.4% overall, and ties the accuracy of the 2018 SOTA from Mahajan et al. (2018), average accuracy under distribution shift slightly decreases.It is surprising to see a 9.2% increase in accuracy, which corresponds to roughly 3 years of improvement in SOTA, fail to translate into any improvement in average performance under distribution shift. We also break down the differences between zero-shot accuracy and linear classifier accuracy per dataset in Figure 14 and find performance still increases significantly on one dataset, ImageNetV2. ImageNetV2 closely followed the creation process of the original Ima-geNet dataset which suggests that gains in accuracy from supervised adaptation are closely concentrated around the ImageNet distribution. Performance decreases by 4.7% on   ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch, and 1.9% on ImageNet-A. The change in accuracy on the two other datasets, Youtube-BB and ImageNet Vid, is insignificant.How is it possible to improve accuracy by 9.2% on the Im-ageNet dataset with little to no increase in accuracy under distribution shift? Is the gain primarily from \\\\\"exploiting spurious correlations\\\\\"? Is this behavior unique to some combination of CLIP, the ImageNet datatset, and the distribution shifts studied, or a more general phenomena? Does it hold for end-to-end finetuning as well as linear classifiers? We do not have confident answers to these questions at this time. Prior work has also pre-trained models on distributions other than ImageNet, but it is common to study and release models only after they have been fine-tuned to ImageNet. As a step towards understanding whether pre-trained zero-shot models consistently have higher effective robustness than fine-tuned models, we encourage the authors of Mahajan et al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al. (2020) to, if possible, study these questions on their models as well.We also investigate another robustness intervention enabled by flexible zero-shot natural-language-based image classifiers. The target classes across the 7 transfer datasets are not always perfectly aligned with those of ImageNet. Two datasets, Youtube-BB and ImageNet-Vid, consist of superclasses of ImageNet. This presents a problem when trying to use the fixed 1000-way classifier of an ImageNet model to make predictions. Taori et al. (2020) handle this by max-pooling predictions across all sub-classes according to the ImageNet class hierarchy. Sometimes this mapping is much less than perfect. For the person class in Youtube-BB, predictions are made by pooling over the ImageNet classes for a baseball player, a bridegroom, and a scuba diver. With CLIP we can instead generate a custom zero-shot classifier for each dataset directly based on its class names. In Figure 14 we see that this improves average effective robustness by 5% but is concentrated in large improvements on only a few datasets. Curiously, accuracy on ObjectNet also increases by 2.3%. Although the dataset was designed to closely overlap with ImageNet classes, using the names provided for each class by ObjectNet\\'s creators still helps a small amount compared to using ImageNet class names and pooling predictions when necessary.While zero-shot CLIP improves effective robustness, Figure 14 shows that the benefit is almost entirely gone in a fully supervised setting. To better understand this difference, we investigate how effective robustness changes on the continuum from zero-shot to fully supervised. In Figure 15 we visualize the performance of 0-shot, 1-shot, 2-shot, 4-shot ..., 128-shot, and fully supervised logistic regression classifiers on the best CLIP model\\'s features. We see that while few-shot models also show higher effective robustness than existing models, this benefit fades as in-distribution performance increases with more training data and is mostly, though not entirely, gone for the fully supervised model. Additionally, zero-shot CLIP is notably more robust than a few-shot model with equivalent ImageNet performance.   Across our experiments, high effective robustness seems to result from minimizing the amount of distribution specific training data a model has access to, but this comes at a cost of reducing dataset-specific performance.Taken together, these results suggest that the recent shift towards large-scale task and dataset agnostic pre-training combined with a reorientation towards zero-shot and fewshot benchmarking on broad evaluation suites (as advocated by Yogatama et al. (2019) and Linzen (2020)) promotes the development of more robust systems and provides a more accurate assessment of performance. We are curious to see if the same results hold for zero-shot models in the field of NLP such as the GPT family. While Hendrycks et al. (2020b) has reported that pre-training improves relative robustness on sentiment analysis, Miller et al. (2020)\\'s study of the robustness of question answering models under natural distribution shift finds, similar to Taori et al. (2020), little evidence of effective robustness improvements to date.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2507 request_id=56d920dcaf96063fcea86523b876a2be response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"How does CLIP compare to human performance and human learning? To get a better understanding of how well humans perform in similar evaluation settings to CLIP, we evaluated humans on one of our tasks. We wanted to get a sense of how strong human zero-shot performance is at these tasks, and how much human performance is improved if they are shown one or two image samples. This can help us to compare task difficulty for humans and CLIP, and identify correlations and differences between them.We had five different humans look at each of 3669 images in the test split of the Oxford IIT Pets dataset (Parkhi et al., 2012) and select which of the 37 cat or dog breeds best matched the image (or \\'I don\\'t know\\' if they were completely uncertain). In the zero-shot case the humans were given no examples of the breeds and asked to label them to the best of their ability without an internet search. In the one-shot experiment the humans were given one sample image of each breed and in the two-shot experiment they were given two sample images of each breed. 5One possible concern was that the human workers were not sufficiently motivated in the zero-shot task. High human accuracy of 94% on the STL-10 dataset (Coates et al., 2011)  and 97-100% accuracy on the subset of attention check images increased our trust in the human workers.Interestingly, humans went from a performance average of 54% to 76% with just one training example per class, and the marginal gain from an additional training example is minimal. The gain in accuracy going from zero to one shot is almost entirely on images that humans were uncertain about. This suggests that humans \\\\\"know what they don\\'t know\\\\\" and are able to update their priors on the images they are most uncertain in based on a single example. Given this, it seems that while CLIP is a promising training strategy for zero-shot performance (Figure 5) and does well on tests of natural distribution shift (Figure 13), there is a large difference between how humans learn from a few examples and the few-shot methods in this paper.This suggests that there are still algorithmic improvements waiting to be made to decrease the gap between machine and human sample efficiency, as noted by Lake et al. (2016) and others. Because these few-shot evaluations of CLIP don\\'t make effective use of prior knowledge and the humans do, we speculate that finding a method to properly integrate prior knowledge into few-shot learning is an important step in algorithmic improvements to CLIP. To our knowledge, using a linear classifier on top of the features of a high- As in Parkhi et al. (2012), the metric is average per-class classification accuracy. Most of the gain in performance when going from the human zero shot case to the human one shot case is on images that participants were highly uncertain on. \\\\\"Guesses\\\\\" refers to restricting the dataset to where participants selected an answer other than \\\\\"I don\\'t know\\\\\", the \\\\\"majority vote\\\\\" is taking the most frequent (exclusive of ties) answer per image.quality pre-trained model is near state-of-the-art for few shot learning (Tian et al., 2020), which suggests that there is a gap between the best few-shot machine learning methods and human few-shot learning.If we plot human accuracy vs CLIP\\'s zero shot accuracy (Figure 16), we see that the hardest problems for CLIP are also hard for humans. To the extent that errors are consistent, our hypothesis is that this is due to at least a two factors: noise in the dataset (including mislabeled images) and out of distribution images being hard for both humans and models.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2710 request_id=26cf7ccb82da9f0d83b51bc53f2ac86c response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"A concern with pre-training on a very large internet dataset is unintentional overlap with downstream evals. This is important to investigate since, in a worst-case scenario, a complete copy of an evaluation dataset could leak into the pre-training dataset and invalidate the evaluation as a meaningful test of generalization. One option to prevent this is to identify and remove all duplicates before training a model. While this guarantees reporting true hold-out performance, it requires knowing all possible data which a model might be evaluated on ahead of time. This has the downside of limiting the scope of benchmarking and analysis. Adding a new evaluation would require an expensive re-train or risk reporting an un-quantified benefit due to overlap.Instead, we document how much overlap occurs and how performance changes due to these overlaps. To do this, we use the following procedure: contains all examples that are below this threshold. We denote the unaltered full dataset All for reference. From this we first record the degree of data contamination as the ratio of the number of examples in Overlap to the size of All.2) We then compute the zero-shot accuracy of CLIP RN50x64 on the three splits and report All -Clean as our main metric. This is the difference in accuracy due to contamination. When positive it is our estimate of how much the overall reported accuracy on the dataset was inflated by over-fitting to overlapping data.3) The amount of overlap is often small so we also run a binomial significance test where we use the accuracy on Clean as the null hypothesis and compute the one-tailed (greater) p-value for the Overlap subset. We also calculate 99.5% Clopper-Pearson confidence intervals on Dirty as another check.A summary of this analysis is presented in Figure 17. Out of 35 datasets studied, 9 datasets have no detected overlap at all. Most of these datasets are synthetic or specialized making them unlikely to be posted as normal images on the internet (for instance MNIST, CLEVR, and GTSRB) or are guaranteed to have no overlap due to containing novel data from after the date our dataset was created (ObjectNet and Hateful Memes). This demonstrates our detector has a low-false positive rate which is important as false positives would under-estimate the effect of contamination in our analysis. There is a median overlap of 2.2% and an average overlap of 3.2%. Due to this small amount of overlap, overall accuracy is rarely shifted by more than 0.1% with only 7 datasets above this threshold. Of these, only 2 are statistically significant after Bonferroni correction. The max detected improvement is only 0.6% on Birdsnap which has the second largest overlap at 12.1%. The largest overlap is for Country211 at 21.5%. This is due to it being constructed out of YFCC100M, which our pre-training dataset contains a filtered subset of. Despite this large overlap there is only a 0.2% increase in accuracy on Country211. This may be because the training text accompanying an example is often not related to the specific task a downstream eval measures.Country211 measures geo-localization ability, but inspecting the training text for these duplicates showed they often do not mention the location of the image.We are aware of two potential concerns with our analysis. First our detector is not perfect. While it achieves near 100% accuracy on its proxy training task and manual inspection + threshold tuning results in very high precision with good recall among the found nearest-neighbors, we can not tractably check its recall across 400 million examples.Another potential confounder of our analysis is that the underlying data distribution may shift between the Overlap and Clean subsets. For example, on Kinetics-700 many \\\\\"overlaps\\\\\" are in fact all black transition frames. This explains why Kinetics-700 has an apparent 20% accuracy drop on Overlap. We suspect more subtle distribution shifts likely exist. One possibility we noticed on CIFAR-100 is that, due to the very low resolution of its images, many duplicates were false positives of small objects such as birds or planes. Changes in accuracy could instead be due to changes in the class distribution or difficulty of the duplicates. Unfortunately, these distribution and difficulty shifts could also mask the effects of over-fitting.However, these results closely follow the findings of similar duplicate analysis in previous work on large scale pretraining. Mahajan et al. (2018) and Kolesnikov et al. (2019) detected similar overlap rates and found minimal changes in overall performance. Importantly, Kolesnikov et al. ( 2019) also compared the alternative de-duplication strategy discussed in the introduction to this section with the approach we settled on and observed little difference between the two approaches.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3649 request_id=7a90e98f8fbbbf0f27c0e58d7c19d008 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"There are still many limitations to CLIP. While several of these are discussed as part of analysis in various sections, we summarize and collect them here.On datasets with training splits, the performance of zeroshot CLIP is on average competitive with the simple su- (Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy improvements statistically significant when calculated using a one-sided binomial test.pervised baseline of a linear classifier on top of ResNet-50 features. On most of these datasets, the performance of this baseline is now well below the overall state of the art. Significant work is still needed to improve the task learning and transfer capabilities of CLIP. While scaling has so far steadily improved performance and suggests a route for continued improvement, we estimate around a 1000x increase in compute is required for zero-shot CLIP to reach overall state-of-the-art performance. This is infeasible to train with current hardware. Further research into improving upon the computational and data efficiency of CLIP will be necessary.Analysis in Section 3.1 found that CLIP\\'s zero-shot performance is still quite weak on several kinds of tasks. When compared to task-specific models, the performance of CLIP is poor on several types of fine-grained classification such as differentiating models of cars, species of flowers, and variants of aircraft. CLIP also struggles with more abstract and systematic tasks such as counting the number of objects in an image. Finally for novel tasks which are unlikely to be included in CLIP\\'s pre-training dataset, such as classifying the distance to the nearest car in a photo, CLIP\\'s performance can be near random. We are confident that there are still many, many, tasks where CLIP\\'s zero-shot performance is near chance level.While zero-shot CLIP generalizes well to many natural image distributions as investigated in Section 3.3, we\\'ve observed that zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. An illustrative example occurs for the task of OCR as reported in Appendix E.CLIP learns a high quality semantic OCR representation that performs well on digitally rendered text, which is common in its pre-training dataset, as evidenced by performance on Rendered SST2. However, CLIP only achieves 88% accuracy on the handwritten digits of MNIST. An embarrassingly simple baseline of logistic regression on raw pixels outperforms zero-shot CLIP. Both semantic and near-duplicate nearest-neighbor retrieval verify that there are almost no images that resemble MNIST digits in our pre-training dataset. This suggests CLIP does little to address the underlying problem of brittle generalization of deep learning models. Instead CLIP tries to circumvent the problem and hopes that by training on such a large and varied dataset that all data will be effectively in-distribution. This is a naive assumption that, as MNIST demonstrates, is easy to violate.Although CLIP can flexibly generate zero-shot classifiers for a wide variety of tasks and datasets, CLIP is still limited to choosing from only those concepts in a given zero-shot classifier. This is a significant restriction compared to a truly flexible approach like image captioning which could generate novel outputs. Unfortunately, as described in Section 2.3 we found the computational efficiency of the image caption baseline we tried to be much lower than CLIP. A simple idea worth trying is joint training of a contrastive and generative objective with the hope of combining the efficiency of CLIP with the flexibility of a caption model. As another alternative, search could be performed at inference time over many natural language explanations of a given image, similar to approach proposed in Learning with Latent Language Andreas et al. (2017).CLIP also does not address the poor data efficiency of deep learning. Instead CLIP compensates by using a source of supervision that can be scaled to hundreds of millions of training examples. If every image seen during training of a CLIP model was presented at a rate of one per second, it would take 405 years to iterate through the 12.8 billion images seen over 32 training epochs. Combining CLIP with self-supervision (Henaff, 2020;Chen et al., 2020c) and self-training (Lee;Xie et al., 2020) methods is a promising direction given their demonstrated ability to improve data efficiency over standard supervised learning.Our methodology has several significant limitations. Despite our focus on zero-shot transfer, we repeatedly queried performance on full validation sets to guide the development of CLIP. These validation sets often have thousands of examples, which is unrealistic for true zero-shot scenarios. Similar concerns have been raised in the field of semi-supervised learning (Oliver et al., 2018). Another potential issue is our selection of evaluation datasets. While we have reported results on Kornblith et al. (2019)\\'s 12 dataset evaluation suite as a standardized collection, our main results use a somewhat haphazardly assembled collection of 27 datasets that is undeniably co-adapted with the development and capabilities of CLIP. Creating a new benchmark of tasks designed explicitly to evaluate broad zero-shot transfer capabilities, rather than re-using existing supervised datasets, would help address these issues.CLIP is trained on text paired with images on the internet. These image-text pairs are unfiltered and uncurated and result in CLIP models learning many social biases. This has been previously demonstrated for image caption models (Bhargava & Forsyth, 2019). We refer readers to Section 7 for detailed analysis and quantification of these behaviors for CLIP as well as discussion of potential mitigation strategies.While we have emphasized throughout this work that specifying image classifiers through natural language is a flexible and general interface, it has its own limitations. Many complex tasks and visual concepts can be difficult to specify just through text. Actual training examples are undeniably useful but CLIP does not optimize for few-shot performance directly. In our work, we fall back to fitting linear classifiers on top of CLIP\\'s features. This results in a counter-intuitive drop in performance when transitioning from a zero-shot to a few-shot setting. As discussed in Section 4, this is notably different from human performance which shows a large increase from a zero to a one shot setting. Future work is needed to develop methods that combine CLIP\\'s strong zero-shot performance with efficient few-shot learning.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2348 request_id=cff78969118cb53901233bbdc17fc5c6 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"Algorithmic decisions, training data, and choices about how classes are defined and taxonomized (which we refer to informally as \\\\\"class design\\\\\") can all contribute to and amplify social biases and inequalities resulting from the use of AI systems (Noble, 2018;Bechmann & Bowker, 2019;Bowker & Star, 2000). Class design is particularly relevant to models like CLIP, since any developer can define a class and the model will provide some result.In this section, we provide preliminary analysis of some of the biases in CLIP, using bias probes inspired by those outlined in Buolamwini & Gebru (2018) and K\\\\u00e4rkk\\\\u00e4inen & Joo (2019). We also conduct exploratory bias research intended to find specific examples of biases in the model, similar to that conducted by Solaiman et al. (2019).We start by analyzing the performance of Zero-Shot CLIP on the face image dataset FairFace (K\\\\u00e4rkk\\\\u00e4inen & Joo, 2019) 6   6 FairFace is a face image dataset designed to balance age, gender, and race, in order to reduce asymmetries common in previous face datasets. It categorizes gender into 2 groups: female and male and race into 7 groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. There are inherent problems with race and gender classifications, as e.g. Bowker & Star (2000) as an initial bias probe, then probe the model further to surface additional biases and sources of biases, including class design.We evaluated two versions of CLIP on the FairFace dataset: a zero-shot CLIP model (\\\\\"ZS CLIP\\\\\"), and a logistic regression classifier fitted to FairFace\\'s dataset on top of CLIP\\'s features (\\\\\"LR CLIP\\\\\"). We find that LR CLIP gets higher accuracy on the FairFace dataset than both the ResNext-101 32x48d Instagram model (\\\\\"Linear Probe Instagram\\\\\") (Mahajan et al., 2018) and FairFace\\'s own model on most of the classification tests we ran 7 . ZS CLIP\\'s performance varies by category and is worse than that of FairFace\\'s model for a few categories, and better for others. (See Table 3 andTable  4). and Keyes (2018) have shown. While FairFace\\'s dataset reduces the proportion of White faces, it still lacks representation of entire large demographic groups, effectively erasing such categories. We use the 2 gender categories and 7 race categories defined in the FairFace dataset in a number of our experiments not in order to reinforce or endorse the use of such reductive categories, but in order to enable us to make comparisons to prior work. 7 One challenge with this comparison is that the FairFace model uses binary classes for race (\\\\\"White\\\\\" and \\\\\"Non-White\\\\\"), instead of breaking down races into finer-grained sub-groups. Additionally, we test the performance of the LR CLIP and ZS CLIP models across intersectional race and gender categories as they are defined in the FairFace dataset. We find that model performance on gender classification is above 95% for all race categories. Table 5 summarizes these results.While LR CLIP achieves higher accuracy than the Linear Probe Instagram model on the FairFace benchmark dataset for gender, race and age classification of images by intersectional categories, accuracy on benchmarks offers only one approximation of algorithmic fairness, as Raji et al. (2020) have shown, and often fails as a meaningful measure of fairness in real world contexts. Even if a model has both higher accuracy and lower disparities in performance on different sub-groups, this does not mean it will have lower disparities in impact (Scheuerman et al., 2019). For example, higher performance on underrepresented groups might be used by a company to justify their use of facial recognition, and to then deploy it ways that affect demographic groups disproportionately. Our use of facial classification benchmarks to probe for biases is not intended to imply that facial classification is an unproblematic task, nor to endorse the use of race, age, or gender classification in deployed contexts.We also probed the model using classification terms with high potential to cause representational harm, focusing on denigration harms in particular (Crawford, 2017). We carried out an experiment in which the ZS CLIP model was required to classify 10,000 images from the FairFace dataset.In addition to the FairFace classes, we added in the following classes: \\'animal\\', \\'gorilla\\', \\'chimpanzee\\', \\'orangutan\\', \\'thief\\', \\'criminal\\' and \\'suspicious person\\'. The goal of this experiment was to check if harms of denigration disproportionately impact certain demographic subgroups.We found that 4.9% (confidence intervals between 4.6% and 5.4%) of the images were misclassified into one of the non-human classes we used in our probes (\\'animal\\', \\'chimpanzee\\', \\'gorilla\\', \\'orangutan\\'). Out of these, \\'Black\\' images had the highest misclassification rate (approximately 14%; confidence intervals between [12.6% and 16.4%]) while all other races had misclassification rates under 8%. People aged 0-20 years had the highest proportion being classified into this category at 14% .We also found that 16.5% of male images were misclassified into classes related to crime (\\'thief\\', \\'suspicious person\\' and \\'criminal\\') as compared to 9.8% of female images. Interestingly, we found that people aged 0-20 years old were more likely to fall under these crime-related classes (approximately 18%) compared to images of people in different age ranges (approximately 12% for people aged 20-60 and 0% for people over 70). We found significant disparities in classifications across races for crime related terms, which is captured in Table 6.Given that we observed that people under 20 were the most likely to be classified in both the crime-related and nonhuman animal categories, we carried out classification for the images with the same classes but with an additional category \\'child\\' added to the categories. Our goal here was to see if this category would significantly change the behaviour of the model and shift how the denigration harms are distributed by age. We found that this drastically reduced the number of images of people under 20 classified in either crime-related categories or non-human animal categories (Table 7). This points to how class design has the potential to be a key factor determining both the model performance and the unwanted biases or behaviour the model may exhibit while also asks overarching questions about the use of face images to automatically classify people along such lines (y Arcas et al., 2017).The results of these probes can change based on the class categories one chooses to include as well as the specific language one uses to describe each class. Poor class design can lead to poor real world performance; this concern is particularly relevant to a model like CLIP, given how easily developers can design their own classes.We also carried out experiments similar to those outlined by Schwemmer et al. (2020) to test how CLIP treated images of men and women differently using images of Members of Congress. As part of these experiments, we studied how certain additional design decisions such as deciding thresholds for labels can impact the labels output by CLIP and how biases manifest.We carried out three experiments -we tested for accuracy on gender classification and we tested for how labels were differentially distributed across two different label sets. For our first label set, we used a label set of 300 occupations and for our second label set we used a combined set of labels that Google Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision returned for all the images.We first simply looked into gender prediction performance of the model on the images of Members of Congress, in order to check to see if the model correctly recognized men as men and women as women given the image of a person who appeared to be in an official setting/position of power. We found that the model got 100% accuracy on the images. This is slightly better performance than the model\\'s performance on the FairFace dataset. We hypothesize that one of the reasons for this is that all the images in the Members of Congress dataset were high-quality and clear, with the people clearly centered, unlike those in the FairFace dataset.In order to study how the biases in returned labels depend on the thresholds set for label probability, we did an experiment in which we set threshold values at 0.5% and 4.0%. We found that the lower threshold led to lower quality of labels. However, even the differing distributions of labels under this threshold can hold signals for bias. For example, we find that under the 0.5% threshold labels such as \\'nanny\\' and \\'housekeeper\\' start appearing for women whereas labels such as \\'prisoner\\' and \\'mobster\\' start appearing for men. This points to gendered associations similar to those that have previously been found for occupations (Schwemmer et al., 2020)  (Nosek et al., 2002)  (Bolukbasi et al., 2016).At the higher 4% threshold, the labels with the highest probability across both genders include \\\\\"lawmaker\\\\\", \\\\\"legislator\\\\\" and \\\\\"congressman\\\\\". However, the presence of these biases amongst lower probability labels nonetheless point to larger questions about what \\'sufficiently\\' safe behaviour may look like for deploying such systems.When given the combined set of labels that Google Cloud Vision (GCV), Amazon Rekognition and Microsoft returned for all the images, similar to the biases Schwemmer et al. (2020) found in GCV systems, we found our system also disproportionately attached labels to do with hair and appearance in general to women more than men. For example, labels such as \\'brown hair\\', \\'blonde\\' and \\'blond\\' appeared significantly more often for women. Additionally, CLIP attached some labels that described high status occupations disproportionately more often to men such as \\'executive\\' and \\'doctor\\'. Out of the only four occupations that it attached more often to women, three were \\'newscaster\\', \\'television presenter\\' and \\'newsreader\\' and the fourth was \\'Judge\\'. This is again similar to the biases found in GCV and points to historical gendered differences (Schwemmer et al., 2020).Interestingly, when we lowered the threshold to 0.5% for this set of labels, we found that the labels disproportionately describing men also shifted to appearance oriented words such as \\'suit\\', \\'tie\\' and \\'necktie\\' (Figure 18). Many occupation oriented words such as \\'military person\\' and \\'executive\\' -which were not used to describe images of women at the higher 4% threshold -were used for both men and women at the lower 0.5% threshold, which could have caused the change in labels for men. The reverse was not true. Descriptive words used to describe women were still uncommon amongst men. \"}]}' message='Post details'\n",
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3018 request_id=7a80a0fc1d8dc9d76aa6d6395527b305 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"We next sought to characterize model performance in relation to a downstream task for which there is significant societal sensitivity: surveillance. Our analysis aims to better embody the characterization approach described above and to help orient the research community towards the potential future impacts of increasingly general purpose computer vision models and aid the development of norms and checks Top labels, images of men Women MenFigure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were identified with \\\\u03c7 2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a certain label by gender. around such systems. Our inclusion of surveillance is not intended to indicate enthusiasm for this domain -rather, we think surveillance is an important domain to try to make predictions about given its societal implications (Zuboff, 2015;Browne, 2015).We measure the model\\'s performance on classification of images from CCTV cameras and zero-shot celebrity identification. We first tested model performance on low-resolution images captured from surveillance cameras (e.g. CCTV cameras). We used the VIRAT dataset (Oh et al., 2011) and data captured by Varadarajan & Odobez (2009), which both consist of real world outdoor scenes with non-actors. Given CLIP\\'s flexible class construction, we tested 515 surveillance images captured from 12 different video sequences on self-constructed general classes for coarse and fine grained classification. Coarse classification required the model to correctly identify the main subject of the image (i.e. determine if the image was a picture of an empty parking lot, school campus, etc.). For fine-grained classification, the model had to choose between two options constructed to determine if the model could identify the presence/absence of smaller features in the image such as a person standing in the corner.For coarse classification, we constructed the classes by handcaptioning the images ourselves to describe the contents of the image and there were always at least 6 options for the model to choose from. Additionally, we carried out a \\'stress test\\' where the class set included at least one more caption for something that was \\'close\\' to the image (for example, \\'parking lot with white car\\' vs. \\'parking lot with red car\\'). We found that the model had a top-1 accuracy of 91.8% on the CCTV images for the initial evaluation. The accuracy dropped significantly to 51.1% for the second evaluation, with the model incorrectly choosing the \\'close\\' answer 40.7% of the time.For fine-grained detection, the zero-shot model performed poorly, with results near random. Note that this experiment was targeted only towards detecting the presence or absence of small objects in image sequences.We also tested CLIP\\'s zero-shot performance for \\'in the wild\\' identity detection using the CelebA dataset8 . We did this to evaluate the model\\'s performance for identity detection using just the publicly available data it was pre-trained on. While we tested this on a dataset of celebrities who have a larger number of images on the internet, we hypothesize that the number of images in the pre-training data needed for the model to associate faces with names will keep decreasing as models get more powerful (see Table 8), which has significant societal implications (Garvie, 2019) mirrors recent developments in natural language processing, in which recent large language models trained on Internet data often exhibit a surprising ability to provide information related to relatively minor public figures (Brown et al., 2020).We found that the model had 59.2% top-1 accuracy out of 100 possible classes for \\'in the wild\\' 8k celebrity images. However, this performance dropped to 43.3% when we increased our class sizes to 1k celebrity names. This performance is not competitive when compared to production level models such as Google\\'s Celebrity Recognition (Google). However, what makes these results noteworthy is that this analysis was done using only zero-shot identification capabilities based on names inferred from pre-training data -we didn\\'t use any additional task-specific dataset, and so the (relatively) strong results further indicate that before deploying multimodal models, people will need to carefully study them for behaviors in a given context and domain.CLIP offers significant benefit for tasks that have relatively little data given its zero-shot capabilities. However, large datasets and high performing supervised models exist for many in-demand surveillance tasks such as facial recognition. As a result, CLIP\\'s comparative appeal for such uses is low. Additionally, CLIP is not designed for common surveillance-relevant tasks like object detection and semantic segmentation. This means it has limited use for certain surveillance tasks when models that are designed with these uses in mind such as Detectron2 (Wu et al., 2019) are widely available.However, CLIP does unlock a certain aspect of usability given how it removes the need for training data. Thus, CLIP and similar models could enable bespoke, niche surveillance use cases for which no well-tailored models or datasets exist, and could lower the skill requirements to build such applications. As our experiments show, ZS CLIP displays nontrivial, but not exceptional, performance on a few surveillance relevant tasks today.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3448 request_id=0339d12dcfab9f1241c556d66176a410 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"model\": \"gpt-3.5-turbo-0301\", \"messages\": [{\"role\": \"system\", \"content\": \"\\\\n### \\\\u6307\\\\u793a ###\\\\n\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9\\\\u3092\\\\u7406\\\\u89e3\\\\u3057\\\\u305f\\\\u4e0a\\\\u3067\\\\uff0c\\\\u91cd\\\\u8981\\\\u306a\\\\u30dd\\\\u30a4\\\\u30f3\\\\u30c8\\\\u3092\\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u30673\\\\u70b9\\\\u66f8\\\\u3044\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002\\\\n### \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d\\\\u306e\\\\u5236\\\\u7d04 ###\\\\n- \\\\u6700\\\\u59273\\\\u500b\\\\n- \\\\u65e5\\\\u672c\\\\u8a9e\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\u500b\\\\u3092100\\\\u6587\\\\u5b57\\\\u4ee5\\\\u5185\\\\n### \\\\u5bfe\\\\u8c61\\\\u3068\\\\u3059\\\\u308b\\\\u8ad6\\\\u6587\\\\u306e\\\\u5185\\\\u5bb9 ###\\\\n{text}\\\\n### \\\\u51fa\\\\u529b\\\\u5f62\\\\u5f0f ###\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d1\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d2\\\\n- \\\\u7b87\\\\u6761\\\\u66f8\\\\u304d3\\\\n\"}, {\"role\": \"user\", \"content\": \"Any model that leverages written, spoken, signed or any other form of human language as part of its training signal is arguably using natural language as a source of supervision. This is an admittedly extremely broad area and covers most work in the field of distributional semantics including topic models (Blei et al., 2003), word, sentence, and paragraph vectors (Mikolov et al., 2013;Kiros et al., 2015;Le & Mikolov, 2014), and language models (Bengio et al., 2003).It also includes much of the broader field of NLP that deals with predicting or modeling sequences of natural language in some way. Work in NLP intentionally leveraging natural language supervision in the form of explanations, feedback, instructions, and advice for tasks such as classification (as opposed to the commonly used representation of supervision as a set of arbitrarily encoded discrete category labels) has 9 A model could be unfit for use due to inadequate performance or due to the inappropriateness of AI use in the application area itself.been explored in many creative and advanced ways. Dialog based learning (Weston, 2016;Li et al., 2016;Hancock et al., 2019) develops techniques to learn from interactive natural language feedback in dialog. Several papers have leveraged semantic parsing to convert natural language explanations into features (Srivastava et al., 2017) or additional training labels (Hancock et al., 2018). More recently, ExpBERT (Murty et al., 2020) uses feature representations produced by conditioning a deep contextual language model on natural language explanations and descriptions of relations to improve performance on the task of relation extraction.CLIP is an example of using natural language as a training signal for learning about a domain other than language. In this context, the earliest use of the term natural language supervision that we are aware of is the work of Ramanathan et al. (2013) which showed that natural language descriptions could be used along side other sources of supervision to improve performance on the task of video event understanding. However, as mentioned in the introduction and approach section, methods of leveraging natural language descriptions in computer vision well predate the use of this specific term, especially for image retrieval (Mori et al., 1999) and object classification (Wang et al., 2009). Other early work leveraged tags (but not natural language) associated with images for the task of semantic segmentation (Barnard et al., 2003). More recently, He & Peng (2017) and Liang et al. (2020) demonstrated using natural language descriptions and explanations to improve fine-grained visual classification of birds. Others have investigated how grounded language can be used to improve visual representations and classifiers on the ShapeWorld dataset (Kuhnle & Copestake, 2017;Andreas et al., 2017;Mu et al., 2019). Finally, techniques which combine natural language with reinforcement learning environments (Narasimhan et al., 2015) have demonstrated exciting emergent behaviors such as systematically accomplishing zero-shot tasks (Hill et al., 2019). CLIP\\'s pre-training task optimizes for text-image retrieval. This areas of research dates back to the mid-90s with the previously mentioned Mori et al. (1999) as representative of early work. While initial efforts focused primarily on predictive objectives over time research shifted towards learning joint multi-modal embedding spaces with techniques like kernel Canonical Correlation Analysis and various ranking objectives (Weston et al., 2010;Socher & Fei-Fei, 2010;Hodosh et al., 2013). Over time work explored many combinations of training objective, transfer, and more expressive models and steadily improved performance (Frome et al., 2013;Socher et al., 2014;Karpathy et al., 2014;Kiros et al., 2014;Faghri et al., 2017).Other work has leveraged natural language supervision for domains other than images. Stroud et al. (2020) explores large scale representation learning by training a system to pair descriptive text with videos instead of images. Several works have explored using dense spoken natural language supervision for videos (Miech et al., 2019;2020b). When considered together with CLIP, these works suggest that large scale natural language supervision is a promising way to learn high quality perceptual systems for many domains. Alayrac et al. (2020) extended this line of work to an additional modality by adding raw audio as an additional supervision source and demonstrated benefits from combining all three sources of supervision.As part of our work on CLIP we also construct a new dataset of image-text pairs. Modern work on image-text retrieval has relied on a set of crowd-sourced sentence level image caption evaluation datasets like Pascal1K (Rashtchian et al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K (Young et al., 2014). However, these datasets are still relatively small and limit achievable performance. Several methods have been proposed to create larger datasets automatically with Ordonez et al. ( 2011) as a notable early example. In the deep learning era, Mithun et al. (2018) demonstrated an additional set of (image, text) pairs collected from the internet could improve retrieval performance and several new automatically constructed datasets such as Conceptual Captions (Sharma et al., 2018), LAIT (Qi et al., 2020), and OCR-CC (Yang et al., 2020) have been created. However, these datasets still use significantly more aggressive filtering or are designed for a specific task such as OCR and as a result are still much smaller than WIT with between 1 and 10 million training examples.A related idea to CLIP is webly supervised learning. This line of work queries image search engines to build image datasets by querying for terms and uses the queries as the labels for the returned images (Fergus et al., 2005). Classifiers trained on these large but noisily labeled datasets can be competitive with those trained on smaller carefully labeled datasets. These image-query pairs are also often used to improve performance on standard datasets as additional training data (Chen & Gupta, 2015). CLIP also uses search queries as part of its dataset creation process. However CLIP only uses full text sequences co-occuring with images as supervision rather than just the queries, which are often only a single word or short n-gram. We also restrict this step in CLIP to text only querying for sub-string matches while most webly supervised work uses standard image search engines which have their own complex retrieval and filtering pipelines that often involve computer vision systems. Of this line of work, Learning Everything about Anything: Webly-Supervised Visual Concept Learning (Divvala et al., 2014) has a notably similar ambition and goal as CLIP.Finally, CLIP is related to a recent burst of activity on learning joint models of vision and language (Lu et al., 2019;Tan & Bansal, 2019;Chen et al., 2019;Li et al., 2020b;Yu et al., 2020). This line of work focuses on richly connecting vision and language in order to solve complex downstream tasks such as visual question answering, visual commonsense reasoning, or multimodal entailment. These approaches leverage impressively engineered models which combine 3 (or more) pre-trained subsystems, typically an image feature model, a region proposal / object detection model, and a pre-trained masked language model such as BERT. These systems are then jointly fine-tuned via various training objectives on image-text pairs and applied to the aforementioned tasks and achieve impressive results. CLIP is instead focused on learning visual models from scratch via natural language supervision and does not densely connect the two domains with a joint attention model. The only interaction in a CLIP model between the image and text domain is a single dot product in a learned joint embedding space. We are excited to see CLIP hybridized with this line of work.\"}]}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3326 request_id=593af50ad9a21ec26e0b1744902c2ce3 response_code=200\n"
     ]
    }
   ],
   "source": [
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "sections = get_sections(root=root)\n",
    "markdown_text = write_markdown(sections=sections, model=model, tokenizer=tokenizer, summarizer=summarizer)\n",
    "# デバッグ用にテキストを保存する\n",
    "with open(f\"{document_path}/tmp_markdown.md\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
